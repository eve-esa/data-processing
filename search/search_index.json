{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Data Processing Pipeline","text":"<p>A high-performance, modular library for extracting, deduplicating, cleaning, anonymizing, and exporting large-scale Earth science and Earth observation datasets.</p>"},{"location":"#features","title":"Features","text":""},{"location":"#extraction","title":"Extraction","text":"<ul> <li>Supports PDF, HTML, XML, Markdown and nested folder structures</li> <li>Automatically detects file formats unless explicitly specified</li> </ul>"},{"location":"#deduplication","title":"Deduplication","text":"<ul> <li>Performs exact matching using SHA-256 checksum</li> <li>Supports LSH based near-duplicate detection with configurable:</li> <li>Shingle size</li> <li>Permutations</li> <li>Similarity threshold</li> </ul>"},{"location":"#cleaning","title":"Cleaning","text":"<ul> <li>Removes irregularities and noise artifacts</li> <li>Corrects LaTeX equations and tables using LLM assistance</li> </ul>"},{"location":"#pii-removal","title":"PII Removal","text":"<ul> <li>Automatically masks Names and Emails using the Presidio framework</li> <li>Configurable detection patterns</li> </ul>"},{"location":"#metadata-extraction","title":"Metadata Extraction","text":"<ul> <li>Extracts Title, Authors, DOI, URL, Year, Journal, and Citation Count</li> <li>PDF-based extraction using MonkeyOCR integration</li> <li>Support for HTML and other formats</li> </ul>"},{"location":"#export","title":"Export","text":"<ul> <li>Saves processed content in multiple formats (default: Markdown)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Install the packages</p> <pre><code>uv sync\n</code></pre> </li> <li> <p>Configure the pipeline (<code>config.yaml</code>)</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_dir\"\n  stages:\n    - name: extraction\n      config: { format: \"xml\"}\n    - name: duplication\n      config: { method: \"lsh\", shingle_size: 3, num_perm: 128, threshold: 0.8 }\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: export\n      config: { format: \"md\", destination: \"output/files\"}\n</code></pre> </li> <li> <p>Run the pipeline</p> <pre><code>eve run\n</code></pre> </li> </ol>"},{"location":"#funding","title":"Funding","text":"<p>This project is supported by the European Space Agency (ESA) \u03a6-lab through the Large Language Model for Earth Observation and Earth Science project, as part of the Foresight Element within FutureEO Block 4 programme.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this project in academic or research settings, please cite:</p>"},{"location":"#license","title":"License","text":"<p>This project is released under the Apache 2.0 License - see the LICENSE file for more details.</p>"},{"location":"api/core/","title":"Core Components","text":"<p>This section covers the core components of the EVE Pipeline that form the foundation of the data processing framework.</p>"},{"location":"api/core/#pipeline","title":"Pipeline","text":"<p>The main pipeline orchestrator that coordinates all processing stages. This is where the files are first batched, converted to document objects, then passed to each pipeline stages.</p> <pre><code>async def pipeline():\n    logger = get_logger(\"pipeline\")\n    cfg = load_config(\"config.yaml\")\n\n    batch_size = cfg.batch_size\n\n    logger.info(\"Starting pipeline execution\")\n\n    input_files = cfg.inputs.get_files()\n\n    logger.info(f\"Processing {len(input_files)} files with batch size {batch_size}\")\n\n    unique_file_formats = {find_format(f) for f in input_files}\n\n    stages_with_extraction_dependency = {\"dedup\", \"cleaning\", \"pii\"}\n\n    if 'md' not in unique_file_formats:\n        user_stage_names = {stage[\"name\"] for stage in cfg.stages}\n        if not any(stage in user_stage_names for stage in stages_with_extraction_dependency):\n            pass\n        else:\n            if \"extraction\" not in user_stage_names:\n                cfg.stages.insert(0, {\"name\": \"extraction\"})\n\n    # enable export by default\n    if not any(stage[\"name\"] == \"export\" for stage in cfg.stages):\n        cfg.stages.append({\"name\": \"export\"})\n\n\n    logger.info(f\"Stages: {[stage['name'] for stage in cfg.stages]}\")\n\n    step_mapping = {\n        \"cleaning\": CleaningStep,\n        \"export\": ExportStep,\n        \"duplication\": DuplicationStep,\n        \"extraction\": ExtractionStep,\n        \"pii\": PiiStep,\n        \"metadata\": MetadataStep,\n    }\n\n    batchable_steps = {\"cleaning\", \"extraction\", \"pii\", \"metadata\", \"export\"}\n\n    has_dedup = any(stage[\"name\"] == \"duplication\" for stage in cfg.stages)\n\n    if has_dedup: \n        logger.info(\"Deduplication detected - collecting all documents before processing\")\n        all_documents = []\n        async for batch in create_batches(input_files, batch_size):\n            batch_docs = batch\n            for stage in cfg.stages:\n                step_name = stage[\"name\"]\n                if step_name == \"duplication\":\n                    break  # stop here, accumulate all docs and run dedup in phase 2\n                if step_name in batchable_steps and step_name in step_mapping:\n                    step_config = stage.get(\"config\", {})\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step on batch: {step_name}\")\n                    batch_docs = await step(batch_docs)\n\n            all_documents.extend(batch_docs)\n\n        documents = all_documents\n        dedup_started = False\n        for stage in cfg.stages:\n            step_name = stage[\"name\"]\n            if step_name == \"duplication\":\n                dedup_started = True\n\n            if dedup_started:\n                step_config = stage.get(\"config\", {})\n                if step_name in step_mapping:\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step: {step_name}\")\n                    documents = await step(documents)\n                else:\n                    logger.error(f\"No implementation found for step: {step_name}\")\n    else:\n        logger.info(\"No deduplication - using streaming batch processing\")\n        all_processed = []\n\n        async for batch in create_batches(input_files, batch_size):\n            batch_docs = batch\n            logger.info(f\"Processing batch of {len(batch_docs)} documents\")\n\n            for stage in cfg.stages:\n                step_name = stage[\"name\"]\n                step_config = stage.get(\"config\", {})\n                if step_name in step_mapping:\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step on batch: {step_name}\")\n                    batch_docs = await step(batch_docs)\n                else:\n                    logger.error(f\"No implementation found for step: {step_name}\")\n\n            all_processed.extend(batch_docs)\n\n        documents = all_processed\n</code></pre>"},{"location":"api/core/#configuration","title":"Configuration","text":"<p>Configuration management for the pipeline using Pydantic models. These objects provide type-safe settings validation and management for all pipeline components.</p> <pre><code>class PipelineConfig(BaseModel):\n    batch_size: int = 20\n    inputs: Inputs\n    stages: list[dict[str, Any]]\n\n    @validator(\"stages\")\n    def check_stages(cls, v):\n        allowed = {\"ingestion\", \"cleaning\", \"export\", \"duplication\", \"extraction\", \"pii\", \"metadata\"}\n        for stage in v:\n            if stage[\"name\"] not in allowed:\n                raise ValueError(f\"Unsupported stage: {stage['name']}. Allowed: {allowed}\")\n        return v\n</code></pre>"},{"location":"api/core/#document-model","title":"Document Model","text":"<p>The unified document object that represents content and metadata throughout the pipeline. Documents are the core data structure that flow through the pipeline, containing both content and associated metadata.</p> <pre><code>class Document:\n    content: str\n    file_path: Path\n    file_format: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def __hash__(self):\n        return hash(self.file_path)\n\n    def __eq__(self, other):\n        return isinstance(other, Document) and self.file_path == other.file_path\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Get the filename without path.\"\"\"\n        return self.file_path.name\n\n    @property\n    def extension(self) -&gt; str:\n        \"\"\"Get the file extension.\"\"\"\n        return self.file_path.suffix.lstrip('.')\n\n    @property\n    def content_length(self) -&gt; int:\n        \"\"\"Get the length of the content.\"\"\"\n        return len(self.content)\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if the document content is empty.\"\"\"\n        return not self.content.strip()\n\n    def add_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Add a metadata entry.\"\"\"\n        self.metadata[key] = value\n\n    def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a metadata value with optional default.\"\"\"\n        return self.metadata.get(key, default)\n\n    def update_content(self, new_content: str) -&gt; None:\n        \"\"\"Update the document content and track the change in metadata.\"\"\"\n        old_length = self.content_length\n        self.content = new_content\n        new_length = self.content_length\n\n        # Track content changes in metadata\n        changes = self.metadata.get('content_changes', [])\n        changes.append({\n            'old_length': old_length,\n            'new_length': new_length,\n            'size_change': new_length - old_length\n        })\n        self.metadata['content_changes'] = changes\n\n    @classmethod\n    def from_path_and_content(cls, file_path: Path, content: str, **metadata) -&gt; 'Document':\n        \"\"\"Create a Document from a file path and content string.\"\"\"\n        return cls(\n            content=content,\n            file_path=file_path,\n            metadata=metadata\n        )\n\n    @classmethod\n    def from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; 'Document':\n        \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n        file_path, content = path_content_tuple\n        return cls.from_path_and_content(file_path, content, **metadata)\n\n    def to_tuple(self) -&gt; tuple[Path, str]:\n        \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n        return (self.file_path, self.content)\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation showing filename and content length.\"\"\"\n        return f\"Document({self.filename}, {self.file_format} format)\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed representation.\"\"\"\n        return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"api/core/#pipeline-step-base","title":"Pipeline Step Base","text":"<p>Abstract base class that all pipeline stages must implement. All custom pipeline components should inherit from PipelineStep to ensure proper integration with the framework.</p> <pre><code>class PipelineStep(ABC):\n    \"\"\"abstract base class for all pipeline steps.\"\"\"\n\n    def __init__(self, config: Any, name: Optional[str] = None):\n        \"\"\"initialize the pipeline step.\n\n        Args:\n            config: Configuration specific to the step.\n            name: Optional name for the step (used for logging).\n        \"\"\"\n        self.config = config\n        self.debug = config.get(\"debug\", False) if isinstance(config, dict) else False\n        self.logger = get_logger(name or self.__class__.__name__)\n\n    @abstractmethod\n    async def execute(self, input_data: Any) -&gt; Any: # TBD\n        \"\"\"Execute the pipeline step.\n\n        Args:\n            input_data: Input data to process.\n\n        Returns:\n            Processed data or result of the step.\n        \"\"\"\n        pass\n\n    async def __call__(self, input_data: Any) -&gt; Any:\n        \"\"\"shortway of calling `execute` method.\n\n        Args:\n            input_data: Input data to process.\n\n        Returns:\n            Processed data or result of the step.\n        \"\"\"\n        return await self.execute(input_data)\n</code></pre>"},{"location":"api/models/","title":"Data Models","text":"<p>This section documents the data models and structures used throughout the EVE Pipeline.</p>"},{"location":"api/models/#document-model","title":"Document Model","text":"<p>The primary data structure representing documents in the pipeline.</p>"},{"location":"api/models/#eve.model.document.Document","title":"<code>Document</code>  <code>dataclass</code>","text":"<p>Unified document object that encapsulates content and metadata throughout the pipeline.</p> <p>This replaces the need to pass (Path, str) tuples and provides a consistent interface for document handling across all pipeline stages.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The actual document text content</p> <code>file_path</code> <code>Path</code> <p>Path to the source file</p> <code>file_format</code> <code>str</code> <p>Format of the source file (pdf, md, html, etc.)</p> <code>metadata</code> <code>Dict[str, Any]</code> <p>Original metadata from the document (preserved from source)</p> <code>embedding</code> <code>Optional[List[float]]</code> <p>Optional embedding vector for the document</p> <code>pipeline_metadata</code> <code>Dict[str, Any]</code> <p>Metadata added by pipeline steps (filters, processing, etc.)</p> Source code in <code>eve/model/document.py</code> <pre><code>@dataclass\nclass Document:\n    \"\"\"\n    Unified document object that encapsulates content and metadata throughout the pipeline.\n\n    This replaces the need to pass (Path, str) tuples and provides a consistent\n    interface for document handling across all pipeline stages.\n\n    Attributes:\n        content: The actual document text content\n        file_path: Path to the source file\n        file_format: Format of the source file (pdf, md, html, etc.)\n        metadata: Original metadata from the document (preserved from source)\n        embedding: Optional embedding vector for the document\n        pipeline_metadata: Metadata added by pipeline steps (filters, processing, etc.)\n    \"\"\"\n\n    content: str\n    file_path: Path\n    file_format: str\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    embedding: Optional[List[float]] = None\n    pipeline_metadata: Dict[str, Any] = field(default_factory=dict)\n\n    def __dict__(self) -&gt; Dict[str, Any]:\n        result = {\n            \"content\": self.content,\n            \"file_path\": str(self.file_path),\n            \"file_format\": self.file_format,\n            \"metadata\": self.metadata.copy(),\n            \"pipeline_metadata\": self.pipeline_metadata.copy(),\n        }\n        if self.embedding is not None:\n            result[\"embedding\"] = self.embedding\n        return result\n\n    def __hash__(self):\n        return hash(self.file_path)\n\n    def __eq__(self, other):\n        return isinstance(other, Document) and self.file_path == other.file_path\n\n    @property\n    def filename(self) -&gt; str:\n        \"\"\"Get the filename without path.\"\"\"\n        return self.file_path.name\n\n    @property\n    def extension(self) -&gt; str:\n        \"\"\"Get the file extension.\"\"\"\n        return self.file_path.suffix.lstrip(\".\")\n\n    @property\n    def content_length(self) -&gt; int:\n        \"\"\"Get the length of the content.\"\"\"\n        return len(self.content)\n\n    def is_empty(self) -&gt; bool:\n        \"\"\"Check if the document content is empty.\"\"\"\n        return not self.content.strip()\n\n    def add_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Add an entry to the original metadata.\"\"\"\n        self.metadata[key] = value\n\n    def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a value from the original metadata with optional default.\"\"\"\n        return self.metadata.get(key, default)\n\n    def add_pipeline_metadata(self, key: str, value: Any) -&gt; None:\n        \"\"\"Add an entry to pipeline metadata (for tracking pipeline processing).\"\"\"\n        self.pipeline_metadata[key] = value\n\n    def get_pipeline_metadata(self, key: str, default: Any = None) -&gt; Any:\n        \"\"\"Get a value from pipeline metadata with optional default.\"\"\"\n        return self.pipeline_metadata.get(key, default)\n\n    def update_content(self, new_content: str) -&gt; None:\n        \"\"\"Update the document content and track the change in metadata.\"\"\"\n        old_length = self.content_length\n        self.content = new_content\n        new_length = self.content_length\n\n        # Track content changes in metadata\n        changes = self.metadata.get(\"content_changes\", [])\n        changes.append(\n            {\n                \"old_length\": old_length,\n                \"new_length\": new_length,\n                \"size_change\": new_length - old_length,\n            }\n        )\n        self.metadata[\"content_changes\"] = changes\n\n    @classmethod\n    def from_path_and_content(\n        cls, file_path: Path, content: str, **metadata\n    ) -&gt; \"Document\":\n        \"\"\"Create a Document from a file path and content string.\"\"\"\n        return cls(content=content, file_path=file_path, metadata=metadata)\n\n    @classmethod\n    def from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; \"Document\":\n        \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n        file_path, content = path_content_tuple\n        return cls.from_path_and_content(file_path, content, **metadata)\n\n    def to_tuple(self) -&gt; tuple[Path, str]:\n        \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n        return (self.file_path, self.content)\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation showing filename and content length.\"\"\"\n        return f\"Document({self.filename}, {self.file_format} format)\"\n\n    def __repr__(self) -&gt; str:\n        \"\"\"Detailed representation.\"\"\"\n        return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.filename","title":"<code>filename: str</code>  <code>property</code>","text":"<p>Get the filename without path.</p>"},{"location":"api/models/#eve.model.document.Document.extension","title":"<code>extension: str</code>  <code>property</code>","text":"<p>Get the file extension.</p>"},{"location":"api/models/#eve.model.document.Document.content_length","title":"<code>content_length: int</code>  <code>property</code>","text":"<p>Get the length of the content.</p>"},{"location":"api/models/#eve.model.document.Document.is_empty","title":"<code>is_empty() -&gt; bool</code>","text":"<p>Check if the document content is empty.</p> Source code in <code>eve/model/document.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if the document content is empty.\"\"\"\n    return not self.content.strip()\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.add_metadata","title":"<code>add_metadata(key: str, value: Any) -&gt; None</code>","text":"<p>Add an entry to the original metadata.</p> Source code in <code>eve/model/document.py</code> <pre><code>def add_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Add an entry to the original metadata.\"\"\"\n    self.metadata[key] = value\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.get_metadata","title":"<code>get_metadata(key: str, default: Any = None) -&gt; Any</code>","text":"<p>Get a value from the original metadata with optional default.</p> Source code in <code>eve/model/document.py</code> <pre><code>def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a value from the original metadata with optional default.\"\"\"\n    return self.metadata.get(key, default)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.add_pipeline_metadata","title":"<code>add_pipeline_metadata(key: str, value: Any) -&gt; None</code>","text":"<p>Add an entry to pipeline metadata (for tracking pipeline processing).</p> Source code in <code>eve/model/document.py</code> <pre><code>def add_pipeline_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Add an entry to pipeline metadata (for tracking pipeline processing).\"\"\"\n    self.pipeline_metadata[key] = value\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.get_pipeline_metadata","title":"<code>get_pipeline_metadata(key: str, default: Any = None) -&gt; Any</code>","text":"<p>Get a value from pipeline metadata with optional default.</p> Source code in <code>eve/model/document.py</code> <pre><code>def get_pipeline_metadata(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a value from pipeline metadata with optional default.\"\"\"\n    return self.pipeline_metadata.get(key, default)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.update_content","title":"<code>update_content(new_content: str) -&gt; None</code>","text":"<p>Update the document content and track the change in metadata.</p> Source code in <code>eve/model/document.py</code> <pre><code>def update_content(self, new_content: str) -&gt; None:\n    \"\"\"Update the document content and track the change in metadata.\"\"\"\n    old_length = self.content_length\n    self.content = new_content\n    new_length = self.content_length\n\n    # Track content changes in metadata\n    changes = self.metadata.get(\"content_changes\", [])\n    changes.append(\n        {\n            \"old_length\": old_length,\n            \"new_length\": new_length,\n            \"size_change\": new_length - old_length,\n        }\n    )\n    self.metadata[\"content_changes\"] = changes\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.from_path_and_content","title":"<code>from_path_and_content(file_path: Path, content: str, **metadata) -&gt; Document</code>  <code>classmethod</code>","text":"<p>Create a Document from a file path and content string.</p> Source code in <code>eve/model/document.py</code> <pre><code>@classmethod\ndef from_path_and_content(\n    cls, file_path: Path, content: str, **metadata\n) -&gt; \"Document\":\n    \"\"\"Create a Document from a file path and content string.\"\"\"\n    return cls(content=content, file_path=file_path, metadata=metadata)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.from_tuple","title":"<code>from_tuple(path_content_tuple: tuple[Path, str], **metadata) -&gt; Document</code>  <code>classmethod</code>","text":"<p>Create a Document from a (Path, str) tuple for backwards compatibility.</p> Source code in <code>eve/model/document.py</code> <pre><code>@classmethod\ndef from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; \"Document\":\n    \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n    file_path, content = path_content_tuple\n    return cls.from_path_and_content(file_path, content, **metadata)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.to_tuple","title":"<code>to_tuple() -&gt; tuple[Path, str]</code>","text":"<p>Convert to (Path, str) tuple for backwards compatibility.</p> Source code in <code>eve/model/document.py</code> <pre><code>def to_tuple(self) -&gt; tuple[Path, str]:\n    \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n    return (self.file_path, self.content)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>String representation showing filename and content length.</p> Source code in <code>eve/model/document.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation showing filename and content length.\"\"\"\n    return f\"Document({self.filename}, {self.file_format} format)\"\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Detailed representation.</p> Source code in <code>eve/model/document.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Detailed representation.\"\"\"\n    return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"api/models/#configuration-models","title":"Configuration Models","text":"<p>Data models for pipeline configuration.</p>"},{"location":"api/models/#inputs-configuration","title":"Inputs Configuration","text":""},{"location":"api/models/#eve.config.Inputs","title":"<code>Inputs</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>eve/config.py</code> <pre><code>class Inputs(BaseModel):\n    mode: str = \"file\"  # file | directory\n    path: Union[str, list[str]]\n\n    def get_files(self) -&gt; list[Path]:\n        paths = [self.path] if isinstance(self.path, str) else self.path\n        files = []\n\n        for p in paths:\n            p = Path(p)\n\n            if p.is_file():\n                files.append(p)\n            elif p.is_dir():\n                files.extend([f for f in p.rglob(\"*\") if f.is_file()]) # recursive search across multiple levels\n        return files\n</code></pre>"},{"location":"api/models/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"api/models/#eve.config.PipelineConfig","title":"<code>PipelineConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>eve/config.py</code> <pre><code>class PipelineConfig(BaseModel):\n    batch_size: int = 20\n    inputs: Inputs\n    stages: list[dict[str, Any]]  # list of dict since we have stage name + stage configs\n\n    @validator(\"stages\")\n    def check_stages(cls, v):\n        allowed = {\"ingestion\", \"cleaning\", \"export\", \"duplication\", \"extraction\", \"pii\", \"metadata\", \"chunker\", \"export_jsonl\", \"perplexity\", \"pii_filter\", \"length_filter\", \"newline_filter\", \"reference_filter\", \"qdrant_upload\"}\n        for stage in v:\n            if stage[\"name\"] not in allowed:\n                raise ValueError(f\"Unsupported stage: {stage['name']}. Allowed: {allowed}\")\n        return v\n</code></pre>"},{"location":"api/stages/","title":"Pipeline Stages","text":"<p>This section documents all the available pipeline stages for processing documents.</p>"},{"location":"api/stages/#extraction-stage","title":"Extraction Stage","text":"<p>Extracts content from various document formats.</p>"},{"location":"api/stages/#eve.steps.extraction.extract_step","title":"<code>extract_step</code>","text":""},{"location":"api/stages/#eve.steps.extraction.extract_step.ExtractionStep","title":"<code>ExtractionStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>eve/steps/extraction/extract_step.py</code> <pre><code>class ExtractionStep(PipelineStep):\n    async def _html_extraction(self, document: Document) -&gt; Document:\n        html_extractor = HtmlExtractor(document)\n        text = await html_extractor.extract_text()\n        return text\n\n    async def _pdf_extraction(self, document: Document, url: str) -&gt; Document:\n        pdf_extractor = PdfExtractor(document, url)\n        text = await pdf_extractor.extract_text()\n        return text\n\n    async def _xml_extraction(self, document: Document) -&gt; Document:\n        xml_extractor = XmlExtractor(document)\n        text = await xml_extractor.extract_text()\n        return text\n\n    async def _md_extraction(self, document: Document) -&gt; Document:\n        md_extractor = MarkdownExtractor(document)\n        text = await md_extractor.extract_text()\n        return text\n\n    async def _jsonl_extraction(self, document: Document) -&gt; List[Document]:\n        jsonl_extractor = JSONLExtractor(document)\n        documents = await jsonl_extractor.extract_documents()\n        return documents\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute text extraction on input files or documents.\n\n        Args:\n            input_data: List of file paths or Document objects to extract text from\n\n        Returns:\n            List of Document objects with extracted text\n        \"\"\"\n        format = self.config.get(\n            \"format\", None\n        )  # write a wrapper to find out the extension\n        if not format:\n            unique_formats = set()\n\n        unique_formats = {document.file_format for document in documents}\n        text_extraction_formats = [\"html\", \"xml\", \"pdf\", \"md\"]\n        supported_formats = [\"jsonl\"]\n\n        self.logger.info(\n            f\"Extracting text from {unique_formats} files. File count: {len(documents)}\"\n        )\n\n        result = []\n        for document in documents:\n            try:\n                # Skip documents that already have content (already extracted in create_batches)\n                # This happens for JSONL files that are pre-loaded\n                if document.content and document.file_format == \"md\":\n                    result.append(document)\n                    self.logger.debug(f\"Skipping already-loaded document: {document.filename}\")\n                    continue\n\n                if document.file_format in text_extraction_formats:\n                    if document.file_format == \"html\":\n                        document_with_text = await self._html_extraction(document)\n                    elif document.file_format == \"pdf\":\n                        url = self.config.get(\"url\", None)\n                        if not url:\n                            self.logger.error(\n                                \"No URL provided for PDF extraction service\"\n                            )\n                        document_with_text = await self._pdf_extraction(document, url)\n                    elif document.file_format == \"xml\":\n                        document_with_text = await self._xml_extraction(document)\n                    elif document.file_format == \"md\":\n                        document_with_text = await self._md_extraction(document)\n                    else:\n                        self.logger.error(f\"Unsupported format: {document.file_format}\")\n                        continue\n\n                    if (\n                        document_with_text\n                        and hasattr(document_with_text, \"content_length\")\n                        and document_with_text.content_length &gt; 1\n                    ):\n                        result.append(document_with_text)\n                        self.logger.info(\n                            f\"Successfully extracted {document_with_text.content_length} characters from {document_with_text.filename}\"\n                        )\n                elif document.file_format in supported_formats:\n                    docs = await self._jsonl_extraction(document)\n                    docs = docs or []\n                    result.extend(docs)\n                    self.logger.info(\n                        f\"Successfully extracted {len(docs)} documents from {document.filename}\"\n                    )\n                else:\n                    self.logger.warning(f\"No text extracted from {document.filename}\")\n            except Exception as e:\n                self.logger.error(\n                    f\"Failed to extract text from {document.filename}: {str(e)}\"\n                )\n                continue\n        return result\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.extract_step.ExtractionStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute text extraction on input files or documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>List of file paths or Document objects to extract text from</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with extracted text</p> Source code in <code>eve/steps/extraction/extract_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute text extraction on input files or documents.\n\n    Args:\n        input_data: List of file paths or Document objects to extract text from\n\n    Returns:\n        List of Document objects with extracted text\n    \"\"\"\n    format = self.config.get(\n        \"format\", None\n    )  # write a wrapper to find out the extension\n    if not format:\n        unique_formats = set()\n\n    unique_formats = {document.file_format for document in documents}\n    text_extraction_formats = [\"html\", \"xml\", \"pdf\", \"md\"]\n    supported_formats = [\"jsonl\"]\n\n    self.logger.info(\n        f\"Extracting text from {unique_formats} files. File count: {len(documents)}\"\n    )\n\n    result = []\n    for document in documents:\n        try:\n            # Skip documents that already have content (already extracted in create_batches)\n            # This happens for JSONL files that are pre-loaded\n            if document.content and document.file_format == \"md\":\n                result.append(document)\n                self.logger.debug(f\"Skipping already-loaded document: {document.filename}\")\n                continue\n\n            if document.file_format in text_extraction_formats:\n                if document.file_format == \"html\":\n                    document_with_text = await self._html_extraction(document)\n                elif document.file_format == \"pdf\":\n                    url = self.config.get(\"url\", None)\n                    if not url:\n                        self.logger.error(\n                            \"No URL provided for PDF extraction service\"\n                        )\n                    document_with_text = await self._pdf_extraction(document, url)\n                elif document.file_format == \"xml\":\n                    document_with_text = await self._xml_extraction(document)\n                elif document.file_format == \"md\":\n                    document_with_text = await self._md_extraction(document)\n                else:\n                    self.logger.error(f\"Unsupported format: {document.file_format}\")\n                    continue\n\n                if (\n                    document_with_text\n                    and hasattr(document_with_text, \"content_length\")\n                    and document_with_text.content_length &gt; 1\n                ):\n                    result.append(document_with_text)\n                    self.logger.info(\n                        f\"Successfully extracted {document_with_text.content_length} characters from {document_with_text.filename}\"\n                    )\n            elif document.file_format in supported_formats:\n                docs = await self._jsonl_extraction(document)\n                docs = docs or []\n                result.extend(docs)\n                self.logger.info(\n                    f\"Successfully extracted {len(docs)} documents from {document.filename}\"\n                )\n            else:\n                self.logger.warning(f\"No text extracted from {document.filename}\")\n        except Exception as e:\n            self.logger.error(\n                f\"Failed to extract text from {document.filename}: {str(e)}\"\n            )\n            continue\n    return result\n</code></pre>"},{"location":"api/stages/#extractors","title":"Extractors","text":""},{"location":"api/stages/#pdf-extractor","title":"PDF Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.pdfs","title":"<code>pdfs</code>","text":""},{"location":"api/stages/#eve.steps.extraction.pdfs.PdfExtractor","title":"<code>PdfExtractor</code>","text":"Source code in <code>eve/steps/extraction/pdfs.py</code> <pre><code>class PdfExtractor:\n    def __init__(self, document: Document, endpoint: str):\n        self.document = document\n        self.endpoint = f\"{endpoint}/predict\"\n        self.extraction = None\n\n    async def _call_nougat(self, session: aiohttp.ClientSession) -&gt; Optional[str]:\n        \"\"\"internal method to call the Nougat API.\"\"\"\n        try:\n            file_content = await read_file(self.document.file_path, 'rb')\n            if not file_content:\n                logger.error(f\"Failed to read file: {self.file_path}\")\n                return None\n\n            data = aiohttp.FormData()\n            data.add_field('file', file_content, filename = self.document.filename, content_type = 'application/pdf')\n\n            async with session.post(self.endpoint, data = data) as response:\n                if response.status == 200:\n                    return await response.text()\n                else:\n                    logger.error(f\"Nougat API request for {self.document.file_path} failed with status {response.status}\")\n                    return None\n        except Exception as e:\n            logger.error(f\"Failed to process {self.document.file_path}: {str(e)}\")\n            return None\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single PDF file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            async with aiohttp.ClientSession() as session:\n                content = await self._call_nougat(session)\n                if not content:\n                    logger.error(f\"Failed to extract content from {self.document.file_path}\")\n                    return None\n                self.document.content = content\n                return self.document\n        except Exception as e:\n            logger.error(f\"Error in PDF extraction for {self.document.file_path}: {str(e)}\")\n            return None\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.pdfs.PdfExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single PDF file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/pdfs.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single PDF file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            content = await self._call_nougat(session)\n            if not content:\n                logger.error(f\"Failed to extract content from {self.document.file_path}\")\n                return None\n            self.document.content = content\n            return self.document\n    except Exception as e:\n        logger.error(f\"Error in PDF extraction for {self.document.file_path}: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/stages/#html-extractor","title":"HTML Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.htmls","title":"<code>htmls</code>","text":""},{"location":"api/stages/#eve.steps.extraction.htmls.HtmlExtractor","title":"<code>HtmlExtractor</code>","text":"Source code in <code>eve/steps/extraction/htmls.py</code> <pre><code>class HtmlExtractor:\n    def __init__(self, document: Document):\n        self.document = document\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single HTML file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            content = await read_file(self.document.file_path, 'r')\n            if not content:\n                logger.error(f\"Failed to read file: {self.document.file_path}\")\n                return None\n\n            def parse_html():\n                return extract(content, include_comments = False, include_tables = True)\n\n            self.document.content = await asyncio.to_thread(parse_html)\n            return self.document\n        except Exception as e:\n            logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n            return None\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.htmls.HtmlExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single HTML file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/htmls.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single HTML file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        def parse_html():\n            return extract(content, include_comments = False, include_tables = True)\n\n        self.document.content = await asyncio.to_thread(parse_html)\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/stages/#xml-extractor","title":"XML Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.xmls","title":"<code>xmls</code>","text":""},{"location":"api/stages/#eve.steps.extraction.xmls.XmlExtractor","title":"<code>XmlExtractor</code>","text":"Source code in <code>eve/steps/extraction/xmls.py</code> <pre><code>class XmlExtractor:\n    def __init__(self, document: Document):\n        self.document = document\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single XML file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            content = await read_file(self.document.file_path, 'r')\n            if not content:\n                logger.error(f\"Failed to read file: {self.document.file_path}\")\n                return None\n\n            def parse_and_extract():\n                root = ET.fromstring(content)\n\n                def extract_text_from_tree(element):\n                    texts = []\n                    if element.text:\n                        texts.append(element.text)\n                    for child in element:\n                        texts.extend(extract_text_from_tree(child))\n                    if element.tail:\n                        texts.append(element.tail)\n                    return texts\n\n                extracted_texts = extract_text_from_tree(root)\n                full_text = ''.join(extracted_texts)\n                cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n                return cleaned_text.strip()\n\n            self.document.content = await asyncio.to_thread(parse_and_extract)\n            return self.document\n        except Exception as e:\n            logger.error(f\"Error processing XML file {self.document.file_path}: {e}\")\n            return None\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.xmls.XmlExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single XML file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/xmls.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single XML file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        def parse_and_extract():\n            root = ET.fromstring(content)\n\n            def extract_text_from_tree(element):\n                texts = []\n                if element.text:\n                    texts.append(element.text)\n                for child in element:\n                    texts.extend(extract_text_from_tree(child))\n                if element.tail:\n                    texts.append(element.tail)\n                return texts\n\n            extracted_texts = extract_text_from_tree(root)\n            full_text = ''.join(extracted_texts)\n            cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n            return cleaned_text.strip()\n\n        self.document.content = await asyncio.to_thread(parse_and_extract)\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing XML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/stages/#markdown-extractor","title":"Markdown Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.markdown","title":"<code>markdown</code>","text":""},{"location":"api/stages/#eve.steps.extraction.markdown.MarkdownExtractor","title":"<code>MarkdownExtractor</code>","text":"Source code in <code>eve/steps/extraction/markdown.py</code> <pre><code>class MarkdownExtractor:\n    def __init__(self, document: Document):\n        self.document = document\n\n    async def extract_text(self) -&gt; Optional[Document]:\n        \"\"\"Extract text from a single markdown file.\n\n        Returns:\n            Document object with extracted text if successful, None otherwise\n        \"\"\"\n        try:\n            content = await read_file(self.document.file_path, 'r')\n            if not content:\n                logger.error(f\"Failed to read file: {self.document.file_path}\")\n                return None\n\n            self.document.content = content\n            return self.document\n        except Exception as e:\n            logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n            return None\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.markdown.MarkdownExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single markdown file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/markdown.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single markdown file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        self.document.content = content\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/stages/#deduplication-stage","title":"Deduplication Stage","text":"<p>Removes duplicate and near-duplicate documents.</p>"},{"location":"api/stages/#eve.steps.dedup.dedup_step","title":"<code>dedup_step</code>","text":""},{"location":"api/stages/#eve.steps.dedup.dedup_step.DuplicationStep","title":"<code>DuplicationStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>eve/steps/dedup/dedup_step.py</code> <pre><code>class DuplicationStep(PipelineStep):\n    async def _exact_deduplication(self, documents: List[Document]) -&gt; List[Document]:\n        finder = ExactDuplication(documents)\n        duplicates = await finder.find_duplicates()\n        return duplicates\n\n    async def _lsh_deduplication(self, documents: List[Document]) -&gt; List[Document]:\n        shingle_size = self.config.get(\"shingle_size\", 3)\n        num_perm = self.config.get(\"num_perm\", 128)\n        threshold = self.config.get(\"threshold\", 0.8)\n        lsh = LSH(documents, shingle_size, num_perm, threshold)\n        duplicates = lsh.find_duplicates() \n        return duplicates\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute deduplication on input files or documents.\n\n        Args:\n            input_data: List of file paths or Document objects to deduplicate\n\n        Returns:\n            List of Document objects with duplicates removed\n        \"\"\"\n        method = self.config.get(\"method\", \"exact\")  # default to exact\n\n        self.logger.info(f\"Executing duplication step with method: {method} file count: {len(documents)}\")\n\n        if method == \"exact\":\n            duplicates = await self._exact_deduplication(documents)\n        elif method == \"lsh\":\n            duplicates = await self._lsh_deduplication(documents)\n        else:\n            self.logger.error(f\"Invalid deduplication method: {method}\")\n            raise ValueError(f\"Invalid deduplication method: {method}\")\n\n        # Remove duplicates from documents\n        duplicate_docs = set()\n        duplicates_removed = 0\n        for group in duplicates:\n            # Keep the first doc in each group, mark the rest as duplicates\n            for doc in group[1:]:\n                duplicate_docs.add(doc)\n                duplicates_removed += 1\n\n        # Filter out duplicates, keeping the first occurrence\n        result_documents = []\n        for doc in documents:\n            if doc not in duplicate_docs:\n                result_documents.append(doc)\n\n        self.logger.info(\n            f\"Deduplication complete: {len(result_documents)} files remaining, {duplicates_removed} duplicates removed\"\n        )\n        return result_documents\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.dedup_step.DuplicationStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute deduplication on input files or documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>List of file paths or Document objects to deduplicate</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with duplicates removed</p> Source code in <code>eve/steps/dedup/dedup_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute deduplication on input files or documents.\n\n    Args:\n        input_data: List of file paths or Document objects to deduplicate\n\n    Returns:\n        List of Document objects with duplicates removed\n    \"\"\"\n    method = self.config.get(\"method\", \"exact\")  # default to exact\n\n    self.logger.info(f\"Executing duplication step with method: {method} file count: {len(documents)}\")\n\n    if method == \"exact\":\n        duplicates = await self._exact_deduplication(documents)\n    elif method == \"lsh\":\n        duplicates = await self._lsh_deduplication(documents)\n    else:\n        self.logger.error(f\"Invalid deduplication method: {method}\")\n        raise ValueError(f\"Invalid deduplication method: {method}\")\n\n    # Remove duplicates from documents\n    duplicate_docs = set()\n    duplicates_removed = 0\n    for group in duplicates:\n        # Keep the first doc in each group, mark the rest as duplicates\n        for doc in group[1:]:\n            duplicate_docs.add(doc)\n            duplicates_removed += 1\n\n    # Filter out duplicates, keeping the first occurrence\n    result_documents = []\n    for doc in documents:\n        if doc not in duplicate_docs:\n            result_documents.append(doc)\n\n    self.logger.info(\n        f\"Deduplication complete: {len(result_documents)} files remaining, {duplicates_removed} duplicates removed\"\n    )\n    return result_documents\n</code></pre>"},{"location":"api/stages/#deduplication-methods","title":"Deduplication Methods","text":""},{"location":"api/stages/#exact-duplicates","title":"Exact Duplicates","text":""},{"location":"api/stages/#eve.steps.dedup.exact_duplicates","title":"<code>exact_duplicates</code>","text":""},{"location":"api/stages/#eve.steps.dedup.exact_duplicates.ExactDuplication","title":"<code>ExactDuplication</code>","text":"<p>this class does exact duplication by -</p> <ol> <li>calculate size as a first filter to save computation.</li> <li>calcuates checksum and finds the duplicates.</li> </ol> Source code in <code>eve/steps/dedup/exact_duplicates.py</code> <pre><code>class ExactDuplication:\n    \"\"\"this class does exact duplication by -\n\n    1. calculate size as a first filter to save computation.\n    2. calcuates checksum and finds the duplicates.\n    \"\"\"\n\n    def __init__(self, documents: List[Document]):\n        self.documents = documents\n        self.duplicates = []\n\n        self._validate()\n\n    def _validate(self):\n        if len(self.documents) &lt; 2:\n            raise ValueError(\"need at least 2 files for duplication\")\n\n    @staticmethod\n    async def _calculate_sha256(file_path: Path) -&gt; str:\n        \"\"\"calculate SHA-256 checksum of a file.\"\"\"\n        sha256 = hashlib.sha256()\n        async for chunk in read_in_chunks(file_path, 'rb'):\n            sha256.update(chunk)\n        return sha256.hexdigest()\n\n    @staticmethod\n    async def _calculate_size(file_path: Path) -&gt; int:\n        \"\"\"calculate file size\"\"\"\n        stat = await asyncio.to_thread(lambda: file_path.stat())  # run blocking stat in thread\n        return stat.st_size\n\n    async def find_duplicates(self) -&gt; list[list[Document]]:\n        \"\"\"Find duplicate files based on size and SHA-256 checksum.\"\"\"\n\n        # stage 1: group files by size\n        size_tasks = [self._calculate_size(doc.file_path) for doc in self.documents]\n        sizes = await asyncio.gather(*size_tasks)\n\n        size_groups = defaultdict(list)\n        for doc, size in zip(self.documents, sizes):\n            size_groups[size].append(doc)\n\n        # stage 2: calculate checksums for potential duplicates\n        checksum_tasks = []\n        file_info = []\n\n        for size, docs in size_groups.items():\n            if len(docs) &gt;= 2:  # Only consider docs with matching sizes\n                for doc in docs:\n                    checksum_tasks.append(self._calculate_sha256(doc.file_path))\n                    file_info.append((doc, size))\n\n        if not checksum_tasks:\n            return []\n\n        checksums = await asyncio.gather(*checksum_tasks)\n\n        file_map = defaultdict(list)\n        for (doc, size), checksum in zip(file_info, checksums):\n            key = (size, checksum)\n            file_map[key].append(doc)\n\n        self.duplicates = {key: docs for key, docs in file_map.items() if len(docs) &gt; 1}\n        return list(self.duplicates.values())\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.exact_duplicates.ExactDuplication.find_duplicates","title":"<code>find_duplicates() -&gt; list[list[Document]]</code>  <code>async</code>","text":"<p>Find duplicate files based on size and SHA-256 checksum.</p> Source code in <code>eve/steps/dedup/exact_duplicates.py</code> <pre><code>async def find_duplicates(self) -&gt; list[list[Document]]:\n    \"\"\"Find duplicate files based on size and SHA-256 checksum.\"\"\"\n\n    # stage 1: group files by size\n    size_tasks = [self._calculate_size(doc.file_path) for doc in self.documents]\n    sizes = await asyncio.gather(*size_tasks)\n\n    size_groups = defaultdict(list)\n    for doc, size in zip(self.documents, sizes):\n        size_groups[size].append(doc)\n\n    # stage 2: calculate checksums for potential duplicates\n    checksum_tasks = []\n    file_info = []\n\n    for size, docs in size_groups.items():\n        if len(docs) &gt;= 2:  # Only consider docs with matching sizes\n            for doc in docs:\n                checksum_tasks.append(self._calculate_sha256(doc.file_path))\n                file_info.append((doc, size))\n\n    if not checksum_tasks:\n        return []\n\n    checksums = await asyncio.gather(*checksum_tasks)\n\n    file_map = defaultdict(list)\n    for (doc, size), checksum in zip(file_info, checksums):\n        key = (size, checksum)\n        file_map[key].append(doc)\n\n    self.duplicates = {key: docs for key, docs in file_map.items() if len(docs) &gt; 1}\n    return list(self.duplicates.values())\n</code></pre>"},{"location":"api/stages/#minhash-lsh","title":"MinHash LSH","text":""},{"location":"api/stages/#eve.steps.dedup.minhash","title":"<code>minhash</code>","text":"<p>Adjust NUM_PERM: Higher values increase accuracy but use more memory. Adjust THRESHOLD: Higher values find closer duplicates but may miss some. Adjust SHINGLE_SIZE: Larger shingles are more specific but increase computation.</p>"},{"location":"api/stages/#eve.steps.dedup.minhash.LSH","title":"<code>LSH</code>","text":"Source code in <code>eve/steps/dedup/minhash.py</code> <pre><code>class LSH:\n    def __init__(\n        self,\n        documents: List[Document],\n        shingle_size: int = 3,\n        num_perm: int = 128,\n        threshold: float = 0.8,\n    ):\n        self.documents = documents\n        self.shingle_size = shingle_size\n        self.num_perm = num_perm\n        self.threshold = threshold\n        self.doc_hashes = {}   # map: document -&gt; minHash\n        self.duplicates = []\n\n        self._validate()\n\n    def _validate(self):\n        if len(self.documents) &lt; 2:\n            raise ValueError(\"need at least 2 files for duplication\")\n\n    def create_shingles(self, text: str) -&gt; set[str]:\n        \"\"\"Create shingles (word n-grams) from text.\"\"\"\n        words = text.lower().split()\n        return {\" \".join(gram) for gram in ngrams(words, self.shingle_size)}\n\n    def _do_lsh(self) -&gt; Any:\n        lsh = MinHashLSH(threshold=self.threshold, num_perm=self.num_perm)\n\n        for doc in tqdm(self.documents, total=len(self.documents)):\n            shingles = self.create_shingles(doc.content)\n\n            m = MinHash(num_perm=self.num_perm)\n            for shingle in shingles:\n                m.update(shingle.encode(\"utf8\"))\n\n            # Use file_path as the LSH key, but keep mapping to Document\n            lsh.insert(str(doc.file_path), m)\n            self.doc_hashes[doc] = m\n\n        return lsh\n\n    def find_duplicates(self) -&gt; list[list[Document]]:\n        \"\"\"Find near-duplicate documents using LSH.\"\"\"\n        file_hashes = self._do_lsh()\n        processed = set()\n\n        for doc in self.documents:\n            if doc in processed:\n                continue\n\n            m = self.doc_hashes[doc]\n            candidates = file_hashes.query(m)\n\n            # Convert LSH string keys back to Document objects\n            candidate_docs = [\n                d for d in self.documents if str(d.file_path) in candidates and d != doc\n            ]\n\n            if candidate_docs:\n                group = [doc, *candidate_docs]\n                group = sorted(group, key = lambda d: str(d.file_path))  # consistent ordering\n                if group not in self.duplicates:\n                    self.duplicates.append(group)\n                processed.update(group)\n\n        return self.duplicates\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.minhash.LSH.create_shingles","title":"<code>create_shingles(text: str) -&gt; set[str]</code>","text":"<p>Create shingles (word n-grams) from text.</p> Source code in <code>eve/steps/dedup/minhash.py</code> <pre><code>def create_shingles(self, text: str) -&gt; set[str]:\n    \"\"\"Create shingles (word n-grams) from text.\"\"\"\n    words = text.lower().split()\n    return {\" \".join(gram) for gram in ngrams(words, self.shingle_size)}\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.minhash.LSH.find_duplicates","title":"<code>find_duplicates() -&gt; list[list[Document]]</code>","text":"<p>Find near-duplicate documents using LSH.</p> Source code in <code>eve/steps/dedup/minhash.py</code> <pre><code>def find_duplicates(self) -&gt; list[list[Document]]:\n    \"\"\"Find near-duplicate documents using LSH.\"\"\"\n    file_hashes = self._do_lsh()\n    processed = set()\n\n    for doc in self.documents:\n        if doc in processed:\n            continue\n\n        m = self.doc_hashes[doc]\n        candidates = file_hashes.query(m)\n\n        # Convert LSH string keys back to Document objects\n        candidate_docs = [\n            d for d in self.documents if str(d.file_path) in candidates and d != doc\n        ]\n\n        if candidate_docs:\n            group = [doc, *candidate_docs]\n            group = sorted(group, key = lambda d: str(d.file_path))  # consistent ordering\n            if group not in self.duplicates:\n                self.duplicates.append(group)\n            processed.update(group)\n\n    return self.duplicates\n</code></pre>"},{"location":"api/stages/#cleaning-stage","title":"Cleaning Stage","text":"<p>Cleans and improves document quality.</p>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step","title":"<code>cleaning_step</code>","text":"<p>Comprehensive cleaning step that applies all data cleaning components.</p>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep","title":"<code>CleaningStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Comprehensive cleaning step that applies multiple data cleaning components.</p> <p>This step processes extracted text through various cleaning components to:</p> <ul> <li>Fix OCR-induced errors</li> <li>Remove OCR duplicates</li> <li>Apply Nougat corrections</li> <li>Apply rule-based corrections</li> <li>Remove Nougat artifacts</li> <li>Correct LaTeX syntax errors (optional)</li> </ul> Source code in <code>eve/steps/cleaning/cleaning_step.py</code> <pre><code>class CleaningStep(PipelineStep):\n    \"\"\"\n    Comprehensive cleaning step that applies multiple data cleaning components.\n\n    This step processes extracted text through various cleaning components to:\n\n    - Fix OCR-induced errors\n    - Remove OCR duplicates\n    - Apply Nougat corrections\n    - Apply rule-based corrections\n    - Remove Nougat artifacts\n    - Correct LaTeX syntax errors (optional)\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"Initialize the cleaning step with configuration.\n\n        Args:\n            config: Configuration dictionary with component settings.\n\n                Expected keys:\n\n                - ocr_threshold: float (default 0.99) - OCR duplicate threshold\n                - min_words: int (default 2) - Minimum words for processing\n                - enable_latex_correction: bool (default False) - Enable LaTeX correction\n                - openrouter_api_key: str (optional) - API key for LaTeX correction\n                - openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections\n                - debug: bool (default False) - Enable debug output\n        \"\"\"\n        super().__init__(config, name=\"CleaningStep\")\n\n        ocr_threshold = config.get(\"ocr_threshold\", 0.99)\n        min_words = config.get(\"min_words\", 2)\n        enable_latex = config.get(\"enable_latex_correction\", False)\n        openrouter_key = config.get(\"openrouter_api_key\")\n        openrouter_model = config.get(\"openrouter_model\", \"anthropic/claude-3-haiku\")\n\n        self.processors = [\n            OCRProcessor(debug=self.debug),\n            DuplicateRemovalProcessor(threshold=ocr_threshold, min_words=min_words, debug=self.debug),\n            NougatProcessor(debug=self.debug),\n            RuleBasedProcessor(debug=self.debug),\n        ]\n\n        if enable_latex:\n            self.processors.append(\n                LaTeXProcessor(\n                    debug=self.debug,\n                    api_key=openrouter_key,\n                    model=openrouter_model\n                )\n            )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute the cleaning step on input data.\n\n        Args:\n            documents: List of Documents.\n\n        Returns:\n            List of cleaned Documents.\n        \"\"\"\n        self.logger.info(f\"Executing cleaning step on {len(documents)} documents\")\n\n        if not documents:\n            self.logger.warning(\"No input data provided to cleaning step\")\n            return []\n\n        result = []\n        processed_count = 0\n        failed_count = 0\n\n        for document in documents:\n            if document.is_empty():\n                self.logger.warning(f\"{document.filename} - Empty content, skipping cleaning\")\n                result.append(document)\n                failed_count += 1\n                continue\n\n            try:\n                processed_document = document\n                original_length = document.content_length\n\n                for processor in self.processors:\n                    try:\n                        processed_document = await processor.process(processed_document)\n\n                        if processed_document is None:\n                            self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} returned None\")\n                            processed_document = document\n                            break\n\n                    except Exception as e:\n                        self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} failed: {str(e)}\")\n                        continue\n\n                if original_length &gt; 0 and processed_document.content_length != original_length:\n                    reduction_percent = ((original_length - processed_document.content_length) / original_length) * 100\n\n                    if reduction_percent &gt; 0:\n                        self.logger.info(f\"{document.filename} - Cleaned: {reduction_percent:.2f}% text removed ({original_length} -&gt; {processed_document.content_length} chars)\")\n                    else:\n                        self.logger.info(f\"{document.filename} - Cleaned: No significant changes\")\n\n                result.append(processed_document)\n                processed_count += 1\n\n            except Exception as e:\n                self.logger.error(f\"{document.filename} - Cleaning failed: {str(e)}\")\n                result.append(document)\n                failed_count += 1\n\n        self.logger.info(f\"Cleaning step completed: {processed_count} processed, {failed_count} failed\")\n        return result\n\n    def _get_applicable_formats(self) -&gt; List[str]:\n        \"\"\"Get list of formats that these cleaning components apply to.\n\n        Returns:\n            List of file formats that can be processed by cleaning components.\n        \"\"\"\n        return [\n            \"md\",\n            \"txt\",\n            \"tex\",\n            \"html\",\n            \"xml\",\n        ]\n\n    def get_component_info(self) -&gt; dict:\n        \"\"\"Get information about enabled cleaning processors.\n\n        Returns:\n            Dictionary with processor information.\n        \"\"\"\n        component_info = {\n            \"total_processors\": len(self.processors),\n            \"processors\": [processor.__class__.__name__ for processor in self.processors],\n            \"applicable_formats\": self._get_applicable_formats(),\n            \"debug_enabled\": self.debug\n        }\n\n        latex_enabled = any(isinstance(proc, LaTeXProcessor) for proc in self.processors)\n        component_info[\"latex_correction_enabled\"] = latex_enabled\n\n        return component_info\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.__init__","title":"<code>__init__(config: dict)</code>","text":"<p>Initialize the cleaning step with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with component settings.</p> <p>Expected keys:</p> <ul> <li>ocr_threshold: float (default 0.99) - OCR duplicate threshold</li> <li>min_words: int (default 2) - Minimum words for processing</li> <li>enable_latex_correction: bool (default False) - Enable LaTeX correction</li> <li>openrouter_api_key: str (optional) - API key for LaTeX correction</li> <li>openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections</li> <li>debug: bool (default False) - Enable debug output</li> </ul> required Source code in <code>eve/steps/cleaning/cleaning_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the cleaning step with configuration.\n\n    Args:\n        config: Configuration dictionary with component settings.\n\n            Expected keys:\n\n            - ocr_threshold: float (default 0.99) - OCR duplicate threshold\n            - min_words: int (default 2) - Minimum words for processing\n            - enable_latex_correction: bool (default False) - Enable LaTeX correction\n            - openrouter_api_key: str (optional) - API key for LaTeX correction\n            - openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections\n            - debug: bool (default False) - Enable debug output\n    \"\"\"\n    super().__init__(config, name=\"CleaningStep\")\n\n    ocr_threshold = config.get(\"ocr_threshold\", 0.99)\n    min_words = config.get(\"min_words\", 2)\n    enable_latex = config.get(\"enable_latex_correction\", False)\n    openrouter_key = config.get(\"openrouter_api_key\")\n    openrouter_model = config.get(\"openrouter_model\", \"anthropic/claude-3-haiku\")\n\n    self.processors = [\n        OCRProcessor(debug=self.debug),\n        DuplicateRemovalProcessor(threshold=ocr_threshold, min_words=min_words, debug=self.debug),\n        NougatProcessor(debug=self.debug),\n        RuleBasedProcessor(debug=self.debug),\n    ]\n\n    if enable_latex:\n        self.processors.append(\n            LaTeXProcessor(\n                debug=self.debug,\n                api_key=openrouter_key,\n                model=openrouter_model\n            )\n        )\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute the cleaning step on input data.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Documents.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of cleaned Documents.</p> Source code in <code>eve/steps/cleaning/cleaning_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute the cleaning step on input data.\n\n    Args:\n        documents: List of Documents.\n\n    Returns:\n        List of cleaned Documents.\n    \"\"\"\n    self.logger.info(f\"Executing cleaning step on {len(documents)} documents\")\n\n    if not documents:\n        self.logger.warning(\"No input data provided to cleaning step\")\n        return []\n\n    result = []\n    processed_count = 0\n    failed_count = 0\n\n    for document in documents:\n        if document.is_empty():\n            self.logger.warning(f\"{document.filename} - Empty content, skipping cleaning\")\n            result.append(document)\n            failed_count += 1\n            continue\n\n        try:\n            processed_document = document\n            original_length = document.content_length\n\n            for processor in self.processors:\n                try:\n                    processed_document = await processor.process(processed_document)\n\n                    if processed_document is None:\n                        self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} returned None\")\n                        processed_document = document\n                        break\n\n                except Exception as e:\n                    self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} failed: {str(e)}\")\n                    continue\n\n            if original_length &gt; 0 and processed_document.content_length != original_length:\n                reduction_percent = ((original_length - processed_document.content_length) / original_length) * 100\n\n                if reduction_percent &gt; 0:\n                    self.logger.info(f\"{document.filename} - Cleaned: {reduction_percent:.2f}% text removed ({original_length} -&gt; {processed_document.content_length} chars)\")\n                else:\n                    self.logger.info(f\"{document.filename} - Cleaned: No significant changes\")\n\n            result.append(processed_document)\n            processed_count += 1\n\n        except Exception as e:\n            self.logger.error(f\"{document.filename} - Cleaning failed: {str(e)}\")\n            result.append(document)\n            failed_count += 1\n\n    self.logger.info(f\"Cleaning step completed: {processed_count} processed, {failed_count} failed\")\n    return result\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.get_component_info","title":"<code>get_component_info() -&gt; dict</code>","text":"<p>Get information about enabled cleaning processors.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with processor information.</p> Source code in <code>eve/steps/cleaning/cleaning_step.py</code> <pre><code>def get_component_info(self) -&gt; dict:\n    \"\"\"Get information about enabled cleaning processors.\n\n    Returns:\n        Dictionary with processor information.\n    \"\"\"\n    component_info = {\n        \"total_processors\": len(self.processors),\n        \"processors\": [processor.__class__.__name__ for processor in self.processors],\n        \"applicable_formats\": self._get_applicable_formats(),\n        \"debug_enabled\": self.debug\n    }\n\n    latex_enabled = any(isinstance(proc, LaTeXProcessor) for proc in self.processors)\n    component_info[\"latex_correction_enabled\"] = latex_enabled\n\n    return component_info\n</code></pre>"},{"location":"api/stages/#cleaning-components","title":"Cleaning Components","text":""},{"location":"api/stages/#processors","title":"Processors","text":""},{"location":"api/stages/#eve.steps.cleaning.processors","title":"<code>processors</code>","text":"<p>Consolidated text processing components for the cleaning pipeline.</p> <p>This module combines all the individual cleaning components into a unified structure for better organization and maintainability.</p>"},{"location":"api/stages/#eve.steps.cleaning.processors.TextProcessor","title":"<code>TextProcessor</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text processing components.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>class TextProcessor(ABC):\n    \"\"\"Abstract base class for text processing components.\"\"\"\n\n    def __init__(self, debug: bool = False):\n        \"\"\"Initialize the text processor.\n\n        Args:\n            debug: Enable debug output.\n        \"\"\"\n        self.debug = debug\n        self.logger = get_logger(self.__class__.__name__)\n\n    @abstractmethod\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Process a document and return the cleaned result.\n\n        Args:\n            document: The document to process.\n\n        Returns:\n            Processed document.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.TextProcessor.__init__","title":"<code>__init__(debug: bool = False)</code>","text":"<p>Initialize the text processor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"Initialize the text processor.\n\n    Args:\n        debug: Enable debug output.\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.TextProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process a document and return the cleaned result.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>The document to process.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>Processed document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>@abstractmethod\nasync def process(self, document: Document) -&gt; Document:\n    \"\"\"Process a document and return the cleaned result.\n\n    Args:\n        document: The document to process.\n\n    Returns:\n        Processed document.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.OCRProcessor","title":"<code>OCRProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for fixing OCR-induced text issues.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>class OCRProcessor(TextProcessor):\n    \"\"\"Processor for fixing OCR-induced text issues.\"\"\"\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Fix OCR issues in the document content.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before OCR processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in OCR processing\"\n            )\n            return document\n\n        try:\n            # Fix digit-letter spacing issues\n            cleaned_content = fix_ocr_digit_letter_spacing(document.content)\n\n            document.update_content(cleaned_content)\n            document.add_metadata(\"ocr_processed\", True)\n\n            self.logger.info(f\"{document.filename} - OCR processing completed\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After OCR processing ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(f\"{document.filename} - OCR processing failed: {str(e)}\")\n            return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.OCRProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Fix OCR issues in the document content.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Fix OCR issues in the document content.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before OCR processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in OCR processing\"\n        )\n        return document\n\n    try:\n        # Fix digit-letter spacing issues\n        cleaned_content = fix_ocr_digit_letter_spacing(document.content)\n\n        document.update_content(cleaned_content)\n        document.add_metadata(\"ocr_processed\", True)\n\n        self.logger.info(f\"{document.filename} - OCR processing completed\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After OCR processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(f\"{document.filename} - OCR processing failed: {str(e)}\")\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor","title":"<code>DuplicateRemovalProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for removing OCR-induced duplicate text segments.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>class DuplicateRemovalProcessor(TextProcessor):\n    \"\"\"Processor for removing OCR-induced duplicate text segments.\"\"\"\n\n    def __init__(\n        self, threshold: float = 0.99, min_words: int = 2, debug: bool = False\n    ):\n        \"\"\"\n        Initialize the duplicate removal processor.\n\n        Args:\n            threshold: Similarity threshold for duplicates.\n            min_words: Minimum words required for a unit to be processed.\n            debug: Enable debug output.\n        \"\"\"\n        super().__init__(debug=debug)\n        self.threshold = threshold\n        self.min_words = min_words\n\n    def _is_similar(self, sent1: str, sent2: str) -&gt; bool:\n        \"\"\"Check if two sentences are similar based on word overlap.\"\"\"\n        words1 = sent1.lower().split()\n        words2 = sent2.lower().split()\n\n        if len(words1) &lt; self.min_words:\n            return False\n\n        set1, set2 = set(words1), set(words2)\n        overlap = len(set1 &amp; set2)\n        return (\n            overlap / len(set1) &gt;= self.threshold\n            or overlap / len(set2) &gt;= self.threshold\n        )\n\n    def _remove_near_adjacent_duplicates(\n        self, content: str, filename: str\n    ) -&gt; Tuple[str, List[str]]:\n        \"\"\"Remove near-adjacent duplicate sentences.\"\"\"\n        sentences = content.split(\"\\n\")\n        cleaned = []\n        removed = []\n        i = 0\n\n        while i &lt; len(sentences):\n            current = sentences[i]\n            if len(current.split()) &lt; self.min_words:\n                cleaned.append(current)\n                i += 1\n                continue\n\n            j = i + 1\n            while j &lt; len(sentences) and not sentences[j].strip():\n                j += 1\n\n            if j &lt; len(sentences) and self._is_similar(current, sentences[j]):\n                self.logger.info(\n                    f\"{filename} - Removing near-duplicate: {repr(sentences[j])}\"\n                )\n                removed.append(sentences[j])\n                i = j\n            else:\n                cleaned.append(current)\n                i += 1\n\n        return \"\\n\".join(cleaned), removed\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Remove duplicate content from the document.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before duplicate removal ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in duplicate removal\"\n            )\n            return document\n\n        try:\n            cleaned_content, removed = self._remove_near_adjacent_duplicates(\n                document.content, document.filename\n            )\n\n            percent_removed = 0.0\n            if document.content:\n                percent_removed = (\n                    (len(document.content) - len(cleaned_content))\n                    / len(document.content)\n                    * 100\n                )\n\n            document.update_content(cleaned_content)\n            document.add_metadata(\"duplicates_removed\", len(removed))\n            document.add_metadata(\"duplicate_removal_percent\", percent_removed)\n\n            self.logger.info(\n                f\"{document.filename} - Duplicate removal: {len(removed)} segments, {percent_removed:.2f}% text removed\"\n            )\n\n            if self.debug:\n                self.logger.info(\n                    f\"After duplicate removal ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - Duplicate removal failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor.__init__","title":"<code>__init__(threshold: float = 0.99, min_words: int = 2, debug: bool = False)</code>","text":"<p>Initialize the duplicate removal processor.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold for duplicates.</p> <code>0.99</code> <code>min_words</code> <code>int</code> <p>Minimum words required for a unit to be processed.</p> <code>2</code> <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(\n    self, threshold: float = 0.99, min_words: int = 2, debug: bool = False\n):\n    \"\"\"\n    Initialize the duplicate removal processor.\n\n    Args:\n        threshold: Similarity threshold for duplicates.\n        min_words: Minimum words required for a unit to be processed.\n        debug: Enable debug output.\n    \"\"\"\n    super().__init__(debug=debug)\n    self.threshold = threshold\n    self.min_words = min_words\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Remove duplicate content from the document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Remove duplicate content from the document.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before duplicate removal ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in duplicate removal\"\n        )\n        return document\n\n    try:\n        cleaned_content, removed = self._remove_near_adjacent_duplicates(\n            document.content, document.filename\n        )\n\n        percent_removed = 0.0\n        if document.content:\n            percent_removed = (\n                (len(document.content) - len(cleaned_content))\n                / len(document.content)\n                * 100\n            )\n\n        document.update_content(cleaned_content)\n        document.add_metadata(\"duplicates_removed\", len(removed))\n        document.add_metadata(\"duplicate_removal_percent\", percent_removed)\n\n        self.logger.info(\n            f\"{document.filename} - Duplicate removal: {len(removed)} segments, {percent_removed:.2f}% text removed\"\n        )\n\n        if self.debug:\n            self.logger.info(\n                f\"After duplicate removal ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - Duplicate removal failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.NougatProcessor","title":"<code>NougatProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for fixing Nougat-related issues and artifacts.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>class NougatProcessor(TextProcessor):\n    \"\"\"Processor for fixing Nougat-related issues and artifacts.\"\"\"\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Process Nougat-specific issues in the document.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before Nougat processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in Nougat processing\"\n            )\n            return document\n\n        try:\n            # Apply Nougat postprocessing\n            cleaned = postprocess_single(document.content, markdown_fix=True)\n\n            # Clean LaTeX table formatting\n            cleaned = clean_doubled_backslashes(cleaned)\n\n            # Remove Nougat artifacts\n            cleaned = remove_nougat_artifacts(cleaned)\n\n            # Convert escaped newlines\n            cleaned = cleaned.replace(\"\\\\n\", \"\\n\")\n\n            # Remove surrounding quotes\n            cleaned = cleaned.strip('\"')\n\n            document.update_content(cleaned)\n            document.add_metadata(\"nougat_processed\", True)\n\n            self.logger.info(f\"{document.filename} - Nougat processing completed\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After Nougat processing ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - Nougat processing failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.NougatProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Process Nougat-specific issues in the document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Process Nougat-specific issues in the document.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before Nougat processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in Nougat processing\"\n        )\n        return document\n\n    try:\n        # Apply Nougat postprocessing\n        cleaned = postprocess_single(document.content, markdown_fix=True)\n\n        # Clean LaTeX table formatting\n        cleaned = clean_doubled_backslashes(cleaned)\n\n        # Remove Nougat artifacts\n        cleaned = remove_nougat_artifacts(cleaned)\n\n        # Convert escaped newlines\n        cleaned = cleaned.replace(\"\\\\n\", \"\\n\")\n\n        # Remove surrounding quotes\n        cleaned = cleaned.strip('\"')\n\n        document.update_content(cleaned)\n        document.add_metadata(\"nougat_processed\", True)\n\n        self.logger.info(f\"{document.filename} - Nougat processing completed\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After Nougat processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - Nougat processing failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.RuleBasedProcessor","title":"<code>RuleBasedProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for applying rule-based text corrections.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>class RuleBasedProcessor(TextProcessor):\n    \"\"\"Processor for applying rule-based text corrections.\"\"\"\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Apply rule-based corrections to the document.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before rule-based processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in rule-based processing\"\n            )\n            return document\n\n        try:\n            # Remove single symbol lines\n            cleaned = remove_single_symbol_lines(document.content)\n\n            # Normalize excessive newlines\n            cleaned = normalize_excessive_newlines(cleaned)\n\n            # Trim whitespace\n            cleaned = cleaned.strip()\n\n            document.update_content(cleaned)\n            document.add_metadata(\"rule_based_processed\", True)\n\n            self.logger.info(f\"{document.filename} - Rule-based processing completed\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After rule-based processing ({document.filename}): {document.content[:200]}...\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - Rule-based processing failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.RuleBasedProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Apply rule-based corrections to the document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Apply rule-based corrections to the document.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before rule-based processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in rule-based processing\"\n        )\n        return document\n\n    try:\n        # Remove single symbol lines\n        cleaned = remove_single_symbol_lines(document.content)\n\n        # Normalize excessive newlines\n        cleaned = normalize_excessive_newlines(cleaned)\n\n        # Trim whitespace\n        cleaned = cleaned.strip()\n\n        document.update_content(cleaned)\n        document.add_metadata(\"rule_based_processed\", True)\n\n        self.logger.info(f\"{document.filename} - Rule-based processing completed\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After rule-based processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - Rule-based processing failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.LaTeXProcessor","title":"<code>LaTeXProcessor</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for detecting and correcting LaTeX syntax errors.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>class LaTeXProcessor(TextProcessor):\n    \"\"\"Processor for detecting and correcting LaTeX syntax errors.\"\"\"\n\n    def __init__(\n        self,\n        debug: bool = False,\n        api_key: Optional[str] = None,\n        model: str = \"anthropic/claude-3-haiku\",\n    ):\n        \"\"\"Initialize the LaTeX processor.\n\n        Args:\n            debug: Enable debug output.\n            api_key: OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.\n            model: OpenRouter model to use for corrections.\n        \"\"\"\n        super().__init__(debug=debug)\n        self.api_key = api_key or os.getenv(\"OPENROUTER_API_KEY\")\n        self.model = model\n        self.formula_patterns = get_latex_formula_patterns()\n\n        if not self.api_key and debug:\n            self.logger.warning(\n                \"No OPENROUTER_API_KEY found. LaTeX correction will only detect errors.\"\n            )\n\n    def _extract_latex_formulas(self, content: str) -&gt; List[Tuple[str, str]]:\n        \"\"\"Extract LaTeX formulas from content with their types.\"\"\"\n        formulas = []\n\n        for formula_type, pattern in self.formula_patterns.items():\n            for match in pattern.finditer(content):\n                if formula_type == \"environment\":\n                    env_name = match.group(1)\n                    env_content = match.group(2).strip()\n                    formulas.append(\n                        (\n                            formula_type,\n                            f\"\\\\begin{{{env_name}}}{env_content}\\\\end{{{env_name}}}\",\n                        )\n                    )\n                else:\n                    formulas.append((formula_type, match.group(1).strip()))\n\n        return formulas\n\n    async def _check_formula_syntax(\n        self, formula: str, formula_type: str\n    ) -&gt; Tuple[bool, str]:\n        \"\"\"Check if a LaTeX formula has valid syntax using pylatex and subprocess.\"\"\"\n        try:\n            def check_latex():\n                \"\"\"Run pdflatex compilation in a separate thread.\"\"\"\n                try:\n                    with tempfile.TemporaryDirectory() as tmp_dir:\n                        # Create a pylatex Document\n                        doc = LaTeXDocument(documentclass=\"article\")\n\n                        # Add required packages\n                        doc.packages.append(Package(\"amsmath\"))\n                        doc.packages.append(Package(\"amssymb\"))\n\n                        # Add formula-specific content wrapped in NoEscape\n                        if formula_type == \"inline\":\n                            doc.append(NoEscape(f\"${formula}$\"))\n                        elif formula_type == \"display\":\n                            doc.append(NoEscape(f\"$${formula}$$\"))\n                        elif formula_type == \"bracket\":\n                            doc.append(NoEscape(f\"\\\\({formula}\\\\)\"))\n                        elif formula_type == \"square_bracket\":\n                            doc.append(NoEscape(f\"\\\\[{formula}\\\\]\"))\n                        else:\n                            # Environment type - add extra packages\n                            doc.packages.append(Package(\"multirow\"))\n                            doc.packages.append(Package(\"bm\"))\n                            doc.append(NoEscape(formula))\n\n                        # Generate the .tex file\n                        tex_file = os.path.join(tmp_dir, \"test\")\n                        doc.generate_tex(tex_file)\n\n                        # Run pdflatex using subprocess\n                        result = subprocess.run(\n                            [\"pdflatex\", \"-interaction=nonstopmode\", f\"{tex_file}.tex\"],\n                            cwd=tmp_dir,\n                            stdout=subprocess.PIPE,\n                            stderr=subprocess.PIPE,\n                            timeout=30,\n                        )\n\n                        if result.returncode == 0:\n                            return True, \"Formula syntax is valid\"\n                        else:\n                            # Parse output for error messages\n                            output = result.stdout.decode(\"utf-8\", errors=\"replace\")\n                            error_lines = output.split(\"\\n\")\n                            error_msg = \"Unknown error\"\n                            for i, line in enumerate(error_lines):\n                                if \"! \" in line:\n                                    error_msg = line.strip()\n                                    if (\n                                        i + 1 &lt; len(error_lines)\n                                        and error_lines[i + 1].strip()\n                                    ):\n                                        error_msg += \" \" + error_lines[i + 1].strip()\n                                    break\n                            return False, error_msg\n                except subprocess.TimeoutExpired:\n                    return False, \"PDFLaTeX compilation timed out\"\n                except FileNotFoundError:\n                    return False, \"pdflatex command not found. Please ensure LaTeX is installed.\"\n                except Exception as e:\n                    return False, f\"PDFLaTeX compilation failed: {str(e)}\"\n\n            # Run the blocking operation in a thread pool\n            return await asyncio.get_event_loop().run_in_executor(None, check_latex)\n\n        except Exception as e:\n            return False, f\"Syntax check failed: {str(e)}\"\n\n    def _replace_formula_in_content(\n        self, content: str, original: str, corrected: str, formula_type: str\n    ) -&gt; str:\n        \"\"\"Replace original formula with corrected version in content.\"\"\"\n        try:\n            if formula_type == \"inline\":\n                pattern = re.escape(f\"${original}$\")\n                replacement = f\"${corrected}$\"\n            elif formula_type == \"display\":\n                pattern = re.escape(f\"$${original}$$\")\n                replacement = f\"$${corrected}$$\"\n            elif formula_type == \"bracket\":\n                pattern = re.escape(f\"\\\\({original}\\\\)\")\n                replacement = f\"\\\\({corrected}\\\\)\"\n            elif formula_type == \"square_bracket\":\n                pattern = re.escape(f\"\\\\[{original}\\\\]\")\n                replacement = f\"\\\\[{corrected}\\\\]\"\n            else:\n                pattern = re.escape(original)\n                replacement = corrected\n\n            return re.sub(pattern, replacement, content, count=1)\n        except Exception:\n            return content.replace(original, corrected, 1)\n\n    async def process(self, document: Document) -&gt; Document:\n        \"\"\"Process document to detect and correct LaTeX syntax errors.\"\"\"\n        if self.debug:\n            self.logger.info(\n                f\"Before LaTeX processing ({document.filename}): {document.content[:200]}...\"\n            )\n\n        if document.is_empty():\n            self.logger.warning(\n                f\"{document.filename} - Empty content in LaTeX processing\"\n            )\n            return document\n\n        try:\n            formulas = self._extract_latex_formulas(document.content)\n\n            if not formulas:\n                self.logger.info(f\"{document.filename} - No LaTeX formulas found\")\n                document.add_metadata(\"latex_processed\", True)\n                return document\n\n            errors_found = 0\n            corrections_made = 0\n            modified_content = document.content\n\n            for formula_type, formula in formulas:\n                is_valid, error_message = await self._check_formula_syntax(\n                    formula, formula_type\n                )\n\n                if not is_valid:\n                    errors_found += 1\n                    self.logger.warning(\n                        f\"{document.filename} - Invalid LaTeX formula: {formula[:10]}... Error: {error_message}\"\n                    )\n\n                    if self.api_key:\n                        prompt = get_latex_correction_prompt(\n                            formula_type, error_message, formula, document.content\n                        )\n                        corrected_formula = await make_openrouter_request(\n                            self.api_key, self.model, prompt\n                        )\n\n                        if corrected_formula and corrected_formula != formula:\n                            is_corrected_valid, _ = await self._check_formula_syntax(\n                                corrected_formula, formula_type\n                            )\n\n                            if is_corrected_valid:\n                                modified_content = self._replace_formula_in_content(\n                                    modified_content,\n                                    formula,\n                                    corrected_formula,\n                                    formula_type,\n                                )\n                                corrections_made += 1\n                                self.logger.info(\n                                    f\"{document.filename} - Corrected LaTeX formula: {formula[:30]}... -&gt; {corrected_formula[:30]}...\"\n                                )\n                            else:\n                                self.logger.warning(\n                                    f\"{document.filename} - LLM correction still invalid: {corrected_formula[:50]}...\"\n                                )\n\n            document.update_content(modified_content)\n            document.add_metadata(\"latex_errors_found\", errors_found)\n            document.add_metadata(\"latex_corrections_made\", corrections_made)\n            document.add_metadata(\"latex_processed\", True)\n\n            if errors_found &gt; 0:\n                self.logger.info(\n                    f\"{document.filename} - LaTeX processing complete: {errors_found} errors found, {corrections_made} corrected\"\n                )\n            else:\n                self.logger.info(f\"{document.filename} - All LaTeX formulas are valid\")\n\n            if self.debug:\n                self.logger.info(\n                    f\"After LaTeX processing ({document.filename}): {errors_found} errors, {corrections_made} fixed\"\n                )\n\n            return document\n\n        except Exception as e:\n            self.logger.error(\n                f\"{document.filename} - LaTeX processing failed: {str(e)}\"\n            )\n            return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.LaTeXProcessor.__init__","title":"<code>__init__(debug: bool = False, api_key: Optional[str] = None, model: str = 'anthropic/claude-3-haiku')</code>","text":"<p>Initialize the LaTeX processor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> <code>api_key</code> <code>Optional[str]</code> <p>OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.</p> <code>None</code> <code>model</code> <code>str</code> <p>OpenRouter model to use for corrections.</p> <code>'anthropic/claude-3-haiku'</code> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(\n    self,\n    debug: bool = False,\n    api_key: Optional[str] = None,\n    model: str = \"anthropic/claude-3-haiku\",\n):\n    \"\"\"Initialize the LaTeX processor.\n\n    Args:\n        debug: Enable debug output.\n        api_key: OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.\n        model: OpenRouter model to use for corrections.\n    \"\"\"\n    super().__init__(debug=debug)\n    self.api_key = api_key or os.getenv(\"OPENROUTER_API_KEY\")\n    self.model = model\n    self.formula_patterns = get_latex_formula_patterns()\n\n    if not self.api_key and debug:\n        self.logger.warning(\n            \"No OPENROUTER_API_KEY found. LaTeX correction will only detect errors.\"\n        )\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.LaTeXProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Process document to detect and correct LaTeX syntax errors.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Process document to detect and correct LaTeX syntax errors.\"\"\"\n    if self.debug:\n        self.logger.info(\n            f\"Before LaTeX processing ({document.filename}): {document.content[:200]}...\"\n        )\n\n    if document.is_empty():\n        self.logger.warning(\n            f\"{document.filename} - Empty content in LaTeX processing\"\n        )\n        return document\n\n    try:\n        formulas = self._extract_latex_formulas(document.content)\n\n        if not formulas:\n            self.logger.info(f\"{document.filename} - No LaTeX formulas found\")\n            document.add_metadata(\"latex_processed\", True)\n            return document\n\n        errors_found = 0\n        corrections_made = 0\n        modified_content = document.content\n\n        for formula_type, formula in formulas:\n            is_valid, error_message = await self._check_formula_syntax(\n                formula, formula_type\n            )\n\n            if not is_valid:\n                errors_found += 1\n                self.logger.warning(\n                    f\"{document.filename} - Invalid LaTeX formula: {formula[:10]}... Error: {error_message}\"\n                )\n\n                if self.api_key:\n                    prompt = get_latex_correction_prompt(\n                        formula_type, error_message, formula, document.content\n                    )\n                    corrected_formula = await make_openrouter_request(\n                        self.api_key, self.model, prompt\n                    )\n\n                    if corrected_formula and corrected_formula != formula:\n                        is_corrected_valid, _ = await self._check_formula_syntax(\n                            corrected_formula, formula_type\n                        )\n\n                        if is_corrected_valid:\n                            modified_content = self._replace_formula_in_content(\n                                modified_content,\n                                formula,\n                                corrected_formula,\n                                formula_type,\n                            )\n                            corrections_made += 1\n                            self.logger.info(\n                                f\"{document.filename} - Corrected LaTeX formula: {formula[:30]}... -&gt; {corrected_formula[:30]}...\"\n                            )\n                        else:\n                            self.logger.warning(\n                                f\"{document.filename} - LLM correction still invalid: {corrected_formula[:50]}...\"\n                            )\n\n        document.update_content(modified_content)\n        document.add_metadata(\"latex_errors_found\", errors_found)\n        document.add_metadata(\"latex_corrections_made\", corrections_made)\n        document.add_metadata(\"latex_processed\", True)\n\n        if errors_found &gt; 0:\n            self.logger.info(\n                f\"{document.filename} - LaTeX processing complete: {errors_found} errors found, {corrections_made} corrected\"\n            )\n        else:\n            self.logger.info(f\"{document.filename} - All LaTeX formulas are valid\")\n\n        if self.debug:\n            self.logger.info(\n                f\"After LaTeX processing ({document.filename}): {errors_found} errors, {corrections_made} fixed\"\n            )\n\n        return document\n\n    except Exception as e:\n        self.logger.error(\n            f\"{document.filename} - LaTeX processing failed: {str(e)}\"\n        )\n        return document\n</code></pre>"},{"location":"api/stages/#nougat-helpers","title":"Nougat Helpers","text":""},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers","title":"<code>nougat_helpers</code>","text":"<p>Copyright (c) Meta Platforms, Inc. and affiliates.</p> <p>This source code is licensed under the MIT license found in the LICENSE file in the root directory of this source tree.</p> <p>Script from here - https://github.com/facebookresearch/nougat/blob/main/nougat/postprocessing.py</p>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.markdown_compatible","title":"<code>markdown_compatible(s: str) -&gt; str</code>","text":"<p>Make text compatible with Markdown formatting.</p> <p>This function makes various text formatting adjustments to make it compatible with Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text to be made Markdown-compatible.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Markdown-compatible text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def markdown_compatible(s: str) -&gt; str:\n    \"\"\"\n    Make text compatible with Markdown formatting.\n\n    This function makes various text formatting adjustments to make it compatible with Markdown.\n\n    Args:\n        s (str): The input text to be made Markdown-compatible.\n\n    Returns:\n        str: The Markdown-compatible text.\n    \"\"\"\n    s = re.sub(\n        r\"^\\(([\\d.]+[a-zA-Z]?)\\) \\\\\\[(.+?)\\\\\\]$\", r\"\\[\\2 \\\\tag{\\1}\\]\", s, flags=re.M\n    )\n    s = re.sub(\n        r\"^\\\\\\[(.+?)\\\\\\] \\(([\\d.]+[a-zA-Z]?)\\)$\", r\"\\[\\1 \\\\tag{\\2}\\]\", s, flags=re.M\n    )\n    s = re.sub(\n        r\"^\\\\\\[(.+?)\\\\\\] \\(([\\d.]+[a-zA-Z]?)\\) (\\\\\\[.+?\\\\\\])$\",\n        r\"\\[\\1 \\\\tag{\\2}\\] \\3\",\n        s,\n        flags=re.M,\n    )\n    s = s.replace(r\"\\. \", \". \")\n    s = s.replace(r\"\\.}\", \".}\")\n    s = s.replace(r\"\\. }\", \". }\")\n    s = s.replace(r\"\\.\\]\", \".]\")\n    s = s.replace(r\"\\. ]\", \". ]\")\n    s = re.sub(r\"\\\\begin\\{table\\}\\s*\\\\begin\\{tabular\\}(.*?)\\\\end\\{tabular\\}\\s*\\\\end\\{table\\}\", r\"\\n\\\\begin{table}\\n\\\\begin{tabular}\\1\\\\end{tabular}\\n\\\\end{table}\\n\", s, flags=re.DOTALL)\n\n    s = re.sub(r\"([^\\s])\\$([^\\$]*)\\$\", r\"\\1 $\\2$\", s)\n    s = re.sub(r\"\\$([^\\$]*)\\$([^\\s])\", r\"$\\1$ \\2\", s)\n\n    return s\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.truncate_repetitions","title":"<code>truncate_repetitions(generation: str, score_cutoff: float = 0.5, min_len: int = 30)</code>","text":"<p>Truncate repetitions in the given generation.</p> <p>This function identifies and truncates repetitive content in the text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def truncate_repetitions(generation: str, score_cutoff: float = 0.5, min_len: int = 30):\n    \"\"\"\n    Truncate repetitions in the given generation.\n\n    This function identifies and truncates repetitive content in the text.\n    \"\"\"\n    try:\n        sentences = generation.split(\".\")\n        if len(sentences) &lt; 3:\n            return generation\n\n        to_delete = set()\n        for i in range(len(sentences)):\n            for j in range(i + 1, len(sentences)):\n                sent_i = sentences[i].strip()\n                sent_j = sentences[j].strip()\n\n                if len(sent_i) &lt; min_len or len(sent_j) &lt; min_len:\n                    continue\n\n                if ratio(sent_i, sent_j) &gt; score_cutoff:\n                    to_delete.add(j)\n\n        new_sentences = [sent for i, sent in enumerate(sentences) if i not in to_delete]\n        return \".\".join(new_sentences)\n    except Exception:\n        return generation\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.remove_numbers","title":"<code>remove_numbers(lines: List[str]) -&gt; List[str]</code>","text":"<p>Remove number patterns from lines.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_numbers(lines: List[str]) -&gt; List[str]:\n    \"\"\"Remove number patterns from lines.\"\"\"\n    clean_lines = []\n    for line in lines:\n        clean_line = re.sub(r'\\[\\d+\\]', '', line)\n        clean_line = re.sub(r'\\d+\\.', '', clean_line)\n        clean_lines.append(clean_line.strip())\n    return clean_lines\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.get_slices","title":"<code>get_slices(lines: List[str], clean_lines: List[str]) -&gt; List[slice]</code>","text":"<p>Get slices of potentially hallucinated reference sections.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def get_slices(lines: List[str], clean_lines: List[str]) -&gt; List[slice]:\n    \"\"\"Get slices of potentially hallucinated reference sections.\"\"\"\n    slices = []\n    for i, line in enumerate(lines):\n        if line.strip().lower().startswith('## references'):\n            j = i + 1\n            while j &lt; len(lines) and not lines[j].strip().startswith('##'):\n                j += 1\n            slices.append(slice(i, j))\n    return slices\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.remove_slice_from_lines","title":"<code>remove_slice_from_lines(lines: List[str], clean_lines: List[str], sli: slice) -&gt; str</code>","text":"<p>Remove slice from lines and return the removed text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_slice_from_lines(lines: List[str], clean_lines: List[str], sli: slice) -&gt; str:\n    \"\"\"Remove slice from lines and return the removed text.\"\"\"\n    removed_text = '\\n'.join(lines[sli])\n    return removed_text\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.remove_hallucinated_references","title":"<code>remove_hallucinated_references(text: str) -&gt; str</code>","text":"<p>Remove hallucinated or missing references from the text.</p> <p>This function identifies and removes references that are marked as missing or hallucinated from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing references.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with hallucinated references removed.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_hallucinated_references(text: str) -&gt; str:\n    \"\"\"\n    Remove hallucinated or missing references from the text.\n\n    This function identifies and removes references that are marked as missing or hallucinated\n    from the input text.\n\n    Args:\n        text (str): The input text containing references.\n\n    Returns:\n        str: The text with hallucinated references removed.\n    \"\"\"\n    lines = text.split(\"\\n\")\n    if len(lines) == 0:\n        return \"\"\n    clean_lines = remove_numbers(lines)\n    slices = get_slices(lines, clean_lines)\n    to_delete = []\n    for sli in slices:\n        to_delete.append(remove_slice_from_lines(lines, clean_lines, sli))\n    for to_delete in reversed(to_delete):\n        text = text.replace(to_delete, \"\\n\\n[MISSING_PAGE_POST]\\n\\n\")\n    text = re.sub(\n        r\"## References\\n+\\[MISSING_PAGE_POST(:\\d+)?\\]\",\n        \"\\n\\n[MISSING_PAGE_POST\\\\1]\",\n        text,\n    )\n    return text\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.postprocess_single","title":"<code>postprocess_single(generation: str, markdown_fix: bool = True) -&gt; str</code>","text":"<p>Postprocess a single generated text.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generated text to be postprocessed.</p> required <code>markdown_fix</code> <code>bool</code> <p>Whether to perform Markdown formatting fixes. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The postprocessed text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def postprocess_single(generation: str, markdown_fix: bool = True) -&gt; str:\n    \"\"\"\n    Postprocess a single generated text.\n\n    Args:\n        generation (str): The generated text to be postprocessed.\n        markdown_fix (bool, optional): Whether to perform Markdown formatting fixes. Default is True.\n\n    Returns:\n        str: The postprocessed text.\n    \"\"\"\n    generation = re.sub(\n        r\"(?:\\n|^)#+ \\d*\\W? ?(.{100,})\", r\"\\n\\1\", generation\n    )\n    generation = generation.strip()\n    generation = generation.replace(\"\\n* [leftmargin=*]\\n\", \"\\n\")\n    generation = re.sub(\n        r\"^#+ (?:\\.?(?:\\d|[ixv])+)*\\s*(?:$|\\n\\s*)\", \"\", generation, flags=re.M\n    )\n    lines = generation.split(\"\\n\")\n    if (\n        lines[-1].startswith(\"#\")\n        and lines[-1].lstrip(\"#\").startswith(\" \")\n        and len(lines) &gt; 1\n    ):\n        print(\"INFO: likely hallucinated title at the end of the page: \" + lines[-1])\n        generation = \"\\n\".join(lines[:-1])\n    generation = truncate_repetitions(generation)\n    generation = remove_hallucinated_references(generation)\n    generation = re.sub(\n        r\"^\\* \\[\\d+\\](\\s?[A-W]\\.+\\s?){10,}.*$\", \"\", generation, flags=re.M\n    )\n    generation = re.sub(r\"^(\\* \\[\\d+\\])\\[\\](.*)$\", r\"\\1\\2\", generation, flags=re.M)\n    generation = re.sub(r\"(^\\w\\n\\n|\\n\\n\\w$)\", \"\", generation)\n    generation = re.sub(\n        r\"([\\s.,()])_([a-zA-Z0-9])__([a-zA-Z0-9]){1,3}_([\\s.,:()])\",\n        r\"\\1\\(\\2_{\\3}\\)\\4\",\n        generation,\n    )\n    generation = re.sub(\n        r\"([\\s.,\\d])_([a-zA-Z0-9])_([\\s.,\\d;])\", r\"\\1\\(\\2\\)\\3\", generation\n    )\n    generation = re.sub(\n        r\"(\\nFootnote .*?:) (?:footnotetext|thanks):\\W*(.*(?:\\n\\n|$))\",\n        r\"\\1 \\2\",\n        generation,\n    )\n    generation = re.sub(r\"\\[FOOTNOTE:.+?\\](.*?)\\[ENDFOOTNOTE\\]\", \"\", generation)\n    for match in reversed(\n        list(\n            re.finditer(\n                r\"(?:^)(-|\\*)?(?!-|\\*) ?((?:\\d|[ixv])+ )?.+? (-|\\*) (((?:\\d|[ixv])+)\\.(\\d|[ixv]) )?.*(?:$)\",\n                generation,\n                flags=re.I | re.M,\n            )\n        )\n    ):\n        start, stop = match.span()\n        delim = match.group(3) + \" \"\n        splits = match.group(0).split(delim)\n        replacement = \"\"\n        if match.group(1) is not None:\n            splits = splits[1:]\n            delim1 = match.group(1) + \" \"\n        else:\n            delim1 = \"\"\n            continue\n        pre, post = generation[:start], generation[stop:]\n        for i, item in enumerate(splits):\n            level = 0\n            potential_numeral, _, rest = item.strip().partition(\" \")\n            if not rest:\n                continue\n            if re.match(\n                r\"^[\\dixv]+((?:\\.[\\dixv])?)+$\", potential_numeral, flags=re.I | re.M\n            ):\n                level = potential_numeral.count(\".\")\n\n            replacement += (\n                (\"\\n\" if i &gt; 0 else \"\")\n                + (\"\\t\" * level)\n                + (delim if i &gt; 0 or start == 0 else delim1)\n                + item.strip()\n            )\n        if post == \"\":\n            post = \"\\n\"\n        generation = pre + replacement + post\n\n    if generation.endswith((\".\", \"}\")):\n        generation += \"\\n\\n\"\n    if re.match(r\"[A-Z0-9,;:]$\", generation):\n        generation += \" \"\n    elif generation.startswith((\"#\", \"**\", \"\\\\begin\")):\n        generation = \"\\n\\n\" + generation\n    elif generation.split(\"\\n\")[-1].startswith((\"#\", \"Figure\", \"Table\")):\n        generation = generation + \"\\n\\n\"\n    else:\n        try:\n            last_word = generation.split(\" \")[-1]\n            if last_word in words.words():\n                generation += \" \"\n        except LookupError:\n            generation += \" \"\n            import nltk\n\n            nltk.download(\"words\")\n    for l in generation.split(\"\\n\"):\n        if (\n            l.count(\"\\\\begin{tabular}\") &gt; 15\n            or l.count(\"\\\\multicolumn\") &gt; 60\n            or l.count(\"&amp;\") &gt; 400\n        ):\n            generation = generation.replace(l, \"\")\n    generation = generation.replace(\n        \"\\\\begin{table} \\\\begin{tabular}\", \"\\\\begin{table}\\n\\\\begin{tabular}\"\n    )\n    generation = generation.replace(\n        \"\\\\end{tabular} \\\\end{table}\", \"\\\\end{tabular}\\n\\\\end{table}\"\n    )\n    generation = generation.replace(\"\\\\end{table} Tab\", \"\\\\end{table}\\nTab\")\n    generation = re.sub(r\"(^.+)\\\\begin{tab\", r\"\\1\\n\\\\begin{tab\", generation, flags=re.M)\n\n    generation = generation.replace(\n        r\"\\begin{tabular}{l l}  &amp; \\\\ \\end{tabular}\", \"\"\n    ).replace(\"\\\\begin{tabular}{}\\n\\n\\\\end{tabular}\", \"\")\n    generation = generation.replace(\"\\\\begin{array}[]{\", \"\\\\begin{array}{\")\n    generation = re.sub(\n        r\"\\\\begin{tabular}{([clr ]){2,}}\\s*[&amp; ]*\\s*(\\\\\\\\)? \\\\end{tabular}\",\n        \"\",\n        generation,\n    )\n    generation = re.sub(r\"(\\*\\*S\\. A\\. B\\.\\*\\*\\n+){2,}\", \"\", generation)\n    generation = re.sub(r\"^#+( [\\[\\d\\w])?$\", \"\", generation, flags=re.M)\n    generation = re.sub(r\"^\\.\\s*$\", \"\", generation, flags=re.M)\n    generation = re.sub(r\"\\n{3,}\", \"\\n\\n\", generation)\n    if markdown_fix:\n        return markdown_compatible(generation)\n    else:\n        return generation\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.postprocess","title":"<code>postprocess(generation: Union[str, List[str]], markdown_fix: bool = True) -&gt; Union[str, List[str]]</code>","text":"<p>Postprocess generated text or a list of generated texts.</p> <p>This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>Union[str, List[str]]</code> <p>The generated text or a list of generated texts.</p> required <code>markdown_fix</code> <code>bool</code> <p>Whether to perform Markdown formatting fixes. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>Union[str, List[str]]: The postprocessed text or list of postprocessed texts.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def postprocess(\n    generation: Union[str, List[str]], markdown_fix: bool = True\n) -&gt; Union[str, List[str]]:\n    \"\"\"\n    Postprocess generated text or a list of generated texts.\n\n    This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.\n\n    Args:\n        generation (Union[str, List[str]]): The generated text or a list of generated texts.\n        markdown_fix (bool, optional): Whether to perform Markdown formatting fixes. Default is True.\n\n    Returns:\n        Union[str, List[str]]: The postprocessed text or list of postprocessed texts.\n    \"\"\"\n    if type(generation) == list:\n        if os.environ.get(\"NOUGAT_MULTIPROCESSING\"):\n            with Pool(int(os.environ.get(\"NOUGAT_MULTIPROCESSING\"))) as p:\n                return p.map(\n                    partial(postprocess_single, markdown_fix=markdown_fix), generation\n                )\n        else:\n            return [\n                postprocess_single(s, markdown_fix=markdown_fix) for s in generation\n            ]\n    else:\n        return postprocess_single(generation, markdown_fix=markdown_fix)\n</code></pre>"},{"location":"api/stages/#pii-removal-stage","title":"PII Removal Stage","text":"<p>Removes personally identifiable information from documents.</p>"},{"location":"api/stages/#eve.steps.pii.pii_step","title":"<code>pii_step</code>","text":""},{"location":"api/stages/#eve.steps.pii.pii_step.PiiStep","title":"<code>PiiStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>eve/steps/pii/pii_step.py</code> <pre><code>class PiiStep(PipelineStep):\n\n    async def _remove_pii(\n        self,\n        document: Document,\n        entities: Optional[List[str]] = None,\n        threshold: float = 0.35,\n        return_analysis: bool = False,\n        url: str = None,\n    ) -&gt; Document:\n        \"\"\"Make a call to the litserve API and remove PII (async with aiohttp).\"\"\"\n\n        if not url:\n            self.logger.error(\"No URL provided for PII service\")\n            return document\n\n        if entities is None:\n            entities = [\"PERSON\", \"EMAIL_ADDRESS\"]\n\n        payload = {\n            \"text\": document.content,\n            \"entities\": entities,\n            \"score_threshold\": threshold,\n            \"return_analysis\": return_analysis,\n        }\n\n        try:\n            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=120)) as session:\n                async with session.post(\n                    url,\n                    json = payload,\n                    headers={\"Content-Type\": \"application/json\"},\n                ) as response:\n                    response.raise_for_status()\n                    result = await response.json()\n                    self.logger.debug(f\"PII API result: {result}\")\n                    document.content = result.get(\"anonymized_text\", document.content)\n                    return document\n\n        except aiohttp.ClientError as e:\n            self.logger.error(f\"PII API request failed: {e}\")\n            return document\n\n    async def remove_pii(\n        self,\n        document: Document,\n        entities: Optional[List[str]] = None,\n        threshold: float = 0.35,\n        return_analysis: bool = False,\n        url: str = None,\n    ) -&gt; Document:\n        return await self._remove_pii(document, entities, threshold, return_analysis, url)\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        base_url = self.config.get(\"url\")\n        if not base_url:\n            self.logger.error(\"No URL provided for PII service\")\n            return []\n\n        url = f\"{base_url}/predict\"\n\n        # Run all requests concurrently\n        tasks = [self.remove_pii(document, url=url) for document in documents]\n        results = await asyncio.gather(*tasks, return_exceptions = True)\n\n        final = []\n        for doc in results:\n            if doc and getattr(doc, \"content_length\", 0) &gt; 1:\n                final.append(doc)\n                self.logger.info(f\"Successfully anonymized {doc.filename}\")\n\n        return final\n</code></pre>"},{"location":"api/stages/#metadata-extraction-stage","title":"Metadata Extraction Stage","text":"<p>Extracts structured metadata from documents.</p>"},{"location":"api/stages/#eve.steps.metadata.metadata_step","title":"<code>metadata_step</code>","text":"<p>Metadata extraction step for the EVE pipeline.</p>"},{"location":"api/stages/#eve.steps.metadata.metadata_step.MetadataStep","title":"<code>MetadataStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Metadata extraction step that extracts metadata from PDF and HTML documents.</p> Source code in <code>eve/steps/metadata/metadata_step.py</code> <pre><code>class MetadataStep(PipelineStep):\n    \"\"\"\n    Metadata extraction step that extracts metadata from PDF and HTML documents.\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"\n        Initialize the metadata extraction step.\n\n        Args:\n            config: Configuration dictionary with options:\n\n                - enabled_formats: List of file formats to process (pdf, html, txt, md)\n                - fallback_to_filename: Use filename as title fallback\n                - debug: Enable debug logging\n                - export_metadata: Whether to export metadata to JSON file\n                - metadata_destination: Directory to save metadata file\n                - metadata_filename: Name of the metadata JSON file\n                  Note: Text formats (txt, md) automatically enable this feature\n        \"\"\"\n        super().__init__(config)\n\n        self.enabled_formats = set(config.get(\"enabled_formats\", [\"pdf\", \"html\", \"txt\", \"md\"]))\n        self.fallback_to_filename = config.get(\"fallback_to_filename\", True)\n        self.export_metadata = config.get(\"export_metadata\", True)\n        self.metadata_destination = Path(config.get(\"metadata_destination\", \"./output\"))\n        self.metadata_filename = config.get(\"metadata_filename\", \"metadata.jsonl\")\n\n        self.extractors = {\n            \"pdf\": PdfMetadataExtractor(debug=self.debug),\n            \"html\": HtmlMetadataExtractor(debug=self.debug)\n        }\n\n    async def _extract_metadata_for_document(self, document: Document) -&gt; Document:\n        \"\"\"\n        Extract metadata for a single document using appropriate extractor.\n        Args:\n            document: Document to extract metadata from\n\n        Returns:\n            Document with metadata added to the metadata field:\n            - extracted_title: Document title\n            - extracted_authors: List of authors\n            - extracted_year: Publication year\n            - extracted_metadata: Full metadata dictionary\n            - extraction_error: Error message (if extraction failed)\n        \"\"\"\n        if document.file_format not in self.enabled_formats:\n            self.logger.debug(f\"Skipping metadata extraction for unsupported format: {document.file_format}\")\n            return document\n\n        metadata = None\n\n        try:\n            if document.file_format in [\"txt\", \"md\"]:\n                if not document.content.strip() and document.file_path.exists():\n                    try:\n                        with open(document.file_path, 'r', encoding='utf-8') as f:\n                            document.update_content(f.read())\n                        self.logger.info(f\"Loaded content for text file {document.filename}: {len(document.content)} characters\")\n                    except Exception as e:\n                        self.logger.error(f\"Failed to load content for {document.filename}: {str(e)}\")\n                        return document\n\n            elif document.file_format in self.extractors:\n                extractor = self.extractors[document.file_format]\n                metadata = await extractor.extract_metadata(document)\n            else:\n                self.logger.warning(f\"No extractor available for format: {document.file_format}\")\n\n            if metadata:\n                document.add_metadata(\"extracted_metadata\", metadata)\n\n                self.logger.info(f\"Successfully extracted metadata from {document.filename}\")\n                if self.debug:\n                    self.logger.debug(f\"Extracted metadata keys: {list(metadata.keys())}\")\n            else:\n                self.logger.warning(f\"No metadata extracted from {document.filename}\")\n\n                if self.fallback_to_filename:\n                    title = document.file_path.stem.replace(\"_\", \" \").replace(\"-\", \" \")\n                    document.add_metadata(\"title\", title)\n                    self.logger.info(f\"Using filename as title fallback: {title}\")\n\n        except Exception as e:\n            self.logger.error(f\"Failed to extract metadata from {document.filename}: {str(e)}\")\n\n            if self.fallback_to_filename:\n                title = document.file_path.stem.replace(\"_\", \" \").replace(\"-\", \" \")\n                document.add_metadata(\"title\", title)\n            document.add_metadata(\"extraction_error\", str(e))\n\n        return document\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"\n        Execute metadata extraction on input documents.\n\n        Args:\n            documents: List of Document objects to extract metadata from\n\n        Returns:\n            List of Document objects with metadata added\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No input documents provided to metadata step\")\n            return []\n\n        supported_documents = [\n            doc for doc in documents \n            if doc.file_format in self.enabled_formats\n        ]\n\n        unsupported_documents = [\n            doc for doc in documents \n            if doc.file_format not in self.enabled_formats\n        ]\n\n        if unsupported_documents:\n            self.logger.info(f\"Skipping {len(unsupported_documents)} documents with unsupported formats\")\n\n        if not supported_documents:\n            self.logger.warning(\"No documents with supported formats found\")\n            return documents\n\n        self.logger.info(f\"Extracting metadata from {len(supported_documents)} documents\")\n\n        tasks = [\n            self._extract_metadata_for_document(document) \n            for document in supported_documents\n        ]\n\n        processed_supported = await asyncio.gather(*tasks, return_exceptions=True)\n\n        result_supported = []\n        for i, result in enumerate(processed_supported):\n            if isinstance(result, Exception):\n                self.logger.error(f\"Exception processing {supported_documents[i].filename}: {result}\")\n                result_supported.append(supported_documents[i])\n            else:\n                result_supported.append(result)\n\n        final_result = result_supported + unsupported_documents\n\n        successful_count = sum(1 for doc in result_supported if doc.get_metadata(\"extracted_metadata\"))\n        self.logger.info(f\"Successfully extracted metadata from {successful_count}/{len(supported_documents)} supported documents\")\n\n        if self.export_metadata:\n            await self._export_metadata_to_json(final_result)\n\n        return final_result\n\n    async def _export_metadata_to_json(self, documents: List[Document]) -&gt; None:\n        \"\"\"\n        Export extracted metadata to a JSON file.\n\n        Args:\n            documents: List of processed documents with metadata\n        \"\"\"\n        if not self.metadata_destination.exists():\n            self.logger.info(f\"Creating metadata destination directory: {self.metadata_destination}\")\n            self.metadata_destination.mkdir(parents=True, exist_ok=True)\n\n        metadata_file = self.metadata_destination / self.metadata_filename\n\n        for document in documents:\n            doc_metadata = {\n                \"filename\": document.filename,\n                \"file_path\": str(document.file_path),\n                \"file_format\": document.file_format,\n                \"content_length\": document.content_length,\n                \"has_extracted_metadata\": bool(document.get_metadata(\"extracted_metadata\"))\n            }\n            if document.metadata:\n                if document.file_format == 'pdf': # TO-DO, hacky fix, find a more cleaner solution\n                    file_id = document.filename.removesuffix(\".pdf\")\n                    document.metadata = {\n                        \"extracted_metadata\": {\n                            file_id: document.metadata[\"extracted_metadata\"].get(file_id)\n                        }\n                    }\n                for key, value in document.metadata.items():\n                    doc_metadata[key] = value\n\n            metadata_file = self.metadata_destination / self.metadata_filename\n\n            try:\n                with open(metadata_file, 'a', encoding='utf-8') as f:\n                    json.dump(doc_metadata, f, ensure_ascii=False, default=str)\n                    f.write('\\n')\n\n                self.logger.info(f\"Exported metadata to: {metadata_file}\")\n                self.logger.info(f\"Metadata exported for {len(documents)} documents ({sum(1 for doc in documents if doc.get_metadata('extracted_metadata'))} with extracted metadata)\")\n\n            except Exception as e:\n                self.logger.error(f\"Failed to export metadata to {metadata_file}: {str(e)}\")\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.metadata_step.MetadataStep.__init__","title":"<code>__init__(config: dict)</code>","text":"<p>Initialize the metadata extraction step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with options:</p> <ul> <li>enabled_formats: List of file formats to process (pdf, html, txt, md)</li> <li>fallback_to_filename: Use filename as title fallback</li> <li>debug: Enable debug logging</li> <li>export_metadata: Whether to export metadata to JSON file</li> <li>metadata_destination: Directory to save metadata file</li> <li>metadata_filename: Name of the metadata JSON file   Note: Text formats (txt, md) automatically enable this feature</li> </ul> required Source code in <code>eve/steps/metadata/metadata_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"\n    Initialize the metadata extraction step.\n\n    Args:\n        config: Configuration dictionary with options:\n\n            - enabled_formats: List of file formats to process (pdf, html, txt, md)\n            - fallback_to_filename: Use filename as title fallback\n            - debug: Enable debug logging\n            - export_metadata: Whether to export metadata to JSON file\n            - metadata_destination: Directory to save metadata file\n            - metadata_filename: Name of the metadata JSON file\n              Note: Text formats (txt, md) automatically enable this feature\n    \"\"\"\n    super().__init__(config)\n\n    self.enabled_formats = set(config.get(\"enabled_formats\", [\"pdf\", \"html\", \"txt\", \"md\"]))\n    self.fallback_to_filename = config.get(\"fallback_to_filename\", True)\n    self.export_metadata = config.get(\"export_metadata\", True)\n    self.metadata_destination = Path(config.get(\"metadata_destination\", \"./output\"))\n    self.metadata_filename = config.get(\"metadata_filename\", \"metadata.jsonl\")\n\n    self.extractors = {\n        \"pdf\": PdfMetadataExtractor(debug=self.debug),\n        \"html\": HtmlMetadataExtractor(debug=self.debug)\n    }\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.metadata_step.MetadataStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute metadata extraction on input documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document objects to extract metadata from</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with metadata added</p> Source code in <code>eve/steps/metadata/metadata_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Execute metadata extraction on input documents.\n\n    Args:\n        documents: List of Document objects to extract metadata from\n\n    Returns:\n        List of Document objects with metadata added\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No input documents provided to metadata step\")\n        return []\n\n    supported_documents = [\n        doc for doc in documents \n        if doc.file_format in self.enabled_formats\n    ]\n\n    unsupported_documents = [\n        doc for doc in documents \n        if doc.file_format not in self.enabled_formats\n    ]\n\n    if unsupported_documents:\n        self.logger.info(f\"Skipping {len(unsupported_documents)} documents with unsupported formats\")\n\n    if not supported_documents:\n        self.logger.warning(\"No documents with supported formats found\")\n        return documents\n\n    self.logger.info(f\"Extracting metadata from {len(supported_documents)} documents\")\n\n    tasks = [\n        self._extract_metadata_for_document(document) \n        for document in supported_documents\n    ]\n\n    processed_supported = await asyncio.gather(*tasks, return_exceptions=True)\n\n    result_supported = []\n    for i, result in enumerate(processed_supported):\n        if isinstance(result, Exception):\n            self.logger.error(f\"Exception processing {supported_documents[i].filename}: {result}\")\n            result_supported.append(supported_documents[i])\n        else:\n            result_supported.append(result)\n\n    final_result = result_supported + unsupported_documents\n\n    successful_count = sum(1 for doc in result_supported if doc.get_metadata(\"extracted_metadata\"))\n    self.logger.info(f\"Successfully extracted metadata from {successful_count}/{len(supported_documents)} supported documents\")\n\n    if self.export_metadata:\n        await self._export_metadata_to_json(final_result)\n\n    return final_result\n</code></pre>"},{"location":"api/stages/#metadata-extractors","title":"Metadata Extractors","text":""},{"location":"api/stages/#html-metadata-extractor","title":"HTML Metadata Extractor","text":""},{"location":"api/stages/#eve.steps.metadata.extractors.html_extractor","title":"<code>html_extractor</code>","text":""},{"location":"api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor","title":"<code>HtmlMetadataExtractor</code>","text":"<p>Metadata extractor for HTML files and web pages.</p> Source code in <code>eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>class HtmlMetadataExtractor():\n    \"\"\"\n    Metadata extractor for HTML files and web pages.\n    \"\"\"\n\n    def __init__(self, debug: bool = False):\n        \"\"\"\n        Initialize the HTML metadata extractor.\n\n        The HTML extractor relies on regex patterns defined in eve.common.regex_patterns\n        for parsing HTML content efficiently without requiring a full HTML parser.\n\n        Args:\n            debug: Enable debug logging for detailed extraction information\n        \"\"\"\n        self.debug = debug\n        self.logger = get_logger(self.__class__.__name__)\n\n    def _clean_title(self, title: str) -&gt; Optional[str]:\n        \"\"\"\n        Clean and normalize a title string.\n\n        Args:\n            title: Raw title string from extracted metadata\n\n        Returns:\n            Cleaned title string, or None if title is invalid\n        \"\"\"\n        if not title or not isinstance(title, str):\n            return None\n\n        # Remove leading/trailing whitespace\n        cleaned = title.strip()\n\n        # Convert newlines and carriage returns to spaces\n        cleaned = cleaned.replace('\\n', ' ').replace('\\r', ' ')\n\n        # Collapse multiple spaces into single spaces\n        while '  ' in cleaned:\n            cleaned = cleaned.replace('  ', ' ')\n\n        # Filter out invalid titles\n        if len(cleaned) &lt; 3 or cleaned.isdigit():\n            return None\n\n        return cleaned\n\n    def _extract_content_with_tags(self, document: Document) -&gt; Optional[str]:\n        with open(document.file_path, 'r', encoding = 'utf-8') as file:\n            html_content = file.read()\n\n        document.content = html_content\n        return document\n\n    def _extract_title_from_html(self, html_content: str) -&gt; Optional[str]:\n        \"\"\"\n        Extract title from HTML &lt;title&gt; tag using regex patterns.\n\n        Args:\n            html_content: Raw HTML content as string\n\n        Returns:\n            Cleaned title string from &lt;title&gt; tag, or None if not found/invalid\n        \"\"\"\n        # Use regex pattern to extract title content\n        title = extract_html_title(html_content)\n\n        if title:\n            # Apply standard title cleaning (whitespace, length validation, etc.)\n            cleaned_title = self._clean_title(title)\n\n            if cleaned_title:\n                self.logger.debug(f\"Extracted title from HTML: {cleaned_title}\")\n                return cleaned_title\n\n        return None\n\n    async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract metadata from an HTML document using multi-source approach.\n\n        Args:\n            document: HTML document to extract metadata from\n\n        Returns:\n            Dictionary containing extracted metadata with fields:\n            - title: Page title (from various sources, with title_source indicator)\n            - title_source: Source of title ('html_tag', 'meta_tag', 'filename')\n            - url: Source URL if available\n            - domain: Domain name from URL\n            - scheme: URL scheme (http/https)\n            - content_length: Length of HTML content\n            - has_content: Boolean indicating content exists\n            - extraction_methods: List containing 'html_parsing'\n\n            Returns None if document format is invalid\n        \"\"\"\n\n        metadata = {}\n\n        document = self._extract_content_with_tags(document) # do this because extraction from previous step removes tag\n\n        extracted_title = self._extract_title_from_html(document.content)\n        metadata['title'] = extracted_title\n        metadata['content_length'] = len(document.content)\n\n        return metadata\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor.__init__","title":"<code>__init__(debug: bool = False)</code>","text":"<p>Initialize the HTML metadata extractor.</p> <p>The HTML extractor relies on regex patterns defined in eve.common.regex_patterns for parsing HTML content efficiently without requiring a full HTML parser.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug logging for detailed extraction information</p> <code>False</code> Source code in <code>eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"\n    Initialize the HTML metadata extractor.\n\n    The HTML extractor relies on regex patterns defined in eve.common.regex_patterns\n    for parsing HTML content efficiently without requiring a full HTML parser.\n\n    Args:\n        debug: Enable debug logging for detailed extraction information\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor.extract_metadata","title":"<code>extract_metadata(document: Document) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Extract metadata from an HTML document using multi-source approach.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>HTML document to extract metadata from</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing extracted metadata with fields:</p> <code>Optional[Dict[str, Any]]</code> <ul> <li>title: Page title (from various sources, with title_source indicator)</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>title_source: Source of title ('html_tag', 'meta_tag', 'filename')</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>url: Source URL if available</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>domain: Domain name from URL</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>scheme: URL scheme (http/https)</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>content_length: Length of HTML content</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>has_content: Boolean indicating content exists</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>extraction_methods: List containing 'html_parsing'</li> </ul> <code>Optional[Dict[str, Any]]</code> <p>Returns None if document format is invalid</p> Source code in <code>eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract metadata from an HTML document using multi-source approach.\n\n    Args:\n        document: HTML document to extract metadata from\n\n    Returns:\n        Dictionary containing extracted metadata with fields:\n        - title: Page title (from various sources, with title_source indicator)\n        - title_source: Source of title ('html_tag', 'meta_tag', 'filename')\n        - url: Source URL if available\n        - domain: Domain name from URL\n        - scheme: URL scheme (http/https)\n        - content_length: Length of HTML content\n        - has_content: Boolean indicating content exists\n        - extraction_methods: List containing 'html_parsing'\n\n        Returns None if document format is invalid\n    \"\"\"\n\n    metadata = {}\n\n    document = self._extract_content_with_tags(document) # do this because extraction from previous step removes tag\n\n    extracted_title = self._extract_title_from_html(document.content)\n    metadata['title'] = extracted_title\n    metadata['content_length'] = len(document.content)\n\n    return metadata\n</code></pre>"},{"location":"api/stages/#pdf-metadata-extractor","title":"PDF Metadata Extractor","text":""},{"location":"api/stages/#eve.steps.metadata.extractors.pdf_extractor","title":"<code>pdf_extractor</code>","text":"<p>PDF metadata extractor is a two stage process.</p> <ol> <li>Extract content using monkeyocr.</li> <li>Use crossref to extract metadata from the content.</li> </ol>"},{"location":"api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor","title":"<code>PdfMetadataExtractor</code>","text":"Source code in <code>eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>class PdfMetadataExtractor():\n\n    def __init__(self, debug: bool = False):\n        \"\"\"\n        Initialize the PDF metadata extractor.\n\n        Args:\n            debug: Enable debug logging for detailed extraction information\n        \"\"\"\n        self.debug = debug\n        self.logger = get_logger(self.__class__.__name__)\n\n    @staticmethod\n    def _safe_str(value):\n        if isinstance(value, list):\n            return value[0] if value else None\n        if isinstance(value, dict):\n            return json.dumps(value)\n        return str(value) if value not in (None, \"\", []) else None\n\n    @staticmethod\n    def _extract_identifier(main_dir, sub_dir):\n        md_path = f\"{main_dir}/{sub_dir}/{sub_dir}.md\"\n        try:\n            with open(md_path, 'r', encoding=\"utf-8\", errors=\"ignore\") as f:\n                content = f.read()\n        except Exception:\n            return \"NA\", None\n\n        for pattern in doi_regexp:\n            match = re.findall(pattern, content, re.I)\n            if match:\n                return \"doi\", match[0]\n\n        for pattern in arxiv_regexp:\n            match = re.findall(pattern, content, re.I)\n            if match:\n                return \"arxiv\", match[0]\n\n        for pattern in isbn_regexp:\n            match = re.findall(pattern, content, re.I)\n            if match:\n                return \"isbn\", match[0]\n\n        return \"NA\", None\n\n    @staticmethod\n    def _extract_title(main_dir, sub_dir):\n        json_path = os.path.join(main_dir, sub_dir, f\"{sub_dir}_content_list.json\")\n        try:\n            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n                data = json.load(f)\n            candidates = [row[\"text\"] for row in data if row.get(\"text_level\", 0) == 1]\n            return max(candidates, key=len) if candidates else \"NA\"\n        except Exception:\n            return \"NA\"\n\n    @staticmethod\n    async def _fetch_json(client, url):\n        try:\n            resp = await client.get(url, timeout=50) # \n            resp.raise_for_status()\n            return resp.json()\n        except Exception:\n            return None\n\n    async def _fetch_crossref_by_doi(self, client, doi):\n        data = await self._fetch_json(client, f\"https://api.crossref.org/works/{doi}\")\n        if not data:\n            return None\n        item = data.get(\"message\", {})\n\n        return {\n            \"title\": self._safe_str(item.get(\"title\")),\n            \"authors\": \", \".join(\n                f\"{a.get('given', '')} {a.get('family', '')}\".strip()\n                for a in item.get(\"author\", [])\n                if isinstance(a, dict)\n            ) or None,\n            \"year\": self._safe_str(item.get(\"issued\", {}).get(\"date-parts\", [[None]])[0]),\n            \"publisher\": self._safe_str(item.get(\"publisher\")),\n            \"journal\": self._safe_str(item.get(\"container-title\")),\n            \"pub_url\": self._safe_str(item.get(\"URL\")),\n            \"doi\": self._safe_str(item.get(\"DOI\")),\n            \"citation_count\": item.get(\"is-referenced-by-count\")\n        }\n\n\n    async def _fetch_crossref_by_title(self, client, title):\n        q = re.sub(r\"[\\s]+\", \"+\", title)\n        data = await self._fetch_json(client, f\"https://api.crossref.org/works?query.bibliographic={q}&amp;rows=1\")\n        if not data:\n            return None\n        items = data.get(\"message\", {}).get(\"items\", [])\n        if not items:\n            return None\n        item = items[0]\n\n        return {\n            \"title\": self._safe_str(item.get(\"title\")),\n            \"authors\": \", \".join(\n                f\"{a.get('given', '')} {a.get('family', '')}\".strip()\n                for a in item.get(\"author\", [])\n                if isinstance(a, dict)\n            ) or None,\n            \"year\": self._safe_str(item.get(\"issued\", {}).get(\"date-parts\", [[None]])[0]),\n            \"publisher\": self._safe_str(item.get(\"publisher\")),\n            \"journal\": self._safe_str(item.get(\"container-title\")),\n            \"pub_url\": self._safe_str(item.get(\"URL\")),\n            \"doi\": self._safe_str(item.get(\"DOI\")),\n        }\n\n\n    async def fetch_doi_from_arxiv(self, client, arxiv_id):\n        data = await self._fetch_json(client, f\"https://api.crossref.org/works?query={arxiv_id}\")\n        if not data:\n            return None\n        items = data.get(\"message\", {}).get(\"items\", [])\n        return self._safe_str(items[0].get(\"DOI\")) if items else None\n\n    async def _extract_metadata(self, sub_dir, main_dir, client, sem):\n        async with sem:\n            id_type, identifier = self._extract_identifier(main_dir, sub_dir)\n            meta = None\n\n            if id_type == \"doi\":\n                meta = await self._fetch_crossref_by_doi(client, identifier)\n            elif id_type == \"arxiv\":\n                doi = await self._fetch_doi_from_arxiv(client, identifier)\n                if doi:\n                    meta = await self._fetch_crossref_by_doi(client, doi)\n\n            title = self._extract_title(main_dir, sub_dir)\n            if not meta and title != \"NA\":\n                meta = await self._fetch_crossref_by_title(client, title)\n\n            return {\n                \"sub_dir\": sub_dir,\n                \"id_type\": id_type,\n                \"identifier\": identifier,\n                \"title_extracted\": title,\n                \"meta\": meta,\n            }\n\n    async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"\n        Extract metadata from a PDF document using crossref.\n\n        Args:\n            document: PDF document to extract metadata from\n\n        Returns:\n            Dictionary containing extracted metadata with fields:\n            - title: Document title\n            - authors: List of author names \n            - year: Publication year\n            - journal: Publication venue\n            - doi: Digital Object Identifier\n            - identifier: Document identifier (DOI, arXiv, etc.)\n\n            Returns None if document format is invalid\n        \"\"\"\n\n        metadata = {}\n        done = set()\n\n        subdirs = [d for d in os.listdir(main_dir)\n                if os.path.isdir(os.path.join(main_dir, d)) and d not in done]\n\n        if not subdirs:\n            print(\"All subdirectories already processed.\")\n            return\n\n        sem = asyncio.Semaphore(MAX_CONCURRENT)\n        async with httpx.AsyncClient() as client:\n            tasks = [self._extract_metadata(d, main_dir, client, sem) for d in subdirs]\n\n            for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n                try:\n                    r = await coro\n                    metadata[r.get(\"sub_dir\")] = r.get(\"meta\")\n                except Exception as e:\n                    print(f\"Failed on {coro}: {e}\")\n\n        return metadata\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor.__init__","title":"<code>__init__(debug: bool = False)</code>","text":"<p>Initialize the PDF metadata extractor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug logging for detailed extraction information</p> <code>False</code> Source code in <code>eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"\n    Initialize the PDF metadata extractor.\n\n    Args:\n        debug: Enable debug logging for detailed extraction information\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor.extract_metadata","title":"<code>extract_metadata(document: Document) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Extract metadata from a PDF document using crossref.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>PDF document to extract metadata from</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing extracted metadata with fields:</p> <code>Optional[Dict[str, Any]]</code> <ul> <li>title: Document title</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>authors: List of author names </li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>year: Publication year</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>journal: Publication venue</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>doi: Digital Object Identifier</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>identifier: Document identifier (DOI, arXiv, etc.)</li> </ul> <code>Optional[Dict[str, Any]]</code> <p>Returns None if document format is invalid</p> Source code in <code>eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract metadata from a PDF document using crossref.\n\n    Args:\n        document: PDF document to extract metadata from\n\n    Returns:\n        Dictionary containing extracted metadata with fields:\n        - title: Document title\n        - authors: List of author names \n        - year: Publication year\n        - journal: Publication venue\n        - doi: Digital Object Identifier\n        - identifier: Document identifier (DOI, arXiv, etc.)\n\n        Returns None if document format is invalid\n    \"\"\"\n\n    metadata = {}\n    done = set()\n\n    subdirs = [d for d in os.listdir(main_dir)\n            if os.path.isdir(os.path.join(main_dir, d)) and d not in done]\n\n    if not subdirs:\n        print(\"All subdirectories already processed.\")\n        return\n\n    sem = asyncio.Semaphore(MAX_CONCURRENT)\n    async with httpx.AsyncClient() as client:\n        tasks = [self._extract_metadata(d, main_dir, client, sem) for d in subdirs]\n\n        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n            try:\n                r = await coro\n                metadata[r.get(\"sub_dir\")] = r.get(\"meta\")\n            except Exception as e:\n                print(f\"Failed on {coro}: {e}\")\n\n    return metadata\n</code></pre>"},{"location":"api/stages/#export-stage","title":"Export Stage","text":"<p>Saves processed documents to output formats.</p>"},{"location":"api/stages/#eve.steps.export.export_step","title":"<code>export_step</code>","text":""},{"location":"api/stages/#eve.steps.export.export_step.ExportStep","title":"<code>ExportStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>eve/steps/export/export_step.py</code> <pre><code>class ExportStep(PipelineStep):\n\n    def __init__(self, config: dict, name: str = \"ExportStep\"):\n        \"\"\"Initialize the export step.\n\n        Args:\n            config: Configuration containing:\n\n                - output_dir: Output directory path\n                - format: Output format (jsonl, md, etc.)\n                - resume: Whether to enable resume functionality (default: False)\n            name: Name for logging purposes\n        \"\"\"\n        super().__init__(config, name)\n\n        # Initialize checkpoint manager if resume is enabled\n        self.resume = config.get(\"resume\", False)\n        output_dir = Path(config.get(\"output_dir\", \"./output\"))\n\n        if self.resume:\n            self.checkpoint = CheckpointManager(output_dir, resume=True)\n            stats = self.checkpoint.get_stats()\n            self.logger.info(f\"Resume mode enabled: {stats['processed_count']} documents already processed\")\n        else:\n            self.checkpoint = None\n\n    async def export_jsonl(self, documents: List[Document]) -&gt; List[Document]:\n        output_dir = Path(self.config.get(\"output_dir\", \"./output\"))\n        result = []\n\n        if not output_dir.exists():\n            self.logger.info(f\"{output_dir} does not exist. creating...\")\n            output_dir.mkdir(parents=True, exist_ok=True)\n\n        for document in documents:\n            output_file = (\n                output_dir\n                / f\"{Path(document.filename).stem}.{self.config.get('format', 'md')}\"\n            )\n            async with aiofiles.open(output_file, \"a+\", encoding=\"utf-8\") as f:\n                await f.write(json.dumps(document.__dict__()))\n                await f.write(\"\\n\")\n\n            # Mark as processed in checkpoint\n            if self.checkpoint:\n                self.checkpoint.mark_processed(document)\n\n            result.append(document)\n        return result\n\n    async def export_md(self, documents: List[Document]) -&gt; List[Document]:\n        output_dir = Path(self.config.get(\"output_dir\", \"./output\"))\n        result = []\n        if not output_dir.exists():\n            self.logger.info(f\"{output_dir} does not exist. creating...\")\n            output_dir.mkdir(parents=True, exist_ok=True)\n        for document in documents:\n            output_file = (\n                output_dir\n                / f\"{Path(document.filename).stem}.{self.config.get('format', 'md')}\"\n            )\n            async with aiofiles.open(output_file, \"a+\", encoding=\"utf-8\") as f:\n                await f.write(json.dumps(document.content))\n                await f.write(\"\\n\")\n            self.logger.info(f\"Saved file: {output_file}\")\n\n            # Mark as processed in checkpoint\n            if self.checkpoint:\n                self.checkpoint.mark_processed(document)\n\n            result.append(document)\n        return result\n\n    async def dummy_export(self, documents: List[Document]) -&gt; List[Document]:\n        return documents\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        # Note: Documents are already filtered in create_batches() when resume is enabled\n        # No need to filter again here to avoid memory overhead\n\n        format = self.config.get(\"format\", \"md\")\n        if format == \"jsonl\":\n            result = await self.export_jsonl(documents)\n        elif format == \"dummy\":\n            result = await self.dummy_export(documents)\n        else:\n            result = await self.export_md(documents)\n\n        return result\n</code></pre>"},{"location":"api/stages/#eve.steps.export.export_step.ExportStep.__init__","title":"<code>__init__(config: dict, name: str = 'ExportStep')</code>","text":"<p>Initialize the export step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration containing:</p> <ul> <li>output_dir: Output directory path</li> <li>format: Output format (jsonl, md, etc.)</li> <li>resume: Whether to enable resume functionality (default: False)</li> </ul> required <code>name</code> <code>str</code> <p>Name for logging purposes</p> <code>'ExportStep'</code> Source code in <code>eve/steps/export/export_step.py</code> <pre><code>def __init__(self, config: dict, name: str = \"ExportStep\"):\n    \"\"\"Initialize the export step.\n\n    Args:\n        config: Configuration containing:\n\n            - output_dir: Output directory path\n            - format: Output format (jsonl, md, etc.)\n            - resume: Whether to enable resume functionality (default: False)\n        name: Name for logging purposes\n    \"\"\"\n    super().__init__(config, name)\n\n    # Initialize checkpoint manager if resume is enabled\n    self.resume = config.get(\"resume\", False)\n    output_dir = Path(config.get(\"output_dir\", \"./output\"))\n\n    if self.resume:\n        self.checkpoint = CheckpointManager(output_dir, resume=True)\n        stats = self.checkpoint.get_stats()\n        self.logger.info(f\"Resume mode enabled: {stats['processed_count']} documents already processed\")\n    else:\n        self.checkpoint = None\n</code></pre>"},{"location":"api/utilities/","title":"Utilities","text":"<p>This section covers utility functions and helper modules used throughout the pipeline.</p>"},{"location":"api/utilities/#common-utilities","title":"Common Utilities","text":"<p>General-purpose utility functions.</p>"},{"location":"api/utilities/#eve.utils","title":"<code>utils</code>","text":""},{"location":"api/utilities/#eve.utils.read_in_chunks","title":"<code>read_in_chunks(file_path: Path, mode: str, chunk_size: int = 4096) -&gt; AsyncGenerator[bytes, None]</code>  <code>async</code>","text":"<p>read a binary file in chunks.</p> Source code in <code>eve/utils.py</code> <pre><code>async def read_in_chunks(file_path: Path, mode: str, chunk_size: int = 4096) -&gt; AsyncGenerator[bytes, None]:\n    \"\"\"\n    read a binary file in chunks.\n    \"\"\"\n    async with aiofiles.open(file_path, mode) as f:\n        while chunk := await f.read(chunk_size):\n            yield chunk\n</code></pre>"},{"location":"api/utilities/#http-utils","title":"HTTP Utils","text":"<p>HTTP client utilities for server-based processing.</p>"},{"location":"api/utilities/#eve.common.http_utils","title":"<code>http_utils</code>","text":"<p>Common HTTP utilities for making API calls across the pipeline.</p>"},{"location":"api/utilities/#eve.common.http_utils.post_request","title":"<code>post_request(url: str, headers: Dict[str, str], data: Dict[str, Any], timeout: int = 30) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Make an async POST request and return JSON response.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to make the request to</p> required <code>headers</code> <code>Dict[str, str]</code> <p>Request headers</p> required <code>data</code> <code>Dict[str, Any]</code> <p>Request data to send as JSON</p> required <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Response JSON data or None if request failed</p> Source code in <code>eve/common/http_utils.py</code> <pre><code>async def post_request(\n    url: str,\n    headers: Dict[str, str],\n    data: Dict[str, Any],\n    timeout: int = 30\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Make an async POST request and return JSON response.\n\n    Args:\n        url: The URL to make the request to\n        headers: Request headers\n        data: Request data to send as JSON\n        timeout: Request timeout in seconds\n\n    Returns:\n        Response JSON data or None if request failed\n    \"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url, \n                headers=headers, \n                json=data,\n                timeout=aiohttp.ClientTimeout(total=timeout)\n            ) as response:\n                if response.status == 200:\n                    return await response.json()\n                else:\n                    logger.error(f\"HTTP request failed with status {response.status}\")\n                    return None\n    except Exception as e:\n        logger.error(f\"HTTP request failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/utilities/#eve.common.http_utils.make_openrouter_request","title":"<code>make_openrouter_request(api_key: str, model: str, prompt: str, max_tokens: int = 1000, temperature: float = 0.1) -&gt; Optional[str]</code>  <code>async</code>","text":"<p>Make a request to OpenRouter API for LLM completion.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenRouter API key</p> required <code>model</code> <code>str</code> <p>Model name to use</p> required <code>prompt</code> <code>str</code> <p>The prompt to send</p> required <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>1000</code> <code>temperature</code> <code>float</code> <p>Temperature for response generation</p> <code>0.1</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Response content or None if request failed</p> Source code in <code>eve/common/http_utils.py</code> <pre><code>async def make_openrouter_request(\n    api_key: str,\n    model: str,\n    prompt: str,\n    max_tokens: int = 1000,\n    temperature: float = 0.1\n) -&gt; Optional[str]:\n    \"\"\"\n    Make a request to OpenRouter API for LLM completion.\n\n    Args:\n        api_key: OpenRouter API key\n        model: Model name to use\n        prompt: The prompt to send\n        max_tokens: Maximum tokens in response\n        temperature: Temperature for response generation\n\n    Returns:\n        Response content or None if request failed\n    \"\"\"\n    url = \"https://openrouter.ai/api/v1/chat/completions\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    data = {\n        \"model\": model,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature\n    }\n\n    response = await post_request(url, headers, data)\n    if response and \"choices\" in response:\n        content = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        import re\n        content = re.sub(r'^```latex\\n?', '', content)\n        content = re.sub(r'\\n?```$', '', content)\n        return content.strip()\n\n    return None\n</code></pre>"},{"location":"api/utilities/#regex-patterns","title":"Regex Patterns","text":"<p>Common regular expression patterns used throughout the pipeline.</p>"},{"location":"api/utilities/#eve.common.regex_patterns","title":"<code>regex_patterns</code>","text":"<p>Common regex patterns used across the pipeline.</p>"},{"location":"api/utilities/#eve.common.regex_patterns.get_latex_formula_patterns","title":"<code>get_latex_formula_patterns() -&gt; dict[str, Pattern[str]]</code>","text":"<p>Get all LaTeX formula patterns in a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Pattern[str]]</code> <p>Dictionary mapping pattern names to compiled regex patterns</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def get_latex_formula_patterns() -&gt; dict[str, Pattern[str]]:\n    \"\"\"\n    Get all LaTeX formula patterns in a dictionary.\n\n    Returns:\n        Dictionary mapping pattern names to compiled regex patterns\n    \"\"\"\n    return {\n        'inline': INLINE_MATH_PATTERN,\n        'display': DISPLAY_MATH_PATTERN,\n        'bracket': BRACKET_MATH_PATTERN,\n        'square_bracket': SQUARE_BRACKET_MATH_PATTERN,\n        'environment': LATEX_ENV_PATTERN\n    }\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.clean_doubled_backslashes","title":"<code>clean_doubled_backslashes(text: str) -&gt; str</code>","text":"<p>Clean up doubled backslashes in LaTeX content.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def clean_doubled_backslashes(text: str) -&gt; str:\n    \"\"\"Clean up doubled backslashes in LaTeX content.\"\"\"\n    return DOUBLED_BACKSLASH_PATTERN.sub(lambda m: '\\\\' * (len(m.group()) // 2), text)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.normalize_excessive_newlines","title":"<code>normalize_excessive_newlines(text: str) -&gt; str</code>","text":"<p>Replace 3+ consecutive newlines with exactly 2.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def normalize_excessive_newlines(text: str) -&gt; str:\n    \"\"\"Replace 3+ consecutive newlines with exactly 2.\"\"\"\n    return EXCESSIVE_NEWLINES_PATTERN.sub('\\n\\n', text)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.remove_single_symbol_lines","title":"<code>remove_single_symbol_lines(text: str) -&gt; str</code>","text":"<p>Remove lines that contain only a single symbol or punctuation.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def remove_single_symbol_lines(text: str) -&gt; str:\n    \"\"\"Remove lines that contain only a single symbol or punctuation.\"\"\"\n    lines = text.split('\\n')\n    cleaned_lines = []\n\n    for line in lines:\n        stripped = line.strip()\n        if re.search(r'\\w', stripped) or len(stripped) != 1:\n            cleaned_lines.append(line)\n\n    return '\\n'.join(cleaned_lines)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.fix_ocr_digit_letter_spacing","title":"<code>fix_ocr_digit_letter_spacing(text: str) -&gt; str</code>","text":"<p>Fix OCR issues where digits are concatenated with letters.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def fix_ocr_digit_letter_spacing(text: str) -&gt; str:\n    \"\"\"Fix OCR issues where digits are concatenated with letters.\"\"\"\n    return DIGIT_LETTER_PATTERN.sub(r'\\1 \\2', text)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.remove_nougat_artifacts","title":"<code>remove_nougat_artifacts(text: str) -&gt; str</code>","text":"<p>Remove Nougat-specific warning and error artifacts.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def remove_nougat_artifacts(text: str) -&gt; str:\n    \"\"\"Remove Nougat-specific warning and error artifacts.\"\"\"\n    text = WARNING_PATTERN.sub('', text)\n    text = ERROR_PATTERN.sub('', text)\n    text = text.replace('[MISSING_PAGE_POST]', '')\n    return text\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.extract_html_title","title":"<code>extract_html_title(html_content: str) -&gt; str</code>","text":"<p>Extract title from HTML content.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>str</code> <p>Extracted and cleaned title, or None if not found</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def extract_html_title(html_content: str) -&gt; str:\n    \"\"\"\n    Extract title from HTML content.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Extracted and cleaned title, or None if not found\n    \"\"\"\n    if not html_content:\n        return None\n\n    title_match = HTML_TITLE_PATTERN.search(html_content)\n\n    if title_match:\n        title = title_match.group(1)\n\n        title = HTML_TAG_PATTERN.sub('', title)\n        title = HTML_ENTITY_PATTERN.sub(' ', title)\n        title = HTML_NUMERIC_ENTITY_PATTERN.sub(' ', title)\n\n        return title.strip()\n\n    return None\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.extract_html_meta_tags","title":"<code>extract_html_meta_tags(html_content: str) -&gt; dict[str, str]</code>","text":"<p>Extract metadata from HTML meta tags.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary containing extracted meta tag information</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def extract_html_meta_tags(html_content: str) -&gt; dict[str, str]:\n    \"\"\"\n    Extract metadata from HTML meta tags.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Dictionary containing extracted meta tag information\n    \"\"\"\n    meta_data = {}\n\n    if not html_content:\n        return meta_data\n\n    meta_patterns = {\n        'description': r'&lt;meta[^&gt;]*name=[\"\\']description[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'keywords': r'&lt;meta[^&gt;]*name=[\"\\']keywords[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'author': r'&lt;meta[^&gt;]*name=[\"\\']author[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'og_title': r'&lt;meta[^&gt;]*property=[\"\\']og:title[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'og_description': r'&lt;meta[^&gt;]*property=[\"\\']og:description[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'twitter_title': r'&lt;meta[^&gt;]*name=[\"\\']twitter:title[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n    }\n\n    for key, pattern in meta_patterns.items():\n        match = re.search(pattern, html_content, re.IGNORECASE)\n        if match:\n            value = match.group(1).strip()\n            if value:\n                meta_data[key] = value\n\n    return meta_data\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.extract_json_ld_count","title":"<code>extract_json_ld_count(html_content: str) -&gt; int</code>","text":"<p>Count JSON-LD structured data blocks in HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of JSON-LD script blocks found</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def extract_json_ld_count(html_content: str) -&gt; int:\n    \"\"\"\n    Count JSON-LD structured data blocks in HTML.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Number of JSON-LD script blocks found\n    \"\"\"\n    if not html_content:\n        return 0\n\n    json_ld_matches = JSON_LD_SCRIPT_PATTERN.findall(html_content)\n    return len(json_ld_matches)\n</code></pre>"},{"location":"api/utilities/#prompts","title":"Prompts","text":"<p>Prompt templates used in LLM-based processing.</p>"},{"location":"api/utilities/#eve.common.prompts","title":"<code>prompts</code>","text":"<p>Common prompts used across the pipeline.</p>"},{"location":"api/utilities/#eve.common.prompts.get_latex_correction_prompt","title":"<code>get_latex_correction_prompt(formula_type: str, error_message: str, formula: str, context: str) -&gt; str</code>","text":"<p>Generate a LaTeX correction prompt.</p> <p>Parameters:</p> Name Type Description Default <code>formula_type</code> <code>str</code> <p>Type of LaTeX formula (inline, display, etc.)</p> required <code>error_message</code> <code>str</code> <p>The error message from LaTeX compilation</p> required <code>formula</code> <code>str</code> <p>The problematic formula</p> required <code>context</code> <code>str</code> <p>Surrounding context for better understanding</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt string</p> Source code in <code>eve/common/prompts.py</code> <pre><code>def get_latex_correction_prompt(\n    formula_type: str,\n    error_message: str,\n    formula: str,\n    context: str\n) -&gt; str:\n    \"\"\"\n    Generate a LaTeX correction prompt.\n\n    Args:\n        formula_type: Type of LaTeX formula (inline, display, etc.)\n        error_message: The error message from LaTeX compilation\n        formula: The problematic formula\n        context: Surrounding context for better understanding\n\n    Returns:\n        Formatted prompt string\n    \"\"\"\n    context_snippet = context[:1000] + \"...\" if len(context) &gt; 1000 else context\n\n    return LATEX_CORRECTION_PROMPT.format(\n        formula_type=formula_type,\n        error_message=error_message,\n        formula=formula,\n        context_snippet=context_snippet\n    )\n</code></pre>"},{"location":"api/utilities/#logging","title":"Logging","text":"<p>Logging configuration and utilities.</p>"},{"location":"api/utilities/#eve.logging","title":"<code>logging</code>","text":""},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>This section provides practical examples of using the EVE Pipeline for common document processing tasks.</p>"},{"location":"examples/basic-usage/#simple-document-processing","title":"Simple Document Processing","text":"<p>Process all documents from an input directory and export to markdown:</p> <pre><code># config.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_documents\"\n  stages:\n    - name: extraction\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"output\"\n</code></pre> <pre><code># Run the pipeline\neve run\n</code></pre>"},{"location":"examples/basic-usage/#pdf-processing-pipeline","title":"PDF Processing Pipeline","text":"<p>Process PDF documents with cleaning and deduplication:</p> <pre><code>pipeline:\n  batch_size: 5\n  inputs:\n    path: \"research_papers\"\n  stages:\n    - name: extraction\n      config:\n        format: \"pdf\"\n    - name: duplication\n    - name: cleaning\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"processed_papers\"\n</code></pre>"},{"location":"examples/basic-usage/#web-content-processing","title":"Web Content Processing","text":"<p>Process HTML documents with PII removal:</p> <pre><code>pipeline:\n  batch_size: 20\n  inputs:\n    path: \"web_pages\"\n  stages:\n    - name: extraction\n      config:\n        format: \"html\"\n    - name: pii\n      config:\n        url: \"http://127.0.0.1:8000\"\n    - name: export\n      config:\n        format: \"txt\"\n        destination: \"clean_content\"\n</code></pre>"},{"location":"examples/basic-usage/#advanced-pipeline-with-all-features","title":"Advanced Pipeline with All Features","text":"<p>Complete pipeline for scientific document processing:</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"scientific_documents\"\n  stages:\n    - name: extraction\n      config:\n        url: \"http://127.0.0.1:8001\"\n    - name: duplication\n      config:\n        method: \"lsh\"\n        shingle_size: 3\n        num_perm: 128\n        threshold: 0.85\n    - name: cleaning\n      config:\n        ocr_threshold: 0.99\n        enable_latex_correction: true\n        debug: true\n    - name: pii\n      config:\n        url: \"http://127.0.0.1:8000\"\n    - name: metadata\n      config:\n    - name: export\n      config:\n        export_metadata: true\n        metadata_destination: \"./output\"\n</code></pre>"},{"location":"examples/basic-usage/#mixed-format-processing","title":"Mixed Format Processing","text":"<p>Process different document types in the same pipeline:</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"mixed_documents\"\n  stages:\n    - name: extraction\n      # Auto-detect format based on file extension\n    - name: duplication\n      config:\n        method: \"lsh\"\n        threshold: 0.8\n    - name: cleaning\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"unified_output\"\n</code></pre>"},{"location":"examples/basic-usage/#process-and-upload-to-qdrant","title":"Process and Upload to Qdrant","text":"<p>This example demonstrates a complete pipeline that extracts, chunks, filters, embeds, and uploads documents to Qdrant in one workflow.</p> <p>Prerequisites: - VLLM server running for embeddings: <code>python server/vllm.py</code> - Qdrant instance running: <code>docker run -p 6333:6333 qdrant/qdrant</code></p>"},{"location":"examples/basic-usage/#understanding-jsonl-input-format","title":"Understanding JSONL Input Format","text":"<p>The pipeline accepts JSONL (JSON Lines) files where each line is a JSON document. The extractor recognizes the following fields:</p> <p>Required Fields: - <code>content</code> (string): The document text content</p> <p>Optional Fields: - <code>metadata</code> (object): Custom metadata that will be preserved throughout the pipeline - <code>embedding</code> (array): Pre-computed embedding vector (if using <code>use_existing_embeddings: true</code>) - <code>pipeline_metadata</code> (object): Internal metadata from previous pipeline runs</p> <p>Example JSONL file: <pre><code>{\"content\": \"This is the first document.\", \"metadata\": {\"title\": \"Doc 1\", \"author\": \"John Doe\", \"year\": 2024}}\n{\"content\": \"This is the second document.\", \"metadata\": {\"title\": \"Doc 2\", \"source\": \"research_paper.pdf\"}}\n{\"content\": \"Third document with embedding.\", \"metadata\": {\"title\": \"Doc 3\"}, \"embedding\": [0.123, 0.456, ...]}\n</code></pre></p> <p>Important Notes: - Each line must be valid JSON - The <code>content</code> field is required; documents without it will be skipped - Metadata fields are completely flexible - you can include any custom fields - When chunks are created, they inherit all metadata from the original document - Chunking adds a <code>headers</code> field to metadata containing markdown header hierarchy</p> <pre><code># examples/process_and_upload.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"data/doc_w_metadata.jsonl\"\n\n  stages:\n    # Step 1: Extract content from documents\n    - name: extraction\n      config: { format: \"jsonl\" }\n\n    # Step 2: Chunk documents into semantic pieces\n    - name: chunker\n      config: {\n        \"chunk_overlap\": 0,\n        \"max_chunk_size\": 512,\n        \"word_overlap\": 0,\n        \"add_headers\": false,\n        \"merge_small_chunks\": true,\n        \"headers_to_split_on\": [ 1, 2, 3, 4, 5, 6 ]\n      }\n\n    # Step 3: Remove short chunks (&lt; 40 words)\n    - name: length_filter\n      config:\n        length: 40\n        comparison: \"greater\"\n        action: \"keep\"\n\n    # Step 4: Remove long chunks (&gt;= 1024 words)\n    - name: length_filter\n      config:\n        length: 1024\n        comparison: \"less\"\n        action: \"keep\"\n\n    # Step 5: Remove references and acknowledgements\n    - name: reference_filter\n      config:\n        action: \"discard\"\n\n    # Step 6: PII filter with threshold\n    - name: pii_filter\n      config:\n        threshold: 0.03\n        action: \"discard\"\n        apply_filter: true\n\n    # Step 7: Remove chunks with excessive newlines\n    - name: newline_filter\n      config:\n        chunks: 60\n        comparison: \"less\"\n        action: \"keep\"\n\n    # Step 8: Generate embeddings and upload to Qdrant\n    - name: qdrant_upload\n      config:\n        mode: \"qdrant\"\n        use_existing_embeddings: false\n        upload_pipeline_metadata: true\n\n        embedder:\n          model_name: \"Qwen/Qwen3-Embedding-4B\"\n          url: 'http://0.0.0.0:8000'\n          timeout: 300\n          api_key: \"EMPTY\"\n\n        vector_store:\n          batch_size: 1000\n          collection_name: \"your-collection-name\"\n          vector_size: 2560\n          url: \"http://localhost:6333\"\n          api_key: \"your-api-key\"\n</code></pre> <p>To run: <pre><code>cp examples/process_and_upload.yaml config.yaml\n# Edit config.yaml to set your Qdrant collection name, URL, and API key\neve run\n</code></pre></p> <p>What this pipeline does: 1. Extracts content from JSONL documents (preserves all metadata from input) 2. Splits documents into chunks of up to 512 words    - Each chunk inherits all metadata from the original document    - Chunking adds a <code>headers</code> field to metadata with markdown header hierarchy 3. Filters chunks by length (40-1024 words) 4. Removes references and acknowledgements sections 5. Filters out chunks with PII above 3% threshold 6. Removes chunks with excessive newlines 7. Generates embeddings using VLLM server 8. Uploads filtered documents with embeddings to Qdrant    - Includes original metadata from JSONL input    - Includes chunk headers    - Includes filter statistics (if <code>upload_pipeline_metadata: true</code>)</p> <p>Metadata Flow Example:</p> <pre><code>Input JSONL:\n{\"content\": \"# Introduction\\n\\nThis is my paper...\", \"metadata\": {\"title\": \"My Paper\", \"author\": \"Jane Doe\"}}\n\nAfter Chunking:\nDocument 1: {\"content\": \"This is my paper...\", \"metadata\": {\"title\": \"My Paper\", \"author\": \"Jane Doe\", \"headers\": [\"#Introduction\"]}}\n\nAfter Upload to Qdrant:\nAll chunks retain: title=\"My Paper\", author=\"Jane Doe\", headers=[\"#Introduction\"], plus any filter metadata\n</code></pre>"},{"location":"examples/basic-usage/#selective-stage-processing","title":"Selective Stage Processing","text":"<p>Skip certain stages based on your needs:</p>"},{"location":"examples/basic-usage/#extraction-only","title":"Extraction Only","text":"<pre><code>pipeline:\n  batch_size: 20\n  inputs:\n    path: \"raw_documents\"\n  stages:\n    - name: extraction\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"extracted_content\"\n</code></pre>"},{"location":"examples/basic-usage/#deduplication-only","title":"Deduplication Only","text":"<pre><code>pipeline:\n  inputs:\n    path: \"markdown_documents\"\n  stages:\n    - name: duplication\n      config:\n        method: \"lsh\"\n        threshold: 0.9\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"unique_documents\"\n</code></pre>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":"<p>This guide covers the main configuration options for the EVE Pipeline. You can find the detailed configurations under each <code>Pipeline Stage</code>.</p>"},{"location":"getting-started/configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The pipeline is configured using a YAML file (typically <code>config.yaml</code>) with the following structure:</p> <pre><code>pipeline:\n  batch_size: integer\n  inputs:\n    path: string\n    # ... other input options\n  stages:\n    - name: string\n      config: object\n    # ... more stages\n</code></pre>"},{"location":"getting-started/configuration/#global-configuration","title":"Global Configuration","text":""},{"location":"getting-started/configuration/#batch_size","title":"batch_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>10</code></li> <li>Description: Number of documents to process in each batch</li> <li>Note: Not applicable to deduplication stage</li> </ul> <pre><code>pipeline:\n  batch_size: 20\n</code></pre>"},{"location":"getting-started/configuration/#inputs","title":"inputs","text":""},{"location":"getting-started/configuration/#path","title":"path","text":"<ul> <li>Type: String</li> <li>Required: Yes</li> <li>Description: Path to input directory or file containing documents</li> <li>Supported Formats: Directories with PDF/HTML/XML/Markdown files, or JSONL files</li> </ul> <pre><code>pipeline:\n  inputs:\n    path: \"input_documents\"  # Directory with various document formats\n    # OR\n    path: \"data/documents.jsonl\"  # JSONL file with structured data\n</code></pre>"},{"location":"getting-started/configuration/#using-jsonl-input-files","title":"Using JSONL Input Files","text":"<p>JSONL (JSON Lines) format is a powerful way to provide pre-structured documents with metadata. Each line in the file must be a valid JSON object.</p> <p>Required Fields: - <code>content</code> (string): The document text content</p> <p>Optional Fields: - <code>metadata</code> (object): Custom metadata preserved throughout the pipeline - <code>embedding</code> (array): Pre-computed embedding vector - <code>pipeline_metadata</code> (object): Internal metadata from previous pipeline runs</p> <p>Example JSONL file (<code>documents.jsonl</code>): <pre><code>{\"content\": \"First document text here.\", \"metadata\": {\"title\": \"Document 1\", \"author\": \"John Doe\", \"year\": 2024}}\n{\"content\": \"Second document text.\", \"metadata\": {\"title\": \"Document 2\", \"source\": \"research.pdf\", \"tags\": [\"AI\", \"ML\"]}}\n{\"content\": \"Document with embedding.\", \"metadata\": {\"title\": \"Doc 3\"}, \"embedding\": [0.123, 0.456, 0.789, ...]}\n</code></pre></p> <p>Key Benefits: - Metadata Preservation: All metadata fields are preserved throughout the pipeline - Metadata Inheritance: When chunking, each chunk inherits the original document's metadata - Pre-computed Embeddings: Can include embeddings to skip re-computation - Flexible Schema: Add any custom metadata fields you need</p> <p>Usage Example: <pre><code>pipeline:\n  inputs:\n    path: \"data/papers.jsonl\"\n  stages:\n    - name: extraction\n      config: { format: \"jsonl\" }\n    - name: chunker\n      config: { max_chunk_size: 512 }\n    # Chunks will have metadata: {\"title\": \"...\", \"author\": \"...\", \"year\": ..., \"headers\": [...]}\n</code></pre></p>"},{"location":"getting-started/configuration/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"getting-started/configuration/#extraction-stage","title":"Extraction Stage","text":"<p>Extracts content from various document formats.</p> <pre><code>- name: extraction\n  config:\n    format: \"\"  # or \"pdf\", \"html\", \"xml\", \"markdown\", \"jsonl\"\n    url: \"http://127.0.0.1:8001\"  # for server-based extraction\n</code></pre>"},{"location":"getting-started/configuration/#options","title":"Options","text":"<ul> <li> <p>format: Document format specification</p> <ul> <li><code>\"\"</code> (default): Automatically detect format</li> <li><code>\"pdf\"</code>: PDF documents</li> <li><code>\"html\"</code>: HTML documents</li> <li><code>\"xml\"</code>: XML documents</li> <li><code>\"markdown\"</code>: Markdown documents</li> <li><code>\"jsonl\"</code>: JSON Lines format (one JSON object per line)</li> </ul> </li> <li> <p>url: Server URL for nougat extraction (required for PDF format)</p> </li> </ul> <p>Note on JSONL Format: When using JSONL input, each line must be a valid JSON object with a required <code>content</code> field. See the JSONL Input Files section above for detailed format specifications and examples.</p>"},{"location":"getting-started/configuration/#deduplication-stage","title":"Deduplication Stage","text":"<p>Removes duplicate and near-duplicate documents.</p> <pre><code>- name: duplication\n  config:\n    method: \"exact\"  # or \"lsh\"\n    # LSH options (when method: \"lsh\")\n    shingle_size: 3\n    num_perm: 128\n    threshold: 0.8\n</code></pre>"},{"location":"getting-started/configuration/#options_1","title":"Options","text":"<ul> <li> <p>method: Deduplication method</p> <ul> <li><code>\"exact\"</code> (default): Exact hash-based deduplication</li> <li><code>\"lsh\"</code>: Locality Sensitive Hashing for near-duplicates</li> </ul> </li> </ul>"},{"location":"getting-started/configuration/#lsh-options","title":"LSH Options","text":"<ul> <li>shingle_size: Size of text shingles (default: <code>3</code>)</li> <li>num_perm: Number of permutations (default: <code>128</code>)</li> <li>threshold: Similarity threshold (default: <code>0.8</code>)</li> </ul>"},{"location":"getting-started/configuration/#cleaning-stage","title":"Cleaning Stage","text":"<p>Removes noise and improves document quality.</p> <pre><code>- name: cleaning\n  config:\n    ocr_threshold: 0.9\n    min_words: 2\n    enable_latex_correction: True\n</code></pre>"},{"location":"getting-started/configuration/#options_2","title":"Options","text":"<ul> <li>ocr_threshold: OCR duplicate threshold (default: <code>0.99</code>)</li> <li>min_words: Minimum words for processing (default: <code>2</code>)</li> <li>enable_latex_correction: Use LLM to fix latex formulas and tables (default: <code>false</code>)</li> </ul>"},{"location":"getting-started/configuration/#pii-removal-stage","title":"PII Removal Stage","text":"<p>Redacts personally identifiable information.</p> <pre><code>- name: pii\n  config:\n    url: \"http://127.0.0.1:8000\"\n</code></pre>"},{"location":"getting-started/configuration/#options_3","title":"Options","text":"<ul> <li>url: Presidio server URL </li> </ul>"},{"location":"getting-started/configuration/#export-stage","title":"Export Stage","text":"<p>Saves processed documents to output.</p> <pre><code>- name: export\n  config:\n    format: \"md\"  # or \"txt\", \"json\"\n    destination: \"output\"\n</code></pre>"},{"location":"getting-started/configuration/#options_4","title":"Options","text":"<ul> <li> <p>format: Output format</p> <ul> <li><code>\"md\"</code> (default): Markdown</li> <li><code>\"txt\"</code>: Plain text</li> <li><code>\"json\"</code>: JSON with metadata</li> </ul> </li> <li> <p>destination: Output directory path</p> </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install and set up the EVE Pipeline on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10 or higher</li> <li>uv (recommended) or pip for package management</li> </ul>"},{"location":"getting-started/installation/#install-uv-recommended","title":"Install uv (Recommended)","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-using-uv-recommended","title":"Method 1: Using uv (Recommended)","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/eve-esa/eve-pipeline.git\ncd eve-pipeline\n</code></pre> </li> <li> <p>Install dependencies</p> <pre><code>uv sync\n</code></pre> </li> </ol>"},{"location":"getting-started/installation/#method-2-using-pip","title":"Method 2: Using pip","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/eve-esa/eve-pipeline.git\ncd eve-pipeline\n</code></pre> </li> <li> <p>Create a virtual environment</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre> </li> <li> <p>Install dependencies</p> <pre><code>pip install -r requirements.txt\npip install -e .\n</code></pre> </li> </ol>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#server-setup","title":"Server Setup","text":"<p>Some pipeline stages require external servers:</p>"},{"location":"getting-started/installation/#pii-server","title":"PII Server","text":"<p>For PII (Personally Identifiable Information) removal:</p> <pre><code>cd server\npython3 pii_server.py\n</code></pre>"},{"location":"getting-started/installation/#ocr-server","title":"OCR Server","text":"<pre><code>cd server\npython3 nougat_server.py\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation, proceed to the Quick Start guide to learn how to configure and run your first pipeline.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This tutorial will walk you through running your first data processing pipeline with EVE.</p>"},{"location":"getting-started/quick-start/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>Create an input directory with your documents:</p> <pre><code>mkdir -p input_data\n# Copy your PDF, HTML, XML, or Markdown files here\ncp /path/to/your/documents/* input_data/\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Create a <code>config.yaml</code> file:</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_data\"\n  stages:\n    - name: extraction\n      # Automatically detects file format\n    - name: duplication\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"output\"\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline","text":"<p>Execute the pipeline:</p> <pre><code>eve run\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-check-results","title":"Step 4: Check Results","text":"<p>Your processed documents will be in the <code>output</code> directory:</p> <pre><code>ls output/\n</code></pre>"},{"location":"getting-started/quick-start/#example-pipeline-configurations","title":"Example Pipeline Configurations","text":""},{"location":"getting-started/quick-start/#pdf-processing-only","title":"PDF Processing Only","text":"<pre><code>pipeline:\n  batch_size: 5\n  inputs:\n    path: \"pdfs\"\n  stages:\n    - name: extraction\n      config: { format: \"pdf\" }\n    - name: cleaning\n    - name: export\n      config: { format: \"md\", destination: \"processed_pdfs\" }\n</code></pre>"},{"location":"getting-started/quick-start/#html-processing-with-pii-removal","title":"HTML Processing with PII Removal","text":"<pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"html_docs\"\n  stages:\n    - name: extraction\n      config: { format: \"html\", url: \"http://127.0.0.1:8001\" }\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: export\n      config: { format: \"md\"}\n</code></pre>"},{"location":"getting-started/quick-start/#advanced-pipeline-with-all-stages","title":"Advanced Pipeline with All Stages","text":"<pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"mixed_docs\"\n  stages:\n    - name: extraction\n      config: { url: \"http://127.0.0.1:8001\" }\n    - name: duplication\n      config: {\n        method: \"lsh\",\n        shingle_size: 3,\n        num_perm: 128,\n        threshold: 0.8\n      }\n    - name: cleaning\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: metadata\n</code></pre>"},{"location":"getting-started/quick-start/#monitoring-progress","title":"Monitoring Progress","text":"<p>The pipeline provides progress updates:</p> <pre><code>$ eve run\n\n[2024-01-15 10:30:00] INFO: Starting pipeline with 100 documents\n[2024-01-15 10:30:01] INFO: Stage 1/5: Extraction\n[2024-01-15 10:30:15] INFO: Processing batch 1/10 (10 documents)\n[2024-01-15 10:30:30] INFO: Processing batch 2/10 (20 documents)\n...\n[2024-01-15 10:35:00] INFO: Pipeline completed successfully\n[2024-01-15 10:35:00] INFO: Processed 95 documents, 5 duplicates removed\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about configuration options</li> </ul>"},{"location":"pipeline-stages/chunking/","title":"Chunking","text":"<p>The chunking stage splits large documents into smaller, semantically meaningful chunks that are suitable for downstream processing like embedding generation and vector database upload.</p>"},{"location":"pipeline-stages/chunking/#overview","title":"Overview","text":"<p>Chunking is essential for:</p> <ul> <li>Vector database upload: Breaking documents into appropriately-sized pieces for embedding</li> <li>Semantic retrieval: Creating chunks that represent coherent topics or concepts</li> <li>Context window management: Ensuring chunks fit within model token limits</li> <li>Performance optimization: Parallelizing processing across multiple chunks</li> </ul> <p>The Eve pipeline uses a sophisticated two-step chunking strategy that preserves document structure and special content:</p> <ol> <li>Header-based splitting: First splits documents by Markdown headers to maintain semantic structure</li> <li>Sentence-based splitting: If sections exceed the size limit, further splits them by sentences</li> <li>Smart merging: Optionally merges small chunks back together when they share compatible heading levels</li> <li>Content preservation: Keeps LaTeX formulas, equations, and tables intact as atomic units</li> </ol>"},{"location":"pipeline-stages/chunking/#features","title":"Features","text":"<ul> <li>Semantic chunking: Respects document structure by splitting on Markdown headers</li> <li>LaTeX preservation: Keeps mathematical formulas and equations together</li> <li>Table preservation: Maintains tables as complete units without splitting</li> <li>Configurable overlap: Add word-based overlap between chunks for better retrieval</li> <li>Parallel processing: Uses multiprocessing for fast chunking of large document sets</li> <li>Header inclusion: Optionally adds section headers to chunks for context</li> </ul>"},{"location":"pipeline-stages/chunking/#configuration","title":"Configuration","text":"<p>Step name: <code>chunker</code></p>"},{"location":"pipeline-stages/chunking/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>max_chunk_size</code> int No <code>512</code> Maximum size of any chunk in words <code>chunk_overlap</code> int No <code>0</code> Number of characters to overlap between chunks during secondary splitting <code>word_overlap</code> int No <code>0</code> Number of words to overlap between chunks (takes precedence over chunk_overlap) <code>add_headers</code> bool No <code>False</code> Whether to prepend section headers to chunk content <code>merge_small_chunks</code> bool No <code>True</code> Whether to merge small chunks that share compatible heading levels <code>headers_to_split_on</code> list[int] No <code>[1, 2, 3, 4, 5, 6]</code> Markdown header levels to split on (1=<code>#</code>, 2=<code>##</code>, etc.) <code>max_workers</code> int No <code>None</code> Number of parallel workers (None = CPU count)"},{"location":"pipeline-stages/chunking/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 512\n    add_headers: true\n    merge_small_chunks: true\n</code></pre>"},{"location":"pipeline-stages/chunking/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 1024\n    chunk_overlap: 0\n    word_overlap: 50\n    add_headers: true\n    merge_small_chunks: true\n    headers_to_split_on: [1, 2, 3]  # Only split on H1, H2, H3\n    max_workers: 8  # Use 8 parallel workers\n</code></pre>"},{"location":"pipeline-stages/chunking/#how-it-works","title":"How It Works","text":""},{"location":"pipeline-stages/chunking/#two-step-chunking-strategy","title":"Two-Step Chunking Strategy","text":""},{"location":"pipeline-stages/chunking/#step-1-header-based-splitting","title":"Step 1: Header-Based Splitting","text":"<p>The chunker first splits the document based on Markdown headers:</p> <pre><code># Introduction\nThis is the introduction text...\n\n## Background\nThis is the background section...\n\n## Methods\nThis section describes methods...\n</code></pre> <p>This creates initial chunks at natural document boundaries.</p>"},{"location":"pipeline-stages/chunking/#step-2-size-based-splitting","title":"Step 2: Size-Based Splitting","text":"<p>If any chunk exceeds <code>max_chunk_size</code>, it's further split using sentence boundaries while preserving:</p> <ul> <li>LaTeX environments (<code>\\begin{...}...\\end{...}</code>)</li> <li>Inline and display math (<code>$...$</code>, <code>$$...$$</code>)</li> <li>Markdown tables</li> <li>Figure and table references</li> </ul>"},{"location":"pipeline-stages/chunking/#step-3-smart-merging","title":"Step 3: Smart Merging","text":"<p>If <code>merge_small_chunks: true</code>, the chunker merges adjacent chunks when:</p> <ol> <li>Combined length doesn't exceed <code>max_chunk_size</code></li> <li>Chunks have compatible heading levels:</li> <li>Same level headers (e.g., two H2 sections)</li> <li>Previous chunk has higher level header (e.g., H1 followed by H2)</li> </ol> <p>This prevents overly small chunks while maintaining semantic coherence.</p>"},{"location":"pipeline-stages/chunking/#header-inclusion","title":"Header Inclusion","text":"<p>When <code>add_headers: true</code>, section headers are prepended to each chunk:</p> <p>Without headers: <pre><code>This section describes the methodology used in the study...\n</code></pre></p> <p>With headers: <pre><code># Introduction\n## Methods\nThis section describes the methodology used in the study...\n</code></pre></p> <p>This provides context for each chunk, especially useful for retrieval systems.</p>"},{"location":"pipeline-stages/chunking/#content-preservation","title":"Content Preservation","text":"<p>The chunker intelligently handles special content:</p> <p>LaTeX Formulas: <pre><code>The equation \\begin{equation}\nE = mc^2\n\\end{equation} is preserved intact.\n</code></pre></p> <p>Tables: <pre><code>| Column 1 | Column 2 |\n|----------|----------|\n| Data 1   | Data 2   |\n</code></pre></p> <p>These are never split mid-formula or mid-table, even if they exceed <code>max_chunk_size</code>.</p>"},{"location":"pipeline-stages/chunking/#use-cases","title":"Use Cases","text":""},{"location":"pipeline-stages/chunking/#small-chunks-for-dense-retrieval","title":"Small Chunks for Dense Retrieval","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 256\n    word_overlap: 20\n    add_headers: true\n    merge_small_chunks: false\n</code></pre> <p>Creates small, focused chunks with overlap for better semantic retrieval.</p>"},{"location":"pipeline-stages/chunking/#large-chunks-for-context","title":"Large Chunks for Context","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 2048\n    add_headers: false\n    merge_small_chunks: true\n    headers_to_split_on: [1, 2]  # Only split on major sections\n</code></pre> <p>Creates larger chunks that preserve more context, suitable for summarization or large context windows.</p>"},{"location":"pipeline-stages/chunking/#academic-papers","title":"Academic Papers","text":"<pre><code>- name: chunker\n  config:\n    max_chunk_size: 512\n    add_headers: true\n    merge_small_chunks: true\n    headers_to_split_on: [1, 2, 3, 4, 5, 6]\n</code></pre> <p>Respects the hierarchical structure of academic papers while maintaining readable chunk sizes.</p>"},{"location":"pipeline-stages/chunking/#output-format","title":"Output Format","text":"<p>Each chunk becomes a separate <code>Document</code> with:</p> <ul> <li>content: The chunk text (with headers if <code>add_headers: true</code>)</li> <li>file_path: Original document file path</li> <li>file_format: Original document format</li> <li>metadata.headers: List of Markdown headers that apply to this chunk</li> </ul>"},{"location":"pipeline-stages/chunking/#example-output","title":"Example Output","text":"<pre><code>Document(\n    content=\"# Introduction\\n## Background\\nThis paper discusses...\",\n    file_path=\"papers/paper1.pdf\",\n    file_format=\"pdf\",\n    metadata={\n        \"headers\": [\"#Introduction\", \"##Background\"],\n        # ... other metadata from original document\n    }\n)\n</code></pre>"},{"location":"pipeline-stages/chunking/#performance","title":"Performance","text":"<p>The chunker uses parallel processing to handle large document sets efficiently:</p> <ul> <li>Documents are processed in separate processes using <code>ProcessPoolExecutor</code></li> <li>Each process runs an independent chunker instance</li> <li>Results are collected and flattened into a single list</li> <li>Set <code>max_workers</code> to control parallelism (defaults to CPU count)</li> </ul> <p>Performance tip: For I/O-bound operations, use the default <code>max_workers=None</code>. For CPU-intensive chunking of very large documents, experiment with different worker counts.</p>"},{"location":"pipeline-stages/chunking/#integration-with-other-steps","title":"Integration with Other Steps","text":""},{"location":"pipeline-stages/chunking/#typical-pipeline-order","title":"Typical Pipeline Order","text":"<pre><code>pipeline:\n  inputs:\n    path: \"documents\"\n  stages:\n    - name: extraction\n\n    - name: deduplication\n      config:\n        method: \"lsh\"\n\n    - name: cleaning\n\n    - name: chunker\n      config:\n        max_chunk_size: 512\n        add_headers: true\n\n    - name: embedding  # Or qdrant upload\n\n    - name: export\n</code></pre>"},{"location":"pipeline-stages/chunking/#before-vector-database-upload","title":"Before Vector Database Upload","text":"<p>Chunking is typically done before uploading to vector databases:</p> <pre><code>- name: chunker\n  config:\n    max_chunk_size: 512\n    add_headers: true\n\n- name: qdrant\n  config:\n    database:\n      collection_name: \"documents\"\n    # ... other qdrant config\n</code></pre> <p>This ensures each chunk gets its own embedding vector in the database.</p>"},{"location":"pipeline-stages/chunking/#best-practices","title":"Best Practices","text":"<ol> <li>Choose appropriate chunk size:</li> <li>Smaller chunks (256-512 words) for dense retrieval</li> <li> <p>Larger chunks (1024-2048 words) for summarization or large context models</p> </li> <li> <p>Add headers for context: Enable <code>add_headers: true</code> when chunks will be retrieved without surrounding context</p> </li> <li> <p>Merge small chunks: Keep <code>merge_small_chunks: true</code> to avoid tiny chunks that lack sufficient context</p> </li> <li> <p>Adjust header levels: For documents with deep nesting, limit <code>headers_to_split_on</code> to major sections only</p> </li> </ol>"},{"location":"pipeline-stages/chunking/#troubleshooting","title":"Troubleshooting","text":""},{"location":"pipeline-stages/chunking/#chunks-are-too-large","title":"Chunks are too large","text":"<ul> <li>Decrease <code>max_chunk_size</code></li> <li>Add more header levels to <code>headers_to_split_on</code></li> <li>Set <code>merge_small_chunks: false</code></li> </ul>"},{"location":"pipeline-stages/chunking/#chunks-are-too-small","title":"Chunks are too small","text":"<ul> <li>Increase <code>max_chunk_size</code></li> <li>Set <code>merge_small_chunks: true</code></li> <li>Reduce header levels in <code>headers_to_split_on</code></li> </ul>"},{"location":"pipeline-stages/chunking/#latex-formulas-are-broken","title":"LaTeX formulas are broken","text":"<p>The chunker should preserve LaTeX automatically. If formulas are breaking:</p> <ul> <li>Check that LaTeX uses proper <code>\\begin{...}</code> and <code>\\end{...}</code> syntax</li> <li>Verify formulas aren't malformed in the original document</li> <li>Review the cleaning step output before chunking</li> </ul>"},{"location":"pipeline-stages/chunking/#slow-performance","title":"Slow performance","text":"<ul> <li>Adjust <code>max_workers</code> (try different values)</li> <li>Ensure you're chunking after deduplication and cleaning</li> <li>Consider increasing <code>max_chunk_size</code> to reduce total chunk count</li> </ul>"},{"location":"pipeline-stages/chunking/#next-steps","title":"Next Steps","text":"<ul> <li>Set up Qdrant upload to store chunks in a vector database</li> <li>Learn about Export options for saving chunked documents</li> </ul>"},{"location":"pipeline-stages/chunking/#code-reference","title":"Code Reference","text":""},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step","title":"<code>chunker_step</code>","text":"<p>Document chunking step using semantic two-step chunking strategy.</p>"},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep","title":"<code>ChunkerStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Chunk documents into smaller, semantically meaningful pieces.</p> <p>Uses a two-step chunking strategy:</p> <ol> <li>Split by Markdown headers to maintain document structure</li> <li>Further split large sections by sentences while preserving LaTeX and tables</li> <li>Optionally merge small chunks that share compatible heading levels</li> </ol> <p>The chunker processes documents in parallel using multiprocessing for performance.</p> <p>Config parameters:</p> <pre><code>- max_chunk_size (int): Maximum size of any chunk in words (default: 512)\n- chunk_overlap (int): Number of characters to overlap between chunks (default: 0)\n- word_overlap (int): Number of words to overlap between chunks (default: 0)\n- add_headers (bool): Whether to prepend section headers to chunks (default: False)\n- merge_small_chunks (bool): Whether to merge small chunks with compatible headers (default: True)\n- headers_to_split_on (list[int]): Markdown header levels to split on (default: [1, 2, 3, 4, 5, 6])\n- max_workers (int): Number of parallel workers, None uses CPU count (default: None)\n</code></pre> <p>Examples:</p>"},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep--basic-chunking-with-default-settings","title":"Basic chunking with default settings","text":"<p>config: {max_chunk_size: 512}</p>"},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep--chunking-with-headers-and-overlap-for-retrieval","title":"Chunking with headers and overlap for retrieval","text":"<p>config: {     max_chunk_size: 512,     add_headers: true,     word_overlap: 20,     merge_small_chunks: true }</p>"},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep--large-chunks-for-context-preservation","title":"Large chunks for context preservation","text":"<p>config: {     max_chunk_size: 2048,     headers_to_split_on: [1, 2],     merge_small_chunks: true }</p> Source code in <code>eve/steps/chunking/chunker_step.py</code> <pre><code>class ChunkerStep(PipelineStep):\n    \"\"\"Chunk documents into smaller, semantically meaningful pieces.\n\n    Uses a two-step chunking strategy:\n\n    1. Split by Markdown headers to maintain document structure\n    2. Further split large sections by sentences while preserving LaTeX and tables\n    3. Optionally merge small chunks that share compatible heading levels\n\n    The chunker processes documents in parallel using multiprocessing for performance.\n\n    Config parameters:\n\n        - max_chunk_size (int): Maximum size of any chunk in words (default: 512)\n        - chunk_overlap (int): Number of characters to overlap between chunks (default: 0)\n        - word_overlap (int): Number of words to overlap between chunks (default: 0)\n        - add_headers (bool): Whether to prepend section headers to chunks (default: False)\n        - merge_small_chunks (bool): Whether to merge small chunks with compatible headers (default: True)\n        - headers_to_split_on (list[int]): Markdown header levels to split on (default: [1, 2, 3, 4, 5, 6])\n        - max_workers (int): Number of parallel workers, None uses CPU count (default: None)\n\n    Examples:\n        # Basic chunking with default settings\n        config: {max_chunk_size: 512}\n\n        # Chunking with headers and overlap for retrieval\n        config: {\n            max_chunk_size: 512,\n            add_headers: true,\n            word_overlap: 20,\n            merge_small_chunks: true\n        }\n\n        # Large chunks for context preservation\n        config: {\n            max_chunk_size: 2048,\n            headers_to_split_on: [1, 2],\n            merge_small_chunks: true\n        }\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"Initialize the chunker step.\n\n        Args:\n            config: Configuration dictionary containing chunking parameters\n        \"\"\"\n        super().__init__(config, name=\"ChunkerStep\")\n\n        self.chunk_overlap = config.get(\"chunk_overlap\", 0)\n        self.max_chunk_size = config.get(\"max_chunk_size\", 512)\n        self.word_overlap = config.get(\"word_overlap\", 0)\n        self.add_headers = config.get(\"add_headers\", False)\n        self.merge_small_chunks = config.get(\"merge_small_chunks\", True)\n        self.headers_to_split_on = config.get(\"headers_to_split_on\", [1, 2, 3, 4, 5, 6])\n        self.max_workers = config.get(\"max_workers\", None)  # None = CPU count\n\n        self.chunker = MarkdownTwoStepChunker(\n            self.max_chunk_size,\n            self.chunk_overlap,\n            self.add_headers,\n            self.word_overlap,\n            self.headers_to_split_on,\n            self.merge_small_chunks,\n        )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute chunking on documents in parallel.\n\n        Processes each document independently using multiprocessing, then flattens\n        all chunks into a single list.\n\n        Args:\n            documents: List of documents to chunk\n\n        Returns:\n            Flattened list of all chunks from all documents\n        \"\"\"\n        self.logger.info(f\"Chunking {len(documents)} documents\")\n        self.logger.info(f\"Using max_chunk_size={self.max_chunk_size}, chunk_overlap={self.chunk_overlap}\")\n        self.logger.info(f\"Parallel processing with max_workers={self.max_workers or 'CPU count'}\")\n\n        loop = asyncio.get_event_loop()\n\n        # Serialize documents to plain dicts for pickling\n        serialized_docs = [_serialize_document(doc) for doc in documents]\n\n        # Create a partial function with the chunker configuration\n        chunk_func = partial(\n            _chunk_document,\n            max_chunk_size=self.max_chunk_size,\n            chunk_overlap=self.chunk_overlap,\n            add_headers=self.add_headers,\n            word_overlap=self.word_overlap,\n            headers_to_split_on=self.headers_to_split_on,\n            merge_small_chunks=self.merge_small_chunks,\n        )\n\n        # Process documents in parallel\n        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n            tasks = [\n                loop.run_in_executor(executor, chunk_func, doc)\n                for doc in serialized_docs\n            ]\n            results = await asyncio.gather(*tasks)\n\n        # Flatten and deserialize results\n        all_chunks = []\n        for doc_chunks in results:\n            all_chunks.extend([_deserialize_document(chunk) for chunk in doc_chunks])\n\n        self.logger.info(f\"Chunking complete: {len(documents)} documents -&gt; {len(all_chunks)} chunks\")\n\n        return all_chunks\n</code></pre>"},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep.__init__","title":"<code>__init__(config: dict)</code>","text":"<p>Initialize the chunker step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing chunking parameters</p> required Source code in <code>eve/steps/chunking/chunker_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the chunker step.\n\n    Args:\n        config: Configuration dictionary containing chunking parameters\n    \"\"\"\n    super().__init__(config, name=\"ChunkerStep\")\n\n    self.chunk_overlap = config.get(\"chunk_overlap\", 0)\n    self.max_chunk_size = config.get(\"max_chunk_size\", 512)\n    self.word_overlap = config.get(\"word_overlap\", 0)\n    self.add_headers = config.get(\"add_headers\", False)\n    self.merge_small_chunks = config.get(\"merge_small_chunks\", True)\n    self.headers_to_split_on = config.get(\"headers_to_split_on\", [1, 2, 3, 4, 5, 6])\n    self.max_workers = config.get(\"max_workers\", None)  # None = CPU count\n\n    self.chunker = MarkdownTwoStepChunker(\n        self.max_chunk_size,\n        self.chunk_overlap,\n        self.add_headers,\n        self.word_overlap,\n        self.headers_to_split_on,\n        self.merge_small_chunks,\n    )\n</code></pre>"},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step.ChunkerStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute chunking on documents in parallel.</p> <p>Processes each document independently using multiprocessing, then flattens all chunks into a single list.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to chunk</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Flattened list of all chunks from all documents</p> Source code in <code>eve/steps/chunking/chunker_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute chunking on documents in parallel.\n\n    Processes each document independently using multiprocessing, then flattens\n    all chunks into a single list.\n\n    Args:\n        documents: List of documents to chunk\n\n    Returns:\n        Flattened list of all chunks from all documents\n    \"\"\"\n    self.logger.info(f\"Chunking {len(documents)} documents\")\n    self.logger.info(f\"Using max_chunk_size={self.max_chunk_size}, chunk_overlap={self.chunk_overlap}\")\n    self.logger.info(f\"Parallel processing with max_workers={self.max_workers or 'CPU count'}\")\n\n    loop = asyncio.get_event_loop()\n\n    # Serialize documents to plain dicts for pickling\n    serialized_docs = [_serialize_document(doc) for doc in documents]\n\n    # Create a partial function with the chunker configuration\n    chunk_func = partial(\n        _chunk_document,\n        max_chunk_size=self.max_chunk_size,\n        chunk_overlap=self.chunk_overlap,\n        add_headers=self.add_headers,\n        word_overlap=self.word_overlap,\n        headers_to_split_on=self.headers_to_split_on,\n        merge_small_chunks=self.merge_small_chunks,\n    )\n\n    # Process documents in parallel\n    with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n        tasks = [\n            loop.run_in_executor(executor, chunk_func, doc)\n            for doc in serialized_docs\n        ]\n        results = await asyncio.gather(*tasks)\n\n    # Flatten and deserialize results\n    all_chunks = []\n    for doc_chunks in results:\n        all_chunks.extend([_deserialize_document(chunk) for chunk in doc_chunks])\n\n    self.logger.info(f\"Chunking complete: {len(documents)} documents -&gt; {len(all_chunks)} chunks\")\n\n    return all_chunks\n</code></pre>"},{"location":"pipeline-stages/chunking/#eve.steps.chunking.chunker_step.convert_langchain_doc","title":"<code>convert_langchain_doc(doc: Document, chunk: LangchainDocument) -&gt; Document</code>","text":"<p>Convert a LangChain Document chunk to an Eve Document.</p> <p>Extracts headers from chunk metadata and combines with original document metadata.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>Document</code> <p>Original Eve Document</p> required <code>chunk</code> <code>Document</code> <p>LangChain Document chunk with header metadata</p> required <p>Returns:</p> Type Description <code>Document</code> <p>Eve Document with chunk content and combined metadata</p> Source code in <code>eve/steps/chunking/chunker_step.py</code> <pre><code>def convert_langchain_doc(doc: Document, chunk: LangchainDocument) -&gt; Document:\n    \"\"\"Convert a LangChain Document chunk to an Eve Document.\n\n    Extracts headers from chunk metadata and combines with original document metadata.\n\n    Args:\n        doc: Original Eve Document\n        chunk: LangChain Document chunk with header metadata\n\n    Returns:\n        Eve Document with chunk content and combined metadata\n    \"\"\"\n    headers = [\"#\" * key + value for key, value in chunk.metadata.items()]\n    return Document(\n        content=chunk.page_content,\n        file_path=doc.file_path,\n        file_format=doc.file_format,\n        metadata={\"headers\": headers, **doc.metadata},\n    )\n</code></pre>"},{"location":"pipeline-stages/cleaning/","title":"Cleaning Stage","text":"<p>The cleaning stage improves document quality by removing OCR errors, noise artifacts, correcting formatting issues, and enhancing readability.</p>"},{"location":"pipeline-stages/cleaning/#features","title":"Features","text":"<ul> <li>Noise Removal: Fixes the errors introduced during the OCR extraction.</li> <li>Nougat Correction: This is a series of post processing cleaning step by Nougat to make the document markdown compactible.</li> <li>Rule based Correction: Custom regex based patterns to remove the most commonly occuring errors.</li> <li>LaTeX Correction: Fixes mathematical equations and notation</li> </ul>"},{"location":"pipeline-stages/cleaning/#configuration","title":"Configuration","text":""},{"location":"pipeline-stages/cleaning/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: cleaning\n  config:\n    ocr_threshold: 0.99\n</code></pre>"},{"location":"pipeline-stages/cleaning/#llm-enhanced-cleaning-optional","title":"LLM Enhanced Cleaning (Optional)","text":"<p>For the latex correction, the latex components are extracted and passed to an LLM for improvement, the syntax is verified using pdflatex and then merged back into the document.</p> <p>To use this module, You need to set the .env key for <code>OPENROUTER_API_KEY</code>.</p> <pre><code>- name: cleaning\n  config:\n    enable_latex_correction: true\n</code></pre>"},{"location":"pipeline-stages/cleaning/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"pipeline-stages/cleaning/#ocr_threshold","title":"ocr_threshold","text":"<ul> <li>Type: Float</li> <li>Default: <code>0.99</code></li> <li>Description: This parameter controls what level of similarity is required for two sentences to be considered duplicate.</li> </ul>"},{"location":"pipeline-stages/cleaning/#min_words","title":"min_words","text":"<ul> <li>Type: Int</li> <li>Default: 2</li> <li>Description: This parameter defines the minimum number of words a sentence should have for the duplication process. Higher the value, the more accurate the duplicate ocr segments are removed.</li> </ul>"},{"location":"pipeline-stages/cleaning/#enable_latex_correction","title":"enable_latex_correction","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: Use LLM to fix latex formulas and tables</li> </ul>"},{"location":"pipeline-stages/cleaning/#openrouter_model","title":"openrouter_model","text":"<ul> <li>Type: String</li> <li>Default: <code>anthropic/claude-3-haiku</code></li> <li>Description: The model to be used for latex correction</li> </ul>"},{"location":"pipeline-stages/cleaning/#debug","title":"debug","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: To enable debug output</li> </ul>"},{"location":"pipeline-stages/cleaning/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about PII removal</li> <li>Configure metadata extraction</li> <li>Set up document export</li> </ul>"},{"location":"pipeline-stages/deduplication/","title":"Deduplication Stage","text":"<p>The deduplication stage removes duplicate and near-duplicate documents from your dataset, improving data quality and reducing processing overhead.</p>"},{"location":"pipeline-stages/deduplication/#deduplication-methods","title":"Deduplication Methods","text":""},{"location":"pipeline-stages/deduplication/#exact-deduplication","title":"Exact Deduplication","text":"<p>Uses SHA-256 checksums to identify identical documents:</p> <pre><code>- name: duplication\n  config:\n    method: \"exact\"\n</code></pre>"},{"location":"pipeline-stages/deduplication/#lsh-locality-sensitive-hashing","title":"LSH (Locality Sensitive Hashing)","text":"<p>Finds near-duplicates using MinHash:</p> <pre><code>- name: duplication\n  config:\n    method: \"lsh\"\n    shingle_size: 3\n    num_perm: 128\n    threshold: 0.8\n</code></pre>"},{"location":"pipeline-stages/deduplication/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"pipeline-stages/deduplication/#lsh-parameters","title":"LSH Parameters","text":""},{"location":"pipeline-stages/deduplication/#shingle_size","title":"shingle_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>3</code></li> <li>Description: Size of text chunks (shingles) for comparison. Larger shingles are more specific but increases computation.</li> </ul>"},{"location":"pipeline-stages/deduplication/#num_perm","title":"num_perm","text":"<ul> <li>Type: Integer</li> <li>Default: <code>128</code></li> <li>Description: Number of random permutations for MinHash. Higher values increase accuracy but use more memory</li> </ul>"},{"location":"pipeline-stages/deduplication/#threshold","title":"threshold","text":"<ul> <li>Type: Float</li> <li>Default: <code>0.8</code></li> <li>Range: 0.0-1.0</li> <li>Description: Similarity threshold for duplicate detection. Higher values find closer duplicates but may miss some.</li> </ul>"},{"location":"pipeline-stages/deduplication/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about content cleaning</li> <li>Configure PII removal</li> <li>Set up metadata extraction</li> </ul>"},{"location":"pipeline-stages/export/","title":"Export Stage","text":"<p>The final stage of the pipeline where the processed documents are stored in the format required by the user. The default format is .md</p> <pre><code>- name: export\n  config:\n    destination: \"./output\"\n</code></pre>"},{"location":"pipeline-stages/export/#destination","title":"destination","text":"<ul> <li>Type: String</li> <li>Default: <code>./output</code></li> <li>Description: Directory to save the processed files</li> </ul>"},{"location":"pipeline-stages/extraction/","title":"Extraction Stage","text":"<p>The extraction stage is responsible for reading and extracting content from various document formats. It's the first stage in most pipeline configurations.</p>"},{"location":"pipeline-stages/extraction/#supported-formats","title":"Supported Formats","text":"<ul> <li>PDF: Portable Document Format files</li> <li>HTML: Hypertext Markup Language files</li> <li>XML: Extensible Markup Language files</li> <li>Markdown: Markdown text files</li> <li>JSONL: JSON Lines format (one JSON object per line)</li> </ul>"},{"location":"pipeline-stages/extraction/#configuration","title":"Configuration","text":""},{"location":"pipeline-stages/extraction/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: extraction\n  config:\n    format: \"pdf\"  # or , \"html\", \"xml\", \"markdown\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#stage-behavior","title":"Stage Behavior","text":""},{"location":"pipeline-stages/extraction/#input-processing","title":"Input Processing","text":"<p>The extraction stage processes documents from the configured input directory:</p> <pre><code>pipeline:\n  inputs:\n    path: \"input_documents\"\n</code></pre> <ul> <li>Recursively scans the input directory</li> <li>Supports nested folder structures</li> </ul>"},{"location":"pipeline-stages/extraction/#format-specific-features","title":"Format-Specific Features","text":""},{"location":"pipeline-stages/extraction/#pdf-extraction","title":"PDF Extraction","text":"<p>For PDF documents, the extractor:</p> <ul> <li>Extracts text content using Nougat OCR.</li> <li>Preserves document structure (headings, paragraphs).</li> <li>Maintains table and formulas.</li> </ul> <pre><code>- name: extraction\n  config:\n    format: \"pdf\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#nougat-server","title":"Nougat Server","text":"<p>You need to setup the nougat server found under the <code>/server</code></p> <pre><code>cd server\npython3 nougat_server.py\n</code></pre>"},{"location":"pipeline-stages/extraction/#html-extraction","title":"HTML Extraction","text":"<p>For HTML documents, the extractor use Trafilatura to extract the content.</p> <pre><code>- name: extraction\n  config:\n    format: \"html\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#xml-extraction","title":"XML Extraction","text":"<p>For XML documents, the extractor:</p> <ul> <li>Extracts text content from XML tags</li> <li>Preserves document structure</li> <li>Handles namespaces appropriately</li> <li>Maintains attribute information when relevant</li> </ul> <pre><code>- name: extraction\n  config:\n    format: \"xml\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#jsonl-extraction","title":"JSONL Extraction","text":"<p>JSONL (JSON Lines) format allows you to input pre-structured documents with custom metadata. Each line in the file must be a valid JSON object.</p> <p>Format Requirements:</p> <p>Required Fields: - <code>content</code> (string): The document text content</p> <p>Optional Fields: - <code>metadata</code> (object): Custom metadata that will be preserved throughout the pipeline - <code>embedding</code> (array): Pre-computed embedding vector (useful when using <code>use_existing_embeddings: true</code>) - <code>pipeline_metadata</code> (object): Internal metadata from previous pipeline runs</p> <p>Example JSONL file:</p> <pre><code>{\"content\": \"This is the first document.\", \"metadata\": {\"title\": \"Document 1\", \"author\": \"John Doe\", \"year\": 2024}}\n{\"content\": \"Second document with tags.\", \"metadata\": {\"title\": \"Doc 2\", \"source\": \"paper.pdf\", \"tags\": [\"AI\", \"ML\"]}}\n{\"content\": \"Document with pre-computed embedding.\", \"metadata\": {\"title\": \"Doc 3\"}, \"embedding\": [0.123, 0.456, ...]}\n</code></pre> <p>Configuration:</p> <pre><code>pipeline:\n  inputs:\n    path: \"data/documents.jsonl\"\n  stages:\n    - name: extraction\n      config:\n        format: \"jsonl\"\n</code></pre> <p>Key Features:</p> <ol> <li>Flexible Metadata: Add any custom fields you need (title, author, tags, year, etc.)</li> <li>Metadata Preservation: All metadata fields are preserved throughout the entire pipeline</li> <li>Metadata Inheritance: When documents are chunked, each chunk inherits the original document's metadata</li> <li>Pre-computed Embeddings: Include embeddings to skip re-computation in later stages</li> <li>Pipeline Chaining: Output from one pipeline can be input to another via JSONL export</li> </ol> <p>Practical Example:</p> <pre><code>pipeline:\n  inputs:\n    path: \"research_papers.jsonl\"\n  stages:\n    - name: extraction\n      config: { format: \"jsonl\" }\n    - name: chunker\n      config: { max_chunk_size: 512 }\n    - name: qdrant_upload\n      config:\n        mode: \"qdrant\"\n        # ... other config\n</code></pre> <p>After chunking, each chunk will have metadata like: <pre><code>{\n  \"title\": \"Document 1\",\n  \"author\": \"John Doe\",\n  \"year\": 2024,\n  \"headers\": [\"#Introduction\", \"##Background\"]\n}\n</code></pre></p> <p>This metadata is then uploaded to Qdrant, making it easy to filter and search by author, year, or other custom fields.</p>"},{"location":"pipeline-stages/extraction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about deduplication</li> <li>Explore cleaning options</li> <li>Configure PII removal</li> </ul>"},{"location":"pipeline-stages/filters/","title":"Filters","text":"<p>Filters allow you to selectively keep or discard documents based on specific criteria. They are essential for quality control and ensuring your processed documents meet your requirements.</p>"},{"location":"pipeline-stages/filters/#overview","title":"Overview","text":"<p>Filters in the Eve pipeline evaluate documents against configurable criteria and either keep or discard them. Each filter:</p> <ul> <li>Adds metadata to documents (e.g., word count, PII percentage)</li> <li>Can be configured with <code>keep</code> or <code>discard</code> actions</li> <li>Provides detailed logging of filtering results</li> <li>Can be chained together for complex filtering logic</li> </ul>"},{"location":"pipeline-stages/filters/#available-filters","title":"Available Filters","text":""},{"location":"pipeline-stages/filters/#length-filter","title":"Length Filter","text":"<p>Filters documents based on word count, useful for removing documents that are too short or too long.</p> <p>Step name: <code>length_filter</code></p>"},{"location":"pipeline-stages/filters/#configuration-parameters","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>length</code> int Yes - Word count threshold for filtering <code>comparison</code> str No <code>\"greater\"</code> Either <code>\"less\"</code> or <code>\"greater\"</code> to compare against threshold <code>action</code> str No <code>\"keep\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents matching the condition"},{"location":"pipeline-stages/filters/#examples","title":"Examples","text":"<pre><code># Keep only documents with more than 1000 words\n- name: length_filter\n  config:\n    length: 1000\n    comparison: \"greater\"\n    action: \"keep\"\n</code></pre> <pre><code># Remove short documents (less than 100 words)\n- name: length_filter\n  config:\n    length: 100\n    comparison: \"less\"\n    action: \"discard\"\n</code></pre> <pre><code># Create a range filter: keep documents between 50-1000 words\n# This requires chaining two length filters\n- name: length_filter\n  config:\n    length: 50\n    comparison: \"greater\"\n    action: \"keep\"\n\n- name: length_filter\n  config:\n    length: 1000\n    comparison: \"less\"\n    action: \"keep\"\n</code></pre>"},{"location":"pipeline-stages/filters/#metadata-added","title":"Metadata Added","text":"<ul> <li><code>word_count</code>: Number of words in the document</li> </ul>"},{"location":"pipeline-stages/filters/#pii-filter","title":"PII Filter","text":"<p>Filters documents based on the percentage of Personally Identifiable Information (PII) tokens. Useful for removing documents with excessive PII or keeping only anonymized documents.</p> <p>Step name: <code>pii_filter</code></p> <p>Note: This filter expects PII tokens to already be marked in the document (e.g., <code>[PERSON]</code>, <code>[EMAIL_ADDRESS]</code>). Use the PII removal step before this filter to anonymize PII.</p>"},{"location":"pipeline-stages/filters/#special-behavior","title":"Special Behavior","text":"<p>Documents containing \"abstract\" or \"introduction\" sections are always kept regardless of PII percentage. This is because academic papers often mention author names in these sections.</p>"},{"location":"pipeline-stages/filters/#configuration-parameters_1","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>threshold</code> float Yes - PII token percentage threshold (e.g., 0.03 for 3%) <code>action</code> str No <code>\"discard\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents meeting the threshold <code>apply_filter</code> bool No <code>true</code> Whether to apply filtering or just calculate PII percentage"},{"location":"pipeline-stages/filters/#examples_1","title":"Examples","text":"<pre><code># Remove documents with 3% or more PII tokens (keep abstracts/intros)\n- name: pii_filter\n  config:\n    threshold: 0.03\n    action: \"discard\"\n</code></pre> <pre><code># Keep only documents with low PII (less than 1%)\n- name: pii_filter\n  config:\n    threshold: 0.01\n    action: \"discard\"\n</code></pre> <pre><code># Just calculate PII percentage without filtering\n- name: pii_filter\n  config:\n    threshold: 0.03\n    apply_filter: false\n</code></pre>"},{"location":"pipeline-stages/filters/#metadata-added_1","title":"Metadata Added","text":"<ul> <li><code>pii_tokens_percentage</code>: Percentage of PII tokens in the document (0.0 to 1.0)</li> </ul>"},{"location":"pipeline-stages/filters/#newline-filter","title":"Newline Filter","text":"<p>Filters documents based on the number of newline characters, useful for identifying documents with specific formatting characteristics.</p> <p>Step name: <code>newline_filter</code></p>"},{"location":"pipeline-stages/filters/#configuration-parameters_2","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>chunks</code> int Yes - Newline count threshold for filtering <code>comparison</code> str No <code>\"greater\"</code> Either <code>\"less\"</code> or <code>\"greater\"</code> to compare against threshold <code>action</code> str No <code>\"keep\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents matching the condition"},{"location":"pipeline-stages/filters/#examples_2","title":"Examples","text":"<pre><code># Keep documents with more than 10 newlines (well-structured content)\n- name: newline_filter\n  config:\n    chunks: 10\n    comparison: \"greater\"\n    action: \"keep\"\n</code></pre> <pre><code># Remove documents with too many newlines (likely poorly formatted)\n- name: newline_filter\n  config:\n    chunks: 100\n    comparison: \"greater\"\n    action: \"discard\"\n</code></pre>"},{"location":"pipeline-stages/filters/#metadata-added_2","title":"Metadata Added","text":"<ul> <li><code>newline_count</code>: Number of newline characters in the document</li> </ul>"},{"location":"pipeline-stages/filters/#reference-filter","title":"Reference Filter","text":"<p>Filters documents that contain references or acknowledgements sections, useful for academic paper processing.</p> <p>Step name: <code>reference_filter</code></p> <p>The filter detects references and acknowledgements by checking:</p> <ul> <li>Document headers metadata for keywords: \"reference\", \"references\", \"acknowledgement\", \"acknowledgements\"</li> <li>Text content for markdown headers containing these keywords</li> </ul>"},{"location":"pipeline-stages/filters/#configuration-parameters_3","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>action</code> str No <code>\"discard\"</code> Either <code>\"keep\"</code> or <code>\"discard\"</code> documents with references/acknowledgements"},{"location":"pipeline-stages/filters/#examples_3","title":"Examples","text":"<pre><code># Remove documents with references or acknowledgements\n- name: reference_filter\n  config:\n    action: \"discard\"\n</code></pre> <pre><code># Keep only documents with references (academic papers)\n- name: reference_filter\n  config:\n    action: \"keep\"\n</code></pre>"},{"location":"pipeline-stages/filters/#perplexity-filter","title":"Perplexity Filter","text":"<p>Filters documents based on perplexity scores calculated by a language model. Lower perplexity indicates more natural, coherent text.</p> <p>Step name: <code>perplexity_filter</code></p> <p>Note: This filter loads a language model which requires significant memory and compute resources.</p>"},{"location":"pipeline-stages/filters/#configuration-parameters_4","title":"Configuration Parameters","text":"Parameter Type Required Default Description <code>threshold</code> float No <code>0.0</code> Perplexity threshold for filtering <code>enable_threshold</code> bool No <code>false</code> Whether to apply threshold-based filtering <code>model_name</code> str No <code>\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"</code> Hugging Face model to use for perplexity calculation <code>stride</code> int No <code>128</code> Stride for sliding window perplexity calculation <code>batch_size</code> int No <code>128</code> Batch size for model inference <code>max_length</code> int No <code>1024</code> Maximum sequence length for model"},{"location":"pipeline-stages/filters/#examples_4","title":"Examples","text":"<pre><code># Calculate perplexity for all documents without filtering\n- name: perplexity_filter\n  config:\n    model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n    enable_threshold: false\n</code></pre> <pre><code># Keep only documents with perplexity below 50\n- name: perplexity_filter\n  config:\n    threshold: 50.0\n    enable_threshold: true\n    model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n</code></pre>"},{"location":"pipeline-stages/filters/#metadata-added_3","title":"Metadata Added","text":"<ul> <li><code>perplexity</code>: Perplexity score calculated by the language model</li> </ul>"},{"location":"pipeline-stages/filters/#chaining-filters","title":"Chaining Filters","text":"<p>Filters can be chained to create complex filtering logic. Filters are applied sequentially, with each filter receiving the output of the previous filter.</p>"},{"location":"pipeline-stages/filters/#example-multi-stage-quality-filter","title":"Example: Multi-Stage Quality Filter","text":"<pre><code>pipeline:\n  inputs:\n    path: \"input_docs\"\n  stages:\n    - name: extraction\n\n    - name: cleaning\n\n    # Remove very short documents\n    - name: length_filter\n      config:\n        length: 50\n        comparison: \"less\"\n        action: \"discard\"\n\n    # Remove very long documents\n    - name: length_filter\n      config:\n        length: 10000\n        comparison: \"greater\"\n        action: \"discard\"\n\n    # Remove documents with reference sections\n    - name: reference_filter\n      config:\n        action: \"discard\"\n\n    # Remove documents with high PII\n    - name: pii_filter\n      config:\n        threshold: 0.05\n        action: \"discard\"\n\n    - name: export\n      config:\n        output_dir: \"filtered_output\"\n</code></pre>"},{"location":"pipeline-stages/filters/#best-practices","title":"Best Practices","text":"<ol> <li>Order matters: Apply computationally expensive filters (like perplexity) after cheaper filters (like length) to reduce processing time</li> <li>Test thresholds: Start with permissive thresholds and adjust based on your data</li> <li>Use metadata: Even if <code>apply_filter: false</code>, the metadata added by filters can be useful for analysis</li> <li>Monitor filtering: Check the logs to ensure you're not filtering out too many documents</li> <li>Chain wisely: Use multiple filters to create precise selection criteria</li> </ol>"},{"location":"pipeline-stages/filters/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Chunking for splitting documents</li> <li>Configure Metadata Extraction</li> <li>Set up Document Export</li> </ul>"},{"location":"pipeline-stages/filters/#code-reference","title":"Code Reference","text":""},{"location":"pipeline-stages/filters/#eve.steps.filters.length_filter","title":"<code>length_filter</code>","text":"<p>Length-based document filtering step.</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep","title":"<code>LengthFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on content length (word count).</p> <p>This step filters documents based on their word count with configurable thresholds and policies.</p> <p>Config parameters:</p> <pre><code>- length (int): The word count threshold for filtering\n- comparison (str): Either \"less\" or \"greater\" to compare against threshold\n- action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n</code></pre> <p>Examples:</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep--keep-documents-with-more-than-1000-words","title":"Keep documents with more than 1000 words","text":"<p>config: {length: 1000, comparison: \"greater\", action: \"keep\"}</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep--discard-documents-with-less-than-100-words","title":"Discard documents with less than 100 words","text":"<p>config: {length: 100, comparison: \"less\", action: \"discard\"}</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep--keep-documents-with-less-than-5000-words-filter-out-long-docs","title":"Keep documents with less than 5000 words (filter out long docs)","text":"<p>config: {length: 5000, comparison: \"less\", action: \"keep\"}</p> Source code in <code>eve/steps/filters/length_filter.py</code> <pre><code>class LengthFilterStep(PipelineStep):\n    \"\"\"Filter documents based on content length (word count).\n\n    This step filters documents based on their word count with configurable\n    thresholds and policies.\n\n    Config parameters:\n\n        - length (int): The word count threshold for filtering\n        - comparison (str): Either \"less\" or \"greater\" to compare against threshold\n        - action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n\n    Examples:\n        # Keep documents with more than 1000 words\n        config: {length: 1000, comparison: \"greater\", action: \"keep\"}\n\n        # Discard documents with less than 100 words\n        config: {length: 100, comparison: \"less\", action: \"discard\"}\n\n        # Keep documents with less than 5000 words (filter out long docs)\n        config: {length: 5000, comparison: \"less\", action: \"keep\"}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"LengthFilter\")\n\n        # Validate required config parameters\n        if \"length\" not in config:\n            raise ValueError(\"LengthFilterStep requires 'length' parameter in config\")\n\n        self.length_threshold = config.get(\"length\")\n        self.comparison = config.get(\"comparison\", \"greater\").lower()\n        self.action = config.get(\"action\", \"keep\").lower()\n\n        # Validate comparison parameter\n        if self.comparison not in [\"less\", \"greater\"]:\n            raise ValueError(f\"Invalid comparison '{self.comparison}'. Must be 'less' or 'greater'\")\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\")\n\n        self.logger.info(\n            f\"Initialized LengthFilter: {self.action} documents with \"\n            f\"{self.comparison} than {self.length_threshold} words\"\n        )\n\n    def _get_word_count(self, document: Document) -&gt; int:\n        \"\"\"Get the word count of a document.\n\n        Args:\n            document: Document to count words for\n\n        Returns:\n            Number of words in the document\n        \"\"\"\n        return len(document.content.split())\n\n    def _meets_length_condition(self, document: Document) -&gt; bool:\n        \"\"\"Check if document meets the length condition.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if document word count meets the comparison condition\n        \"\"\"\n        word_count = self._get_word_count(document)\n\n        if self.comparison == \"greater\":\n            return word_count &gt; self.length_threshold\n        else:  # \"less\"\n            return word_count &lt; self.length_threshold\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute length filtering on documents.\n\n        Args:\n            documents: List of documents to filter\n\n        Returns:\n            Filtered list of documents based on length criteria\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to filter\")\n            return documents\n\n        original_count = len(documents)\n\n        # Add word count to pipeline metadata for all documents\n        for document in documents:\n            word_count = self._get_word_count(document)\n            document.add_pipeline_metadata(\"word_count\", word_count)\n\n        # Apply filtering based on action\n        filtered_documents = []\n\n        for document in documents:\n            meets_condition = self._meets_length_condition(document)\n\n            # Keep document if:\n            # - action is \"keep\" AND condition is met\n            # - action is \"discard\" AND condition is NOT met\n            should_keep = (self.action == \"keep\" and meets_condition) or \\\n                         (self.action == \"discard\" and not meets_condition)\n\n            if should_keep:\n                filtered_documents.append(document)\n            else:\n                word_count = self._get_word_count(document)\n                self.logger.debug(\n                    f\"Filtered out {document.filename} ({word_count} words)\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_count - filtered_count\n\n        # Log statistics\n        if original_count &gt; 0:\n            percentage_kept = (filtered_count / original_count) * 100\n            self.logger.info(\n                f\"Length filtering complete: {filtered_count}/{original_count} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n        else:\n            self.logger.info(\"No documents were processed\")\n\n        return filtered_documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.length_filter.LengthFilterStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute length filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to filter</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents based on length criteria</p> Source code in <code>eve/steps/filters/length_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute length filtering on documents.\n\n    Args:\n        documents: List of documents to filter\n\n    Returns:\n        Filtered list of documents based on length criteria\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to filter\")\n        return documents\n\n    original_count = len(documents)\n\n    # Add word count to pipeline metadata for all documents\n    for document in documents:\n        word_count = self._get_word_count(document)\n        document.add_pipeline_metadata(\"word_count\", word_count)\n\n    # Apply filtering based on action\n    filtered_documents = []\n\n    for document in documents:\n        meets_condition = self._meets_length_condition(document)\n\n        # Keep document if:\n        # - action is \"keep\" AND condition is met\n        # - action is \"discard\" AND condition is NOT met\n        should_keep = (self.action == \"keep\" and meets_condition) or \\\n                     (self.action == \"discard\" and not meets_condition)\n\n        if should_keep:\n            filtered_documents.append(document)\n        else:\n            word_count = self._get_word_count(document)\n            self.logger.debug(\n                f\"Filtered out {document.filename} ({word_count} words)\"\n            )\n\n    filtered_count = len(filtered_documents)\n    removed_count = original_count - filtered_count\n\n    # Log statistics\n    if original_count &gt; 0:\n        percentage_kept = (filtered_count / original_count) * 100\n        self.logger.info(\n            f\"Length filtering complete: {filtered_count}/{original_count} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n    else:\n        self.logger.info(\"No documents were processed\")\n\n    return filtered_documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.pii_filter","title":"<code>pii_filter</code>","text":"<p>PII-based document filtering step with abstract/introduction exceptions.</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep","title":"<code>PiiFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on PII (Personally Identifiable Information) token percentage.</p> <p>This step calculates the percentage of PII tokens ([PERSON], [EMAIL_ADDRESS]) in documents and filters them based on a threshold. Documents with \"abstract\" or \"introduction\" in their headers or text are kept regardless of PII percentage.</p> <p>Config parameters:</p> <pre><code>- threshold (float): PII token percentage threshold (e.g., 0.03 for 3%)\n- action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n    - \"discard\": Remove documents with PII &gt;= threshold (except abstract/intro)\n    - \"keep\": Keep only documents with PII &gt;= threshold (except abstract/intro)\n- apply_filter (bool): Whether to apply filtering (default: True)\n</code></pre> <p>Examples:</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep--remove-documents-with-3-pii-tokens-but-keep-abstractsintros","title":"Remove documents with &gt;= 3% PII tokens (but keep abstracts/intros)","text":"<p>config: {threshold: 0.03, action: \"discard\", apply_filter: true}</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep--only-calculate-pii-percentage-without-filtering","title":"Only calculate PII percentage without filtering","text":"<p>config: {threshold: 0.03, apply_filter: false}</p> Source code in <code>eve/steps/filters/pii_filter.py</code> <pre><code>class PiiFilterStep(PipelineStep):\n    \"\"\"Filter documents based on PII (Personally Identifiable Information) token percentage.\n\n    This step calculates the percentage of PII tokens ([PERSON], [EMAIL_ADDRESS]) in documents\n    and filters them based on a threshold. Documents with \"abstract\" or \"introduction\" in their\n    headers or text are kept regardless of PII percentage.\n\n    Config parameters:\n\n        - threshold (float): PII token percentage threshold (e.g., 0.03 for 3%)\n        - action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n            - \"discard\": Remove documents with PII &gt;= threshold (except abstract/intro)\n            - \"keep\": Keep only documents with PII &gt;= threshold (except abstract/intro)\n        - apply_filter (bool): Whether to apply filtering (default: True)\n\n    Examples:\n        # Remove documents with &gt;= 3% PII tokens (but keep abstracts/intros)\n        config: {threshold: 0.03, action: \"discard\", apply_filter: true}\n\n        # Only calculate PII percentage without filtering\n        config: {threshold: 0.03, apply_filter: false}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"PiiFilter\")\n\n        # Validate required config\n        if \"threshold\" not in config:\n            raise ValueError(\"PiiFilterStep requires 'threshold' parameter in config\")\n\n        self.threshold = config.get(\"threshold\")\n        self.special_tokens = [\"[PERSON]\", \"[EMAIL_ADDRESS]\"]\n        self.apply_filter = config.get(\"apply_filter\", True)\n        self.action = config.get(\"action\", \"discard\").lower()\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(\n                f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\"\n            )\n\n        # Regex patterns for detecting abstract and introduction\n        # Allow for leading whitespace before the header\n        self.abstract_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*abstract\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n        self.introduction_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*introduction\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n\n        self.logger.info(\n            f\"Initialized PiiFilter: {self.action} documents with PII &gt;= {self.threshold} \"\n            f\"(except abstract/introduction sections), apply_filter={self.apply_filter}\"\n        )\n\n    def _has_abstract_or_introduction(self, doc: Document) -&gt; bool:\n        \"\"\"Check if document has abstract or introduction in headers or text.\n\n        Args:\n            doc: Document to check\n\n        Returns:\n            True if document contains abstract or introduction\n        \"\"\"\n        # Check in text content\n        has_abstract_in_text = bool(self.abstract_header_regex.search(doc.content))\n        has_introduction_in_text = bool(\n            self.introduction_header_regex.search(doc.content)\n        )\n\n        # Check in headers metadata\n        has_abstract_in_headers = False\n        has_introduction_in_headers = False\n\n        if \"headers\" in doc.metadata and isinstance(doc.metadata[\"headers\"], list):\n            headers = doc.metadata[\"headers\"]\n            has_abstract_in_headers = any(\n                \"abstract\" in h.strip().lower() for h in headers\n            )\n            has_introduction_in_headers = any(\n                \"introduction\" in h.strip().lower() for h in headers\n            )\n\n        return (\n            has_abstract_in_text\n            or has_introduction_in_text\n            or has_abstract_in_headers\n            or has_introduction_in_headers\n        )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute PII filtering on documents.\n\n        Args:\n            documents: List of documents to process\n\n        Returns:\n            Filtered list of documents (if apply_filter is True)\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to process\")\n            return documents\n\n        # Calculate PII percentage for all documents\n        for document in documents:\n            try:\n                total_words = len(document.content.split())\n                special_tokens_count = 0\n                for special_token in self.special_tokens:\n                    special_tokens_count += document.content.count(special_token)\n                # Percentage of pii_tokens overall\n                pii_percentage = special_tokens_count / total_words if total_words &gt; 0 else 0\n                document.add_pipeline_metadata(\"pii_tokens_percentage\", pii_percentage)\n            except Exception as e:\n                self.logger.error(\n                    f\"Error processing {document.filename}, exception {e}\"\n                )\n                document.add_pipeline_metadata(\"pii_tokens_percentage\", 0)\n\n        # Apply filtering if enabled\n        if self.apply_filter:\n            original_len = len(documents)\n            filtered_documents = []\n\n            for doc in documents:\n                pii_percentage = doc.get_pipeline_metadata(\"pii_tokens_percentage\", 0)\n                has_abstract_intro = self._has_abstract_or_introduction(doc)\n                meets_threshold = pii_percentage &gt;= self.threshold\n\n                # Logic:\n                # - If document has abstract/intro: ALWAYS keep (regardless of PII)\n                # - Otherwise: apply threshold-based filtering according to action\n                if has_abstract_intro:\n                    should_keep = True\n                elif self.action == \"discard\":\n                    # Discard documents with high PII (keep documents with low PII)\n                    should_keep = not meets_threshold\n                else:  # action == \"keep\"\n                    # Keep only documents with high PII\n                    should_keep = meets_threshold\n\n                if should_keep:\n                    filtered_documents.append(doc)\n                else:\n                    self.logger.debug(\n                        f\"Filtered out {doc.filename} (PII: {pii_percentage:.4f})\"\n                    )\n\n            filtered_count = len(filtered_documents)\n            removed_count = original_len - filtered_count\n            percentage_kept = (\n                (filtered_count / original_len) * 100 if original_len &gt; 0 else 0\n            )\n\n            self.logger.info(\n                f\"PII filtering complete: {filtered_count}/{original_len} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n\n            return filtered_documents\n\n        return documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.pii_filter.PiiFilterStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute PII filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to process</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents (if apply_filter is True)</p> Source code in <code>eve/steps/filters/pii_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute PII filtering on documents.\n\n    Args:\n        documents: List of documents to process\n\n    Returns:\n        Filtered list of documents (if apply_filter is True)\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to process\")\n        return documents\n\n    # Calculate PII percentage for all documents\n    for document in documents:\n        try:\n            total_words = len(document.content.split())\n            special_tokens_count = 0\n            for special_token in self.special_tokens:\n                special_tokens_count += document.content.count(special_token)\n            # Percentage of pii_tokens overall\n            pii_percentage = special_tokens_count / total_words if total_words &gt; 0 else 0\n            document.add_pipeline_metadata(\"pii_tokens_percentage\", pii_percentage)\n        except Exception as e:\n            self.logger.error(\n                f\"Error processing {document.filename}, exception {e}\"\n            )\n            document.add_pipeline_metadata(\"pii_tokens_percentage\", 0)\n\n    # Apply filtering if enabled\n    if self.apply_filter:\n        original_len = len(documents)\n        filtered_documents = []\n\n        for doc in documents:\n            pii_percentage = doc.get_pipeline_metadata(\"pii_tokens_percentage\", 0)\n            has_abstract_intro = self._has_abstract_or_introduction(doc)\n            meets_threshold = pii_percentage &gt;= self.threshold\n\n            # Logic:\n            # - If document has abstract/intro: ALWAYS keep (regardless of PII)\n            # - Otherwise: apply threshold-based filtering according to action\n            if has_abstract_intro:\n                should_keep = True\n            elif self.action == \"discard\":\n                # Discard documents with high PII (keep documents with low PII)\n                should_keep = not meets_threshold\n            else:  # action == \"keep\"\n                # Keep only documents with high PII\n                should_keep = meets_threshold\n\n            if should_keep:\n                filtered_documents.append(doc)\n            else:\n                self.logger.debug(\n                    f\"Filtered out {doc.filename} (PII: {pii_percentage:.4f})\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_len - filtered_count\n        percentage_kept = (\n            (filtered_count / original_len) * 100 if original_len &gt; 0 else 0\n        )\n\n        self.logger.info(\n            f\"PII filtering complete: {filtered_count}/{original_len} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n\n        return filtered_documents\n\n    return documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.newline_filter","title":"<code>newline_filter</code>","text":"<p>Newline-based document filtering step.</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep","title":"<code>NewLineFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on the number of newline chunks.</p> <p>This step filters documents based on their newline chunk count with configurable thresholds and policies. A newline chunk is defined as a sequence of text separated by newline characters.</p> <p>Config parameters:</p> <pre><code>- chunks (int): The newline chunk count threshold for filtering\n- comparison (str): Either \"less\" or \"greater\" to compare against threshold\n- action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n</code></pre> <p>Examples:</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep--keep-documents-with-more-than-10-chunks","title":"Keep documents with more than 10 chunks","text":"<p>config: {chunks: 10, comparison: \"greater\", action: \"keep\"}</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep--discard-documents-with-less-than-5-chunks","title":"Discard documents with less than 5 chunks","text":"<p>config: {chunks: 5, comparison: \"less\", action: \"discard\"}</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep--keep-documents-with-less-than-100-chunks-filter-out-heavily-chunked-docs","title":"Keep documents with less than 100 chunks (filter out heavily chunked docs)","text":"<p>config: {chunks: 100, comparison: \"less\", action: \"keep\"}</p> Source code in <code>eve/steps/filters/newline_filter.py</code> <pre><code>class NewLineFilterStep(PipelineStep):\n    \"\"\"Filter documents based on the number of newline chunks.\n\n    This step filters documents based on their newline chunk count with configurable\n    thresholds and policies. A newline chunk is defined as a sequence of text\n    separated by newline characters.\n\n    Config parameters:\n\n        - chunks (int): The newline chunk count threshold for filtering\n        - comparison (str): Either \"less\" or \"greater\" to compare against threshold\n        - action (str): Either \"keep\" or \"discard\" - what to do with documents matching the condition\n\n    Examples:\n        # Keep documents with more than 10 chunks\n        config: {chunks: 10, comparison: \"greater\", action: \"keep\"}\n\n        # Discard documents with less than 5 chunks\n        config: {chunks: 5, comparison: \"less\", action: \"discard\"}\n\n        # Keep documents with less than 100 chunks (filter out heavily chunked docs)\n        config: {chunks: 100, comparison: \"less\", action: \"keep\"}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"NewLineFilter\")\n\n        # Validate required config parameters\n        if \"chunks\" not in config:\n            raise ValueError(\"NewLineFilterStep requires 'chunks' parameter in config\")\n\n        self.chunk_threshold = config.get(\"chunks\")\n        self.comparison = config.get(\"comparison\", \"greater\").lower()\n        self.action = config.get(\"action\", \"keep\").lower()\n\n        # Validate comparison parameter\n        if self.comparison not in [\"less\", \"greater\"]:\n            raise ValueError(f\"Invalid comparison '{self.comparison}'. Must be 'less' or 'greater'\")\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\")\n\n        self.logger.info(\n            f\"Initialized NewLineFilter: {self.action} documents with \"\n            f\"{self.comparison} than {self.chunk_threshold} chunks\"\n        )\n\n    def _get_chunk_count(self, document: Document) -&gt; int:\n        \"\"\"Get the number of newline characters in a document.\n\n        Counts the actual newline characters (\\n) in the document content.\n\n        Args:\n            document: Document to count newlines for\n\n        Returns:\n            Number of newline characters in the document\n        \"\"\"\n        # Count newline characters\n        return document.content.count('\\n')\n\n    def _meets_chunk_condition(self, document: Document) -&gt; bool:\n        \"\"\"Check if document meets the chunk count condition.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if document chunk count meets the comparison condition\n        \"\"\"\n        chunk_count = self._get_chunk_count(document)\n\n        if self.comparison == \"greater\":\n            return chunk_count &gt; self.chunk_threshold\n        else:  # \"less\"\n            return chunk_count &lt; self.chunk_threshold\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute newline chunk filtering on documents.\n\n        Args:\n            documents: List of documents to filter\n\n        Returns:\n            Filtered list of documents based on chunk count criteria\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to filter\")\n            return documents\n\n        original_count = len(documents)\n\n        # Add chunk count to pipeline metadata for all documents\n        for document in documents:\n            chunk_count = self._get_chunk_count(document)\n            document.add_pipeline_metadata(\"newline_count\", chunk_count)\n\n        # Apply filtering based on action\n        filtered_documents = []\n\n        for document in documents:\n            meets_condition = self._meets_chunk_condition(document)\n\n            # Keep document if:\n            # - action is \"keep\" AND condition is met\n            # - action is \"discard\" AND condition is NOT met\n            should_keep = (self.action == \"keep\" and meets_condition) or \\\n                         (self.action == \"discard\" and not meets_condition)\n\n            if should_keep:\n                filtered_documents.append(document)\n            else:\n                chunk_count = self._get_chunk_count(document)\n                self.logger.debug(\n                    f\"Filtered out {document.filename} ({chunk_count} chunks)\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_count - filtered_count\n\n        # Log statistics\n        if original_count &gt; 0:\n            percentage_kept = (filtered_count / original_count) * 100\n            self.logger.info(\n                f\"NewLine filtering complete: {filtered_count}/{original_count} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n        else:\n            self.logger.info(\"No documents were processed\")\n\n        return filtered_documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.newline_filter.NewLineFilterStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute newline chunk filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to filter</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents based on chunk count criteria</p> Source code in <code>eve/steps/filters/newline_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute newline chunk filtering on documents.\n\n    Args:\n        documents: List of documents to filter\n\n    Returns:\n        Filtered list of documents based on chunk count criteria\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to filter\")\n        return documents\n\n    original_count = len(documents)\n\n    # Add chunk count to pipeline metadata for all documents\n    for document in documents:\n        chunk_count = self._get_chunk_count(document)\n        document.add_pipeline_metadata(\"newline_count\", chunk_count)\n\n    # Apply filtering based on action\n    filtered_documents = []\n\n    for document in documents:\n        meets_condition = self._meets_chunk_condition(document)\n\n        # Keep document if:\n        # - action is \"keep\" AND condition is met\n        # - action is \"discard\" AND condition is NOT met\n        should_keep = (self.action == \"keep\" and meets_condition) or \\\n                     (self.action == \"discard\" and not meets_condition)\n\n        if should_keep:\n            filtered_documents.append(document)\n        else:\n            chunk_count = self._get_chunk_count(document)\n            self.logger.debug(\n                f\"Filtered out {document.filename} ({chunk_count} chunks)\"\n            )\n\n    filtered_count = len(filtered_documents)\n    removed_count = original_count - filtered_count\n\n    # Log statistics\n    if original_count &gt; 0:\n        percentage_kept = (filtered_count / original_count) * 100\n        self.logger.info(\n            f\"NewLine filtering complete: {filtered_count}/{original_count} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n    else:\n        self.logger.info(\"No documents were processed\")\n\n    return filtered_documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.reference_filter","title":"<code>reference_filter</code>","text":"<p>Reference and acknowledgement filtering step.</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep","title":"<code>ReferenceFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents containing references or acknowledgements.</p> <p>This step removes documents that contain reference or acknowledgement sections, checking both in document headers and in the document text content.</p> <p>The filter checks for:</p> <ul> <li>Headers containing: \"reference\", \"references\", \"acknowledgement\", \"acknowledgements\"</li> <li>Text content containing markdown headers with these keywords</li> </ul> <p>Config parameters:</p> <pre><code>- action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n    - \"discard\": Remove documents with references/acknowledgements\n    - \"keep\": Keep only documents with references/acknowledgements\n</code></pre> <p>Examples:</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep--remove-documents-with-references-or-acknowledgements-default-behavior","title":"Remove documents with references or acknowledgements (default behavior)","text":"<p>config: {action: \"discard\"}</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep--keep-only-documents-with-references-or-acknowledgements","title":"Keep only documents with references or acknowledgements","text":"<p>config: {action: \"keep\"}</p> Source code in <code>eve/steps/filters/reference_filter.py</code> <pre><code>class ReferenceFilterStep(PipelineStep):\n    \"\"\"Filter documents containing references or acknowledgements.\n\n    This step removes documents that contain reference or acknowledgement sections,\n    checking both in document headers and in the document text content.\n\n    The filter checks for:\n\n    - Headers containing: \"reference\", \"references\", \"acknowledgement\", \"acknowledgements\"\n    - Text content containing markdown headers with these keywords\n\n    Config parameters:\n\n        - action (str): Either \"keep\" or \"discard\" (default: \"discard\")\n            - \"discard\": Remove documents with references/acknowledgements\n            - \"keep\": Keep only documents with references/acknowledgements\n\n    Examples:\n        # Remove documents with references or acknowledgements (default behavior)\n        config: {action: \"discard\"}\n\n        # Keep only documents with references or acknowledgements\n        config: {action: \"keep\"}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        super().__init__(config, name=\"ReferenceFilter\")\n\n        self.action = config.get(\"action\", \"discard\").lower()\n\n        # Validate action parameter\n        if self.action not in [\"keep\", \"discard\"]:\n            raise ValueError(f\"Invalid action '{self.action}'. Must be 'keep' or 'discard'\")\n\n        # Regex patterns for detecting references and acknowledgements in text\n        # Allow for leading whitespace before the header\n        self.reference_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*references?\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n        self.acknowledgement_header_regex = re.compile(\n            r\"^\\s*#{1,6}\\s*acknowledgements?\\b.*$\", re.IGNORECASE | re.MULTILINE\n        )\n\n        # Keywords to check in headers\n        self.reference_keywords = [\"reference\", \"references\"]\n        self.acknowledgement_keywords = [\"acknowledgement\", \"acknowledgements\"]\n\n        self.logger.info(\n            f\"Initialized ReferenceFilter: {self.action} documents with references/acknowledgements\"\n        )\n\n    def _has_reference_in_headers(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has reference in headers metadata.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if any header contains reference keywords\n        \"\"\"\n        if \"headers\" not in document.metadata:\n            return False\n\n        headers = document.metadata[\"headers\"]\n        if not isinstance(headers, list):\n            return False\n\n        return any(\n            h.strip().lower() in self.reference_keywords\n            for h in headers\n        )\n\n    def _has_acknowledgement_in_headers(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has acknowledgement in headers metadata.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if any header contains acknowledgement keywords\n        \"\"\"\n        if \"headers\" not in document.metadata:\n            return False\n\n        headers = document.metadata[\"headers\"]\n        if not isinstance(headers, list):\n            return False\n\n        return any(\n            h.strip().lower() in self.acknowledgement_keywords\n            for h in headers\n        )\n\n    def _has_reference_in_text(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has reference header in text content.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if text contains reference markdown header\n        \"\"\"\n        return bool(self.reference_header_regex.search(document.content))\n\n    def _has_acknowledgement_in_text(self, document: Document) -&gt; bool:\n        \"\"\"Check if document has acknowledgement header in text content.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if text contains acknowledgement markdown header\n        \"\"\"\n        return bool(self.acknowledgement_header_regex.search(document.content))\n\n    def _contains_reference_or_acknowledgement(self, document: Document) -&gt; bool:\n        \"\"\"Check if document contains references or acknowledgements.\n\n        Checks both headers metadata and text content.\n\n        Args:\n            document: Document to check\n\n        Returns:\n            True if document contains references or acknowledgements\n        \"\"\"\n        return (\n            self._has_reference_in_headers(document) or\n            self._has_acknowledgement_in_headers(document) or\n            self._has_reference_in_text(document) or\n            self._has_acknowledgement_in_text(document)\n        )\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute reference/acknowledgement filtering on documents.\n\n        Args:\n            documents: List of documents to filter\n\n        Returns:\n            Filtered list of documents based on reference/acknowledgement criteria\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to filter\")\n            return documents\n\n        original_count = len(documents)\n        filtered_documents = []\n\n        for document in documents:\n            contains_ref_ack = self._contains_reference_or_acknowledgement(document)\n\n            # Keep document if:\n            # - action is \"discard\" AND document does NOT contain ref/ack\n            # - action is \"keep\" AND document DOES contain ref/ack\n            should_keep = (self.action == \"discard\" and not contains_ref_ack) or \\\n                         (self.action == \"keep\" and contains_ref_ack)\n\n            if should_keep:\n                filtered_documents.append(document)\n            else:\n                self.logger.debug(\n                    f\"Filtered out {document.filename} (contains ref/ack: {contains_ref_ack})\"\n                )\n\n        filtered_count = len(filtered_documents)\n        removed_count = original_count - filtered_count\n\n        # Log statistics\n        if original_count &gt; 0:\n            percentage_kept = (filtered_count / original_count) * 100\n            self.logger.info(\n                f\"Reference/Acknowledgement filtering complete: {filtered_count}/{original_count} documents kept \"\n                f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n            )\n        else:\n            self.logger.info(\"No documents were processed\")\n\n        return filtered_documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.reference_filter.ReferenceFilterStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute reference/acknowledgement filtering on documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to filter</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>Filtered list of documents based on reference/acknowledgement criteria</p> Source code in <code>eve/steps/filters/reference_filter.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute reference/acknowledgement filtering on documents.\n\n    Args:\n        documents: List of documents to filter\n\n    Returns:\n        Filtered list of documents based on reference/acknowledgement criteria\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to filter\")\n        return documents\n\n    original_count = len(documents)\n    filtered_documents = []\n\n    for document in documents:\n        contains_ref_ack = self._contains_reference_or_acknowledgement(document)\n\n        # Keep document if:\n        # - action is \"discard\" AND document does NOT contain ref/ack\n        # - action is \"keep\" AND document DOES contain ref/ack\n        should_keep = (self.action == \"discard\" and not contains_ref_ack) or \\\n                     (self.action == \"keep\" and contains_ref_ack)\n\n        if should_keep:\n            filtered_documents.append(document)\n        else:\n            self.logger.debug(\n                f\"Filtered out {document.filename} (contains ref/ack: {contains_ref_ack})\"\n            )\n\n    filtered_count = len(filtered_documents)\n    removed_count = original_count - filtered_count\n\n    # Log statistics\n    if original_count &gt; 0:\n        percentage_kept = (filtered_count / original_count) * 100\n        self.logger.info(\n            f\"Reference/Acknowledgement filtering complete: {filtered_count}/{original_count} documents kept \"\n            f\"({percentage_kept:.2f}%), {removed_count} documents removed\"\n        )\n    else:\n        self.logger.info(\"No documents were processed\")\n\n    return filtered_documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.perplexity","title":"<code>perplexity</code>","text":"<p>Perplexity-based document filtering step.</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep","title":"<code>PerplexityFilterStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Filter documents based on perplexity scores calculated by a language model.</p> <p>This step computes perplexity scores for documents using a causal language model and optionally filters them based on a threshold. Lower perplexity indicates more natural, coherent text.</p> <p>Config parameters:</p> <pre><code>- model_name (str): Hugging Face model name (default: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\nstride (int): Stride for sliding window perplexity calculation (default: 128)\n- batch_size (int): Batch size for model inference (default: 128)\n- max_length (int): Maximum sequence length for model (default: 1024)\n- threshold (float): Perplexity threshold for filtering (default: 0.0)\n- enable_threshold (bool): Whether to apply threshold-based filtering (default: False)\n</code></pre> <p>Examples:</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep--calculate-perplexity-without-filtering","title":"Calculate perplexity without filtering","text":"<p>config: {model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", enable_threshold: false}</p>"},{"location":"pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep--keep-only-documents-with-perplexity-below-50","title":"Keep only documents with perplexity below 50","text":"<p>config: {threshold: 50.0, enable_threshold: true}</p> Source code in <code>eve/steps/filters/perplexity.py</code> <pre><code>class PerplexityFilterStep(PipelineStep):\n    \"\"\"Filter documents based on perplexity scores calculated by a language model.\n\n    This step computes perplexity scores for documents using a causal language model\n    and optionally filters them based on a threshold. Lower perplexity indicates more\n    natural, coherent text.\n\n    Config parameters:\n\n        - model_name (str): Hugging Face model name (default: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n        stride (int): Stride for sliding window perplexity calculation (default: 128)\n        - batch_size (int): Batch size for model inference (default: 128)\n        - max_length (int): Maximum sequence length for model (default: 1024)\n        - threshold (float): Perplexity threshold for filtering (default: 0.0)\n        - enable_threshold (bool): Whether to apply threshold-based filtering (default: False)\n\n    Examples:\n        # Calculate perplexity without filtering\n        config: {model_name: \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", enable_threshold: false}\n\n        # Keep only documents with perplexity below 50\n        config: {threshold: 50.0, enable_threshold: true}\n    \"\"\"\n\n    def __init__(self, config: dict):\n        \"\"\"Initialize the perplexity filter step.\n\n        Args:\n            config: Configuration dictionary containing model and filtering parameters\n        \"\"\"\n        super().__init__(config, name=\"PerplexityFilterStep\")\n\n        self.tokenizer = None\n        self.model = None\n        self.model_name = config.get(\"model_name\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n        self.stride = config.get(\"stride\", 128)\n        self.batch_size = config.get(\"batch_size\", 128)\n        self.max_length = config.get(\"max_length\", 1024)\n        self.threshold = config.get(\"threshold\", 0.0)\n\n        # Enable filtering on threshold\n        self.enable_filter = config.get(\"enable_threshold\", False)\n\n        self.init_model_tokenizer()\n\n    def init_model_tokenizer(self):\n        \"\"\"Initialize the language model and tokenizer.\n\n        Loads the model from Hugging Face and sets up the tokenizer with\n        appropriate padding configuration.\n        \"\"\"\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name, device_map=\"auto\", torch_dtype=\"auto\"\n        )\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute perplexity calculation and optional filtering on documents.\n\n        Computes perplexity scores for all documents and adds them to metadata.\n        If enable_threshold is True, filters documents based on the perplexity threshold.\n\n        Args:\n            documents: List of documents to process\n\n        Returns:\n            List of documents (filtered if enable_threshold is True, otherwise all documents\n            with perplexity scores in metadata)\n        \"\"\"\n        try:\n            for doc in documents:\n                ppl = perplexity(\n                    [doc.content],\n                    self.model,\n                    self.tokenizer,\n                    self.stride,\n                    self.batch_size,\n                    self.max_length,\n                )\n                doc.metadata[\"perplexity\"] = ppl\n        except Exception as e:\n            self.logger.warning(\n                f\"Failed computing ppl for {doc.filename}, exception {e}\"\n            )\n\n        if self.enable_filter:\n            documents = [\n                doc for doc in documents if doc.metadata[\"perplexity\"] &lt;= self.threshold\n            ]\n\n        return documents\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep.__init__","title":"<code>__init__(config: dict)</code>","text":"<p>Initialize the perplexity filter step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing model and filtering parameters</p> required Source code in <code>eve/steps/filters/perplexity.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the perplexity filter step.\n\n    Args:\n        config: Configuration dictionary containing model and filtering parameters\n    \"\"\"\n    super().__init__(config, name=\"PerplexityFilterStep\")\n\n    self.tokenizer = None\n    self.model = None\n    self.model_name = config.get(\"model_name\", \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n    self.stride = config.get(\"stride\", 128)\n    self.batch_size = config.get(\"batch_size\", 128)\n    self.max_length = config.get(\"max_length\", 1024)\n    self.threshold = config.get(\"threshold\", 0.0)\n\n    # Enable filtering on threshold\n    self.enable_filter = config.get(\"enable_threshold\", False)\n\n    self.init_model_tokenizer()\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep.init_model_tokenizer","title":"<code>init_model_tokenizer()</code>","text":"<p>Initialize the language model and tokenizer.</p> <p>Loads the model from Hugging Face and sets up the tokenizer with appropriate padding configuration.</p> Source code in <code>eve/steps/filters/perplexity.py</code> <pre><code>def init_model_tokenizer(self):\n    \"\"\"Initialize the language model and tokenizer.\n\n    Loads the model from Hugging Face and sets up the tokenizer with\n    appropriate padding configuration.\n    \"\"\"\n    self.model = AutoModelForCausalLM.from_pretrained(\n        self.model_name, device_map=\"auto\", torch_dtype=\"auto\"\n    )\n    self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n    self.tokenizer.pad_token = self.tokenizer.eos_token\n</code></pre>"},{"location":"pipeline-stages/filters/#eve.steps.filters.perplexity.PerplexityFilterStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute perplexity calculation and optional filtering on documents.</p> <p>Computes perplexity scores for all documents and adds them to metadata. If enable_threshold is True, filters documents based on the perplexity threshold.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of documents to process</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of documents (filtered if enable_threshold is True, otherwise all documents</p> <code>List[Document]</code> <p>with perplexity scores in metadata)</p> Source code in <code>eve/steps/filters/perplexity.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute perplexity calculation and optional filtering on documents.\n\n    Computes perplexity scores for all documents and adds them to metadata.\n    If enable_threshold is True, filters documents based on the perplexity threshold.\n\n    Args:\n        documents: List of documents to process\n\n    Returns:\n        List of documents (filtered if enable_threshold is True, otherwise all documents\n        with perplexity scores in metadata)\n    \"\"\"\n    try:\n        for doc in documents:\n            ppl = perplexity(\n                [doc.content],\n                self.model,\n                self.tokenizer,\n                self.stride,\n                self.batch_size,\n                self.max_length,\n            )\n            doc.metadata[\"perplexity\"] = ppl\n    except Exception as e:\n        self.logger.warning(\n            f\"Failed computing ppl for {doc.filename}, exception {e}\"\n        )\n\n    if self.enable_filter:\n        documents = [\n            doc for doc in documents if doc.metadata[\"perplexity\"] &lt;= self.threshold\n        ]\n\n    return documents\n</code></pre>"},{"location":"pipeline-stages/metadata-extraction/","title":"Metadata Extraction Stage","text":"<p>The metadata extraction stage automatically identifies and extracts structured metadata from documents.</p>"},{"location":"pipeline-stages/metadata-extraction/#extracted-metadata-fields","title":"Extracted Metadata Fields","text":""},{"location":"pipeline-stages/metadata-extraction/#document-identification","title":"Document Identification","text":"<ul> <li>title: Document title</li> <li>authors: List of author names</li> <li>doi: Digital Object Identifier</li> <li>url: Source URL or link</li> <li>year: Publication year</li> <li>journal: Journal or publication name</li> <li>publisher: Publisher name</li> </ul>"},{"location":"pipeline-stages/metadata-extraction/#extraction-methods","title":"Extraction Methods","text":""},{"location":"pipeline-stages/metadata-extraction/#pdf-metadata-extraction","title":"PDF Metadata Extraction","text":"<p>Setup MonkeyOCR using the bash file under the <code>\\server</code> directory. Then run the extractions using this command <code>python3 parse.py &lt;dir&gt; --pred-abandon</code> You will see the predictions stored in the MonkeyOCR folder. You can then run the metadata extraction pipeline given below -</p> <pre><code>pipeline:\n  batch_size: 2\n  inputs:\n    path: \"htmls\" # path to the folder\n  stages:\n    - name: metadata\n      config:\n        enabled_formats: [\"pdf\", \"html\", \"txt\", \"md\"]\n\n    - name: export\n      config: { format: \"jsonl\", output_dir: \"output\"}\n</code></pre> <ol> <li>We first extract text from the first page of the PDF files using MonkeyOCR. The doi and the title are usually present within the first page of the document.</li> <li>We extract dois using handwritten regex patterns<ul> <li>if the file is from arXiv, we invoke the arXiv API to extract metadata.</li> <li>if the file is from other publishers, we invoke the crossref API to extract metadata.</li> </ul> </li> <li>Fallback - if doi is not present, we extract the title and then invoke the crossref API using the title to extract the metadata.</li> </ol>"},{"location":"pipeline-stages/metadata-extraction/#other-format-extraction","title":"Other format Extraction","text":"<p>For other documents like HTML, TXT, JSON, the extractor uses handwritten regex patterns to extract the document title and the URL of the page.</p>"},{"location":"pipeline-stages/metadata-extraction/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"pipeline-stages/metadata-extraction/#enabled_formats","title":"enabled_formats","text":"<ul> <li>Type: List</li> <li>Default: <code>[\"pdf\", \"html\", \"txt\", \"md\"]</code></li> <li>Description: The list of file formats to process.</li> </ul>"},{"location":"pipeline-stages/metadata-extraction/#export_metadata","title":"export_metadata","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>true</code></li> <li>Description: Whether to export metadata to JSON file.</li> </ul>"},{"location":"pipeline-stages/metadata-extraction/#metadata_destination","title":"metadata_destination","text":"<ul> <li>Type: String</li> <li>Default: <code>./output</code></li> <li>Description: Directory to save metadata file</li> </ul>"},{"location":"pipeline-stages/metadata-extraction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about document export</li> </ul>"},{"location":"pipeline-stages/pii-removal/","title":"PII Removal Stage","text":"<p>This stage detects and redacts NAMES and EMAILS to protect privacy and ensure compliance. We use the Presidio framework with <code>flair/ner-english-large</code> model to detect the entities.</p>"},{"location":"pipeline-stages/pii-removal/#pii-server","title":"PII Server","text":"<p>You need to setup the PII server found under the <code>/server</code></p> <pre><code>cd server\npython3 pii_server.py\n</code></pre>"},{"location":"pipeline-stages/pii-removal/#configuration","title":"Configuration","text":""},{"location":"pipeline-stages/pii-removal/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: pii\n  config:\n    url: \"http://127.0.0.1:8000\"\n</code></pre>"},{"location":"pipeline-stages/pii-removal/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"pipeline-stages/pii-removal/#url","title":"url","text":"<ul> <li>Type: URL</li> <li>Default: <code>http://127.0.0.1:8000</code></li> <li>Description: The endpoint for the pii server.</li> </ul>"},{"location":"pipeline-stages/pii-removal/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about metadata extraction</li> <li>Configure document export</li> </ul>"},{"location":"pipeline-stages/qdrant/","title":"Qdrant Upload Stage","text":"<p>The Qdrant upload stage generates embeddings for documents and optionally uploads them to a Qdrant vector database for semantic search and retrieval.</p>"},{"location":"pipeline-stages/qdrant/#features","title":"Features","text":"<ul> <li>Dual Mode Operation: Upload to Qdrant database or store embeddings locally</li> <li>Flexible Embedding Sources: Generate new embeddings or use existing ones from metadata</li> <li>Automatic Deduplication: Skips documents already present in the collection</li> <li>Batch Processing: Efficient batch upload with configurable size</li> <li>Optimized Collection Setup: Automatic creation with HNSW indexing and quantization</li> <li>Metadata Indexing: Creates indexes on key fields for efficient filtering</li> <li>Retry Logic: Automatic retry on failures with configurable attempts</li> </ul>"},{"location":"pipeline-stages/qdrant/#operating-modes","title":"Operating Modes","text":""},{"location":"pipeline-stages/qdrant/#qdrant-mode-default","title":"Qdrant Mode (Default)","text":"<p>Uploads document embeddings to a Qdrant vector database:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"qdrant\"\n    vector_store:\n      url: \"http://localhost:6333\"\n      api_key: \"your-api-key\"  # Optional\n      collection_name: \"my_documents\"\n      batch_size: 100\n      vector_size: 768\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"BAAI/bge-base-en-v1.5\"\n      timeout: 300\n      api_key: \"EMPTY\"  # Optional\n</code></pre>"},{"location":"pipeline-stages/qdrant/#local-mode","title":"Local Mode","text":"<p>Generates embeddings and stores them in document metadata without uploading:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"local\"\n    batch_size: 10\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"BAAI/bge-base-en-v1.5\"\n      timeout: 300\n</code></pre>"},{"location":"pipeline-stages/qdrant/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"pipeline-stages/qdrant/#mode-configuration","title":"Mode Configuration","text":""},{"location":"pipeline-stages/qdrant/#mode","title":"mode","text":"<ul> <li>Type: String</li> <li>Default: <code>\"qdrant\"</code></li> <li>Options: <code>\"qdrant\"</code>, <code>\"local\"</code></li> <li>Description: Operating mode - upload to Qdrant database or store embeddings locally</li> </ul>"},{"location":"pipeline-stages/qdrant/#use_existing_embeddings","title":"use_existing_embeddings","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: Use embeddings already stored in <code>document.embedding</code> field instead of generating new ones</li> </ul>"},{"location":"pipeline-stages/qdrant/#upload_pipeline_metadata","title":"upload_pipeline_metadata","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: Include pipeline processing metadata in the Qdrant payload</li> </ul>"},{"location":"pipeline-stages/qdrant/#batch_size","title":"batch_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>10</code> (local mode)</li> <li>Description: Number of documents to process in each batch (for local mode only)</li> </ul>"},{"location":"pipeline-stages/qdrant/#vector-store-configuration-qdrant-mode","title":"Vector Store Configuration (Qdrant Mode)","text":""},{"location":"pipeline-stages/qdrant/#vector_storeurl","title":"vector_store.url","text":"<ul> <li>Type: String</li> <li>Default: <code>\"http://localhost:6333\"</code></li> <li>Description: URL of the Qdrant instance</li> </ul>"},{"location":"pipeline-stages/qdrant/#vector_storeapi_key","title":"vector_store.api_key","text":"<ul> <li>Type: String</li> <li>Default: None</li> <li>Description: API key for Qdrant authentication (optional for local instances)</li> </ul>"},{"location":"pipeline-stages/qdrant/#vector_storecollection_name","title":"vector_store.collection_name","text":"<ul> <li>Type: String</li> <li>Required: Yes (for Qdrant mode)</li> <li>Description: Name of the target collection in Qdrant</li> </ul>"},{"location":"pipeline-stages/qdrant/#vector_storebatch_size","title":"vector_store.batch_size","text":"<ul> <li>Type: Integer</li> <li>Required: Yes (for Qdrant mode)</li> <li>Description: Number of documents to upload per batch</li> </ul>"},{"location":"pipeline-stages/qdrant/#vector_storevector_size","title":"vector_store.vector_size","text":"<ul> <li>Type: Integer</li> <li>Required: Yes (for Qdrant mode)</li> <li>Description: Dimension of embedding vectors (must match model output)</li> </ul>"},{"location":"pipeline-stages/qdrant/#embedder-configuration","title":"Embedder Configuration","text":""},{"location":"pipeline-stages/qdrant/#embedderurl","title":"embedder.url","text":"<ul> <li>Type: String</li> <li>Required: Yes (unless <code>use_existing_embeddings: true</code>)</li> <li>Description: URL of the VLLM embedding server</li> </ul>"},{"location":"pipeline-stages/qdrant/#embeddermodel_name","title":"embedder.model_name","text":"<ul> <li>Type: String</li> <li>Required: Yes (unless <code>use_existing_embeddings: true</code>)</li> <li>Description: Name of the embedding model</li> </ul>"},{"location":"pipeline-stages/qdrant/#embeddertimeout","title":"embedder.timeout","text":"<ul> <li>Type: Integer</li> <li>Default: <code>300</code></li> <li>Description: Request timeout in seconds</li> </ul>"},{"location":"pipeline-stages/qdrant/#embedderapi_key","title":"embedder.api_key","text":"<ul> <li>Type: String</li> <li>Default: <code>\"EMPTY\"</code></li> <li>Description: API key for VLLM authentication (use \"EMPTY\" for local servers)</li> </ul>"},{"location":"pipeline-stages/qdrant/#collection-optimization","title":"Collection Optimization","text":"<p>When creating a new collection, the step automatically applies optimized settings for production use. These settings balance search quality, speed, and resource usage for large-scale document collections.</p> <p>Learn more about Qdrant Collections \u2192</p> Vector Configuration  #### Distance Metric: COSINE  Cosine similarity measures the angle between vectors, making it ideal for text embeddings where the direction matters more than magnitude. This is the standard choice for semantic search applications.  [Read about Distance Metrics \u2192](https://qdrant.tech/documentation/concepts/search/#metrics)  #### On-Disk Storage: Enabled  Stores vectors on disk rather than in RAM, significantly reducing memory requirements for large collections. While slightly slower than in-memory storage, this allows you to store millions of vectors affordably.  #### Shards: 8  Distributes data across 8 shards for parallel processing. More shards improve write throughput and allow better resource utilization, especially important for large datasets.  [Learn about Sharding \u2192](https://qdrant.tech/documentation/guides/distributed_deployment/)"},{"location":"pipeline-stages/qdrant/#hnsw-index-parameters","title":"HNSW Index Parameters","text":"<p>HNSW (Hierarchical Navigable Small World) is the indexing algorithm that enables fast approximate nearest neighbor search.</p> <p>Deep dive into HNSW \u2192</p>"},{"location":"pipeline-stages/qdrant/#m-16","title":"m: 16","text":"<p>Number of bidirectional links created for each node. Higher values improve search quality but increase memory usage and indexing time. 16 is a balanced choice for most applications.</p> <ul> <li>Lower (4-8): Less memory, faster indexing, slightly lower recall</li> <li>Higher (32-64): Better recall, more memory, slower indexing</li> </ul>"},{"location":"pipeline-stages/qdrant/#ef_construct-128","title":"ef_construct: 128","text":"<p>Size of the dynamic candidate list during index construction. Higher values produce better index quality but take longer to build. 128 provides good quality without excessive build time.</p> <ul> <li>Lower (64): Faster indexing, slightly lower search quality</li> <li>Higher (256-512): Better search quality, slower indexing</li> </ul>"},{"location":"pipeline-stages/qdrant/#full_scan_threshold-10000","title":"full_scan_threshold: 10,000","text":"<p>When collection size is below this threshold, Qdrant uses exact (brute-force) search instead of the HNSW index. Exact search is faster for small collections.</p>"},{"location":"pipeline-stages/qdrant/#max_indexing_threads-2","title":"max_indexing_threads: 2","text":"<p>Limits CPU cores used during indexing. Prevents indexing from consuming all available resources.</p>"},{"location":"pipeline-stages/qdrant/#on_disk-enabled","title":"on_disk: Enabled","text":"<p>Stores the HNSW graph on disk to reduce RAM usage. Essential for collections with millions of vectors.</p>"},{"location":"pipeline-stages/qdrant/#quantization","title":"Quantization","text":"<p>Binary quantization compresses vectors from 32-bit floats to 1-bit representations, reducing memory by ~32x with minimal quality loss. This makes it possible to store much larger collections.</p> <p>Learn about Quantization \u2192</p>"},{"location":"pipeline-stages/qdrant/#type-binary-quantization","title":"Type: Binary Quantization","text":"<p>Converts vector components to binary (0 or 1) for massive memory savings. The original vectors are still used for final re-ranking, so search quality remains high.</p> <p>Benefits:</p> <ul> <li>32x memory reduction (32-bit float \u2192 1-bit)</li> <li>Faster distance calculations</li> <li>More vectors fit in RAM for better performance</li> <li>Negligible impact on search quality (typically &lt;2% recall loss)</li> </ul>"},{"location":"pipeline-stages/qdrant/#always_ram-false","title":"always_ram: false","text":"<p>Allows quantized vectors to be stored on disk when needed, rather than always keeping them in RAM. This provides flexibility for very large collections.</p>"},{"location":"pipeline-stages/qdrant/#optimizer-settings","title":"Optimizer Settings","text":"<p>These settings control how Qdrant manages and optimizes data segments over time.</p> <p>Learn about Storage Optimization \u2192</p>"},{"location":"pipeline-stages/qdrant/#indexing_threshold-20000","title":"indexing_threshold: 20,000","text":"<p>Build HNSW index when segment reaches this size. Smaller values create indexes sooner but may cause more frequent rebuilds.</p>"},{"location":"pipeline-stages/qdrant/#memmap_threshold-5000","title":"memmap_threshold: 5,000","text":"<p>Use memory-mapped files for segments larger than this. Memory mapping allows efficient disk-based storage without loading everything into RAM.</p>"},{"location":"pipeline-stages/qdrant/#max_segment_size-5000000","title":"max_segment_size: 5,000,000","text":"<p>Maximum vectors per segment. Larger segments are more memory-efficient but may slow down some operations.</p>"},{"location":"pipeline-stages/qdrant/#max_optimization_threads-2","title":"max_optimization_threads: 2","text":"<p>CPU cores dedicated to background optimization tasks. Prevents optimization from impacting query performance.</p>"},{"location":"pipeline-stages/qdrant/#payload-indexes","title":"Payload Indexes","text":"<p>Payload indexes enable fast filtering on metadata fields, similar to database indexes. Without these, filtering requires scanning all documents.</p> <p>Learn about Payload Indexes \u2192</p> <p>The pipeline automatically creates indexes on common academic metadata fields:</p>"},{"location":"pipeline-stages/qdrant/#text-indexes-title-journal","title":"Text Indexes (title, journal)","text":"<p>Enable full-text search and filtering on text fields. The word tokenizer splits text into searchable terms.</p> <ul> <li>min_token_len: Minimum word length to index</li> <li>max_token_len: Maximum word length to index</li> <li>lowercase: Normalize to lowercase for case-insensitive search</li> </ul> <p>Example filters:</p> <ul> <li>Find papers with \"neural\" in title</li> <li>Filter by journal name</li> <li>Combine with vector search for semantic + keyword search</li> </ul>"},{"location":"pipeline-stages/qdrant/#integer-indexes-year-n_citations","title":"Integer Indexes (year, n_citations)","text":"<p>Enable efficient range queries on numeric fields.</p> <p>Example filters:</p> <ul> <li>Papers published after 2020</li> <li>Papers with &gt;100 citations</li> <li>Combine filters: papers from 2015-2023 with &gt;50 citations</li> </ul> <p>Performance Impact:</p> <ul> <li>Indexes speed up filtering by 100-1000x</li> <li>Small storage overhead (~10-20% of original data)</li> <li>Slightly slower writes (indexes must be updated)</li> </ul>"},{"location":"pipeline-stages/qdrant/#stage-behavior","title":"Stage Behavior","text":""},{"location":"pipeline-stages/qdrant/#metadata-handling","title":"Metadata Handling","text":"<p>Document metadata is prepared for storage:</p> <ul> <li>content: Document text content</li> <li>metadata: All user metadata fields (unwrapped at root level)</li> <li>pipeline_metadata: Processing metadata (if <code>upload_pipeline_metadata: true</code>)</li> </ul> <p>Type conversions:</p> <ul> <li><code>year</code> field converted to integer</li> <li><code>title</code> field converted to string</li> </ul>"},{"location":"pipeline-stages/qdrant/#error-handling","title":"Error Handling","text":"<ul> <li>Batch uploads retry up to 3 times on failure</li> <li>Failed batches are logged and skipped</li> <li>Individual embedding failures are logged without stopping the pipeline</li> <li>Scroll operations retry with exponential backoff</li> </ul>"},{"location":"pipeline-stages/qdrant/#usage-examples","title":"Usage Examples","text":""},{"location":"pipeline-stages/qdrant/#basic-upload-with-new-embeddings","title":"Basic Upload with New Embeddings","text":"<pre><code>- name: qdrant\n  config:\n    mode: \"qdrant\"\n    vector_store:\n      url: \"http://localhost:6333\"\n      collection_name: \"research_papers\"\n      batch_size: 100\n      vector_size: 768\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"BAAI/bge-base-en-v1.5\"\n</code></pre>"},{"location":"pipeline-stages/qdrant/#upload-with-existing-embeddings","title":"Upload with Existing Embeddings","text":"<p>If embeddings were generated in a previous step:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"qdrant\"\n    use_existing_embeddings: true\n    upload_pipeline_metadata: true\n    vector_store:\n      url: \"http://localhost:6333\"\n      api_key: \"your-api-key\"\n      collection_name: \"processed_docs\"\n      batch_size: 50\n      vector_size: 1024\n</code></pre>"},{"location":"pipeline-stages/qdrant/#local-embedding-generation","title":"Local Embedding Generation","text":"<p>Store embeddings in document metadata without uploading:</p> <pre><code>- name: qdrant\n  config:\n    mode: \"local\"\n    batch_size: 20\n    embedder:\n      url: \"http://localhost:8000\"\n      model_name: \"sentence-transformers/all-MiniLM-L6-v2\"\n      timeout: 600\n</code></pre>"},{"location":"pipeline-stages/qdrant/#vllm-server-setup","title":"VLLM Server Setup","text":"<p>The Qdrant step requires a VLLM server for embedding generation. Start the server:</p> <pre><code>cd server\npython vllm.py\n</code></pre> <p>The server provides an OpenAI-compatible embeddings API at <code>/v1/embeddings</code>.</p>"},{"location":"pipeline-stages/qdrant/#complete-pipeline-example","title":"Complete Pipeline Example","text":"<pre><code>pipeline:\n  inputs:\n    path: \"processed_documents\"\n\n  stages:\n    - name: chunking\n      config:\n        max_chunk_size: 512\n        chunk_overlap: 50\n\n    - name: metadata\n      config:\n        enabled_formats: [\"pdf\", \"markdown\"]\n        enable_scholar_search: true\n\n    - name: qdrant\n      config:\n        mode: \"qdrant\"\n        upload_pipeline_metadata: true\n        vector_store:\n          url: \"http://localhost:6333\"\n          collection_name: \"academic_papers\"\n          batch_size: 100\n          vector_size: 768\n        embedder:\n          url: \"http://localhost:8000\"\n          model_name: \"BAAI/bge-base-en-v1.5\"\n          timeout: 300\n</code></pre>"},{"location":"pipeline-stages/qdrant/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about chunking strategy</li> <li>Configure metadata extraction</li> <li>Explore export options</li> </ul>"},{"location":"pipeline-stages/qdrant/#code-reference","title":"Code Reference","text":""},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step","title":"<code>qdrant_step</code>","text":""},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder","title":"<code>VLLMEmbedder</code>","text":"<p>Client for VLLM embedding server.</p> <p>This class provides a simple interface to interact with a VLLM server that exposes an OpenAI-compatible embeddings API endpoint.</p> Source code in <code>eve/steps/qdrant/qdrant_step.py</code> <pre><code>class VLLMEmbedder:\n    \"\"\"Client for VLLM embedding server.\n\n    This class provides a simple interface to interact with a VLLM server\n    that exposes an OpenAI-compatible embeddings API endpoint.\n    \"\"\"\n\n    def __init__(\n        self, url: str, model_name: str, timeout: int = 300, api_key: str = \"EMPTY\"\n    ):\n        \"\"\"Initialize VLLM embedder client.\n\n        Args:\n            url (str): Base URL of the VLLM server.\n            model_name (str): Name of the embedding model to use.\n            timeout (int, optional): Request timeout in seconds. Defaults to 300.\n            api_key (str, optional): API key for authentication. Use \"EMPTY\" for\n                local servers. Defaults to \"EMPTY\".\n        \"\"\"\n        self.url = url.rstrip(\"/\")\n        self.model_name = model_name\n        self.timeout = timeout\n        self.api_key = api_key\n\n        # Set up headers with API key\n        headers = (\n            {\"Authorization\": f\"Bearer {api_key}\"}\n            if api_key and api_key != \"EMPTY\"\n            else {}\n        )\n        self.client = httpx.Client(timeout=timeout, headers=headers)\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float] | None]:\n        \"\"\"Generate embeddings for a list of texts.\n\n        Sends requests to the VLLM server's /v1/embeddings endpoint to generate\n        embeddings for each text. Failed requests return None for that text.\n\n        Args:\n            texts (List[str]): List of text strings to embed.\n\n        Returns:\n            List[List[float] | None]: List of embedding vectors. Each element is\n                either a list of floats (the embedding) or None if embedding failed.\n        \"\"\"\n        embeddings = []\n        for text in texts:\n            endpoint = f\"{self.url}/v1/embeddings\"\n\n            payload = {\"input\": [text], \"model\": self.model_name, \"encoding_format\": \"float\"}\n\n            try:\n                response = self.client.post(endpoint, json=payload)\n                response.raise_for_status()\n\n                result = response.json()\n\n                # Extract embedding (single document)\n                embedding = result[\"data\"][0][\"embedding\"]\n                embeddings.append(embedding)\n\n            except httpx.HTTPError as e:\n                print(f\"VLLM embedding request failed: {e}\")\n                embeddings.append(None)\n            except KeyError as e:\n                embeddings.append(None)\n                print(f\"Unexpected response format from VLLM server: {e}\")\n        return embeddings\n\n    def __del__(self):\n        \"\"\"Clean up HTTP client.\"\"\"\n        if hasattr(self, \"client\"):\n            self.client.close()\n</code></pre>"},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder.__init__","title":"<code>__init__(url: str, model_name: str, timeout: int = 300, api_key: str = 'EMPTY')</code>","text":"<p>Initialize VLLM embedder client.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Base URL of the VLLM server.</p> required <code>model_name</code> <code>str</code> <p>Name of the embedding model to use.</p> required <code>timeout</code> <code>int</code> <p>Request timeout in seconds. Defaults to 300.</p> <code>300</code> <code>api_key</code> <code>str</code> <p>API key for authentication. Use \"EMPTY\" for local servers. Defaults to \"EMPTY\".</p> <code>'EMPTY'</code> Source code in <code>eve/steps/qdrant/qdrant_step.py</code> <pre><code>def __init__(\n    self, url: str, model_name: str, timeout: int = 300, api_key: str = \"EMPTY\"\n):\n    \"\"\"Initialize VLLM embedder client.\n\n    Args:\n        url (str): Base URL of the VLLM server.\n        model_name (str): Name of the embedding model to use.\n        timeout (int, optional): Request timeout in seconds. Defaults to 300.\n        api_key (str, optional): API key for authentication. Use \"EMPTY\" for\n            local servers. Defaults to \"EMPTY\".\n    \"\"\"\n    self.url = url.rstrip(\"/\")\n    self.model_name = model_name\n    self.timeout = timeout\n    self.api_key = api_key\n\n    # Set up headers with API key\n    headers = (\n        {\"Authorization\": f\"Bearer {api_key}\"}\n        if api_key and api_key != \"EMPTY\"\n        else {}\n    )\n    self.client = httpx.Client(timeout=timeout, headers=headers)\n</code></pre>"},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder.embed_documents","title":"<code>embed_documents(texts: List[str]) -&gt; List[List[float] | None]</code>","text":"<p>Generate embeddings for a list of texts.</p> <p>Sends requests to the VLLM server's /v1/embeddings endpoint to generate embeddings for each text. Failed requests return None for that text.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>List of text strings to embed.</p> required <p>Returns:</p> Type Description <code>List[List[float] | None]</code> <p>List[List[float] | None]: List of embedding vectors. Each element is either a list of floats (the embedding) or None if embedding failed.</p> Source code in <code>eve/steps/qdrant/qdrant_step.py</code> <pre><code>def embed_documents(self, texts: List[str]) -&gt; List[List[float] | None]:\n    \"\"\"Generate embeddings for a list of texts.\n\n    Sends requests to the VLLM server's /v1/embeddings endpoint to generate\n    embeddings for each text. Failed requests return None for that text.\n\n    Args:\n        texts (List[str]): List of text strings to embed.\n\n    Returns:\n        List[List[float] | None]: List of embedding vectors. Each element is\n            either a list of floats (the embedding) or None if embedding failed.\n    \"\"\"\n    embeddings = []\n    for text in texts:\n        endpoint = f\"{self.url}/v1/embeddings\"\n\n        payload = {\"input\": [text], \"model\": self.model_name, \"encoding_format\": \"float\"}\n\n        try:\n            response = self.client.post(endpoint, json=payload)\n            response.raise_for_status()\n\n            result = response.json()\n\n            # Extract embedding (single document)\n            embedding = result[\"data\"][0][\"embedding\"]\n            embeddings.append(embedding)\n\n        except httpx.HTTPError as e:\n            print(f\"VLLM embedding request failed: {e}\")\n            embeddings.append(None)\n        except KeyError as e:\n            embeddings.append(None)\n            print(f\"Unexpected response format from VLLM server: {e}\")\n    return embeddings\n</code></pre>"},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.VLLMEmbedder.__del__","title":"<code>__del__()</code>","text":"<p>Clean up HTTP client.</p> Source code in <code>eve/steps/qdrant/qdrant_step.py</code> <pre><code>def __del__(self):\n    \"\"\"Clean up HTTP client.\"\"\"\n    if hasattr(self, \"client\"):\n        self.client.close()\n</code></pre>"},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.QdrantUploadStep","title":"<code>QdrantUploadStep</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Pipeline step for uploading chunked documents to Qdrant vector database or storing embeddings locally.</p> <p>Supports two modes:</p> <ul> <li>\"qdrant\": Upload embeddings to a Qdrant vector database</li> <li>\"local\": Store embeddings in document metadata without uploading</li> </ul> Source code in <code>eve/steps/qdrant/qdrant_step.py</code> <pre><code>class QdrantUploadStep(PipelineStep):\n    \"\"\"Pipeline step for uploading chunked documents to Qdrant vector database or storing embeddings locally.\n\n    Supports two modes:\n\n    - \"qdrant\": Upload embeddings to a Qdrant vector database\n    - \"local\": Store embeddings in document metadata without uploading\n    \"\"\"\n\n    def __init__(self, config: dict, name: str = \"QdrantUpload\"):\n        \"\"\"Initialize the Qdrant upload step.\n\n        Args:\n            config (dict): Configuration dictionary containing:\n\n                - mode (str, optional): \"qdrant\" or \"local\". Defaults to \"qdrant\".\n                - use_existing_embeddings (bool, optional): If True, use embeddings\n                    from document.embedding field. Defaults to False.\n                - upload_pipeline_metadata (bool, optional): If True, include\n                    pipeline_metadata in Qdrant payload. Defaults to False.\n                - vector_store (dict, required for \"qdrant\" mode):\n                    - url (str): Qdrant instance URL.\n                    - api_key (str, optional): API key for Qdrant authentication.\n                    - collection_name (str): Target collection name.\n                    - batch_size (int): Number of documents per batch.\n                    - vector_size (int): Dimension of embedding vectors.\n                - embedder (dict, required if use_existing_embeddings=False):\n                    - url (str): URL of VLLM embedding server.\n                    - model_name (str): Embedding model identifier.\n                    - timeout (int, optional): Request timeout in seconds. Defaults to 300.\n                    - api_key (str, optional): API key for VLLM. Defaults to \"EMPTY\".\n                - batch_size (int, optional): Batch size for local mode. Defaults to 10.\n            name (str, optional): Name for logging purposes. Defaults to \"QdrantUpload\".\n\n        Raises:\n            ValueError: If mode is not \"qdrant\" or \"local\".\n        \"\"\"\n        super().__init__(config, name)\n\n        # Determine mode: \"qdrant\" or \"local\"\n        self.mode = config.get(\"mode\", \"qdrant\").lower()\n\n        if self.mode not in [\"qdrant\", \"local\"]:\n            raise ValueError(f\"Invalid mode '{self.mode}'. Must be 'qdrant' or 'local'\")\n\n        # Check if we should use existing embeddings from document.embedding field\n        self.use_existing_embeddings = config.get(\"use_existing_embeddings\", False)\n\n        # Check if we should upload pipeline_metadata to Qdrant\n        self.upload_pipeline_metadata = config.get(\"upload_pipeline_metadata\", False)\n\n        # Initialize VLLM embedder only if not using existing embeddings\n        if not self.use_existing_embeddings:\n            embedding_cfg = config[\"embedder\"]\n            self.embedder = VLLMEmbedder(\n                url=embedding_cfg[\"url\"],\n                model_name=embedding_cfg[\"model_name\"],\n                timeout=embedding_cfg.get(\"timeout\", 300),\n                api_key=embedding_cfg.get(\"api_key\", \"EMPTY\"),\n            )\n            self.logger.info(f\"Initialized VLLM embedder at {embedding_cfg['url']}\")\n        else:\n            self.embedder = None\n            self.logger.info(\"Using existing embeddings from document metadata\")\n\n        self.logger.info(f\"Mode: {self.mode}\")\n\n        # Initialize Qdrant-specific configuration only if mode is \"qdrant\"\n        if self.mode == \"qdrant\":\n            vector_store_cfg = config.get(\"vector_store\", {})\n            self.qdrant_url = vector_store_cfg.get(\"url\", \"http://localhost:6333\")\n            self.vector_store_api_key = vector_store_cfg.get(\"api_key\")\n\n            # Get collection configuration\n            self.collection_name = vector_store_cfg[\"collection_name\"]\n            self.batch_size = vector_store_cfg[\"batch_size\"]\n            self.vector_size = vector_store_cfg[\"vector_size\"]\n\n            # Initialize Qdrant client\n            self.client = QdrantClient(\n                url=self.qdrant_url, api_key=self.vector_store_api_key\n            )\n\n            # Ensure collection exists\n            self._ensure_collection()\n            self.existing_ids = self._get_existing_ids()\n        else:\n            # Local mode: set batch size for processing\n            self.batch_size = config.get(\"batch_size\", 10)\n            self.client = None\n\n    def _ensure_collection(self) -&gt; None:\n        \"\"\"Create Qdrant collection if it doesn't exist.\n\n        Creates a new collection with optimized settings including HNSW indexing,\n        binary quantization, and on-disk storage. Also creates payload indexes\n        for efficient filtering.\n        \"\"\"\n        if self.client.collection_exists(self.collection_name):\n            self.logger.info(f\"Collection '{self.collection_name}' already exists\")\n            return\n\n        self.logger.info(f\"Creating collection '{self.collection_name}'\")\n\n        # Create collection with optimized settings\n        self.client.create_collection(\n            collection_name=self.collection_name,\n            vectors_config=models.VectorParams(\n                size=self.vector_size,\n                distance=models.Distance.COSINE,\n                on_disk=True,\n            ),\n            shard_number=8,\n            on_disk_payload=True,\n            quantization_config=models.BinaryQuantization(\n                binary=models.BinaryQuantizationConfig(always_ram=False)\n            ),\n        )\n\n        # Update HNSW configuration\n        self.client.update_collection(\n            collection_name=self.collection_name,\n            hnsw_config=models.HnswConfigDiff(\n                m=16,\n                ef_construct=128,\n                full_scan_threshold=10_000,\n                max_indexing_threads=2,\n                on_disk=True,\n            ),\n        )\n\n        # Update optimizer configuration\n        self.client.update_collection(\n            collection_name=self.collection_name,\n            optimizers_config=models.OptimizersConfigDiff(\n                indexing_threshold=20000,\n                memmap_threshold=5000,\n                deleted_threshold=0.2,\n                vacuum_min_vector_number=1000,\n                default_segment_number=2,\n                max_segment_size=5_000_000,\n                max_optimization_threads=2,\n            ),\n        )\n\n        # Create payload indexes\n        self._create_payload_indexes()\n\n        self.logger.info(\"Collection created and optimized\")\n\n    def _create_payload_indexes(self) -&gt; None:\n        \"\"\"Create indexes on payload fields for efficient filtering.\n\n        Creates text indexes for 'title' and 'journal' fields, and integer\n        indexes for 'year' and 'n_citations' fields to enable fast filtering\n        and searching on these metadata fields.\n        \"\"\"\n        # Text index for title\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"title\",\n            field_schema=models.TextIndexParams(\n                type=\"text\",\n                tokenizer=models.TokenizerType.WORD,\n                min_token_len=2,\n                max_token_len=50,\n                lowercase=True,\n            ),\n        )\n\n        # Integer indexes\n        for field in [\"year\", \"n_citations\"]:\n            self.client.create_payload_index(\n                collection_name=self.collection_name,\n                field_name=field,\n                field_schema=\"integer\",\n            )\n\n        # Text index for journal\n        self.client.create_payload_index(\n            collection_name=self.collection_name,\n            field_name=\"journal\",\n            field_schema=models.TextIndexParams(\n                type=\"text\",\n                tokenizer=models.TokenizerType.WORD,\n                min_token_len=1,\n                max_token_len=50,\n                lowercase=True,\n            ),\n        )\n\n    @staticmethod\n    def _string_to_uint(s: str) -&gt; int:\n        \"\"\"Convert string to unsigned integer using SHA256 hash.\n\n        Args:\n            s (str): Input string to hash.\n\n        Returns:\n            int: Unsigned 64-bit integer derived from the hash.\n        \"\"\"\n        hash_bytes = hashlib.sha256(s.encode(\"utf-8\")).digest()\n        return int.from_bytes(hash_bytes[:8], byteorder=\"big\", signed=False)\n\n    def _get_existing_ids(self) -&gt; Set[int]:\n        \"\"\"Retrieve all existing point IDs from the collection with retry logic.\n\n        Scrolls through the entire collection to fetch all point IDs. Implements\n        retry logic with exponential backoff to handle temporary failures.\n\n        Returns:\n            Set[int]: Set of existing point IDs in the collection.\n\n        Raises:\n            Exception: If scroll request fails after max retries.\n        \"\"\"\n        existing_ids = set()\n        scroll_offset = None\n        max_retries = 3\n        retry_delay = 5\n\n        while True:\n            response = None\n            for attempt in range(max_retries):\n                try:\n                    response = self.client.scroll(\n                        collection_name=self.collection_name,\n                        offset=scroll_offset,\n                        limit=10000,\n                        with_payload=False,\n                        with_vectors=False,\n                        timeout=3000,\n                    )\n                    break  # Success, exit retry loop\n                except Exception as e:\n                    if attempt &lt; max_retries - 1:\n                        self.logger.warning(f\"Scroll request failed (attempt {attempt + 1}/{max_retries}): {e}\")\n                        self.logger.info(f\"Retrying in {retry_delay}s...\")\n                        time.sleep(retry_delay)\n                    else:\n                        self.logger.error(f\"Scroll request failed after {max_retries} attempts: {e}\")\n                        raise\n\n            for point in response[0]:\n                existing_ids.add(point.id)\n\n            if response[1] is None:\n                break\n            scroll_offset = response[1]\n\n        return existing_ids\n\n    def _upload_batch(\n        self, batch_ids: List[int], batch_chunks: List[str], batch_metadata: List[dict], batch_embeddings: List[List[float]] = None\n    ) -&gt; None:\n        \"\"\"Upload a batch of documents to Qdrant.\n\n        Generates embeddings (if not provided) and uploads points to Qdrant.\n        Implements retry logic with up to 3 attempts on failure.\n\n        Args:\n            batch_ids (List[int]): List of unique point IDs.\n            batch_chunks (List[str]): List of text chunks to embed.\n            batch_metadata (List[dict]): List of metadata dictionaries for each point.\n            batch_embeddings (List[List[float]], optional): Pre-computed embeddings\n                to use instead of generating new ones. Defaults to None.\n        \"\"\"\n        # Generate embeddings if not provided\n        if batch_embeddings is None:\n            if self.use_existing_embeddings:\n                self.logger.error(\"use_existing_embeddings=True but no embeddings provided\")\n                return\n\n            try:\n                batch_vectors = self.embedder.embed_documents(batch_chunks)\n            except Exception as e:\n                self.logger.error(f\"Embedding error: {e}\")\n                return\n        else:\n            batch_vectors = batch_embeddings\n\n        points = [\n            PointStruct(id=id_val, vector=vec, payload=meta)\n            for id_val, vec, meta in zip(batch_ids, batch_vectors, batch_metadata)\n        ]\n\n        for attempt in range(3):\n            try:\n                self.client.upload_points(\n                    collection_name=self.collection_name,\n                    points=points,\n                    parallel=10,\n                    max_retries=3,\n                )\n                return\n            except Exception as e:\n                self.logger.error(f\"Error uploading batch (attempt {attempt + 1}): {e}\")\n                time.sleep(10)\n                if attempt &lt; 2:\n                    self.logger.info(\"Retrying...\")\n                else:\n                    self.logger.warning(\"Skipping batch after 3 failed attempts\")\n\n    def _prepare_metadata(self, doc: Document) -&gt; dict:\n        \"\"\"Prepare metadata from Document object for Qdrant storage.\n\n        Extracts and cleans metadata fields, performing type conversions and\n        formatting for Qdrant compatibility. Includes document content, user\n        metadata (unwrapped), and optionally pipeline metadata (wrapped).\n\n        Args:\n            doc (Document): Document object containing content and metadata.\n\n        Returns:\n            dict: Dictionary with cleaned metadata ready for Qdrant payload.\n                Includes 'content' field, all metadata fields at root level,\n                and optionally 'pipeline_metadata' as a nested dict.\n        \"\"\"\n        payload = {}\n\n        # Add content\n        payload[\"content\"] = doc.content\n\n        # Add original metadata fields directly to root level (unwrapped)\n        if doc.metadata:\n            # Clean up metadata types\n            metadata_copy = doc.metadata.copy()\n\n            # Ensure year is an integer if present\n            if \"year\" in metadata_copy:\n                try:\n                    metadata_copy[\"year\"] = int(float(metadata_copy[\"year\"]))\n                except (ValueError, TypeError):\n                    metadata_copy[\"year\"] = None\n\n            # Ensure title is a string if present\n            if \"title\" in metadata_copy:\n                metadata_copy[\"title\"] = str(metadata_copy[\"title\"])\n\n            # Add all metadata fields directly to payload (unwrapped)\n            payload.update(metadata_copy)\n\n        # Add pipeline_metadata as wrapped dict if configured\n        if self.upload_pipeline_metadata and doc.pipeline_metadata:\n            payload[\"pipeline_metadata\"] = doc.pipeline_metadata.copy()\n\n        return payload\n\n    async def execute(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute the upload step.\n\n        Routes to appropriate execution method based on configured mode\n        (local or qdrant).\n\n        Args:\n            documents (List[Document]): List of Document objects to process.\n\n        Returns:\n            List[Document]: The same list of documents passed through for\n                pipeline chaining.\n        \"\"\"\n        if not documents:\n            self.logger.warning(\"No documents to process\")\n            return documents\n\n        self.logger.info(f\"Processing {len(documents)} documents\")\n\n        if self.mode == \"local\":\n            # Local mode: add embeddings to document metadata\n            return await self._execute_local(documents)\n        else:\n            # Qdrant mode: upload to vector database\n            return await self._execute_qdrant(documents)\n\n    async def _execute_local(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute local embedding storage.\n\n        Generates embeddings for documents and stores them in the document.embedding\n        field without uploading to Qdrant. Processes documents in configurable batches.\n\n        Args:\n            documents (List[Document]): List of Document objects to process.\n\n        Returns:\n            List[Document]: Documents with embeddings added to embedding field.\n        \"\"\"\n        self.logger.info(\"Generating embeddings in local mode\")\n\n        # Process in batches\n        for i in tqdm(\n            range(0, len(documents), self.batch_size),\n            desc=\"Generating embeddings\",\n        ):\n            batch = documents[i : i + self.batch_size]\n            batch_texts = [doc.content for doc in batch]\n\n            try:\n                # Generate embeddings\n                batch_embeddings = self.embedder.embed_documents(batch_texts)\n\n                # Add embeddings to document.embedding field\n                for doc, embedding in zip(batch, batch_embeddings):\n                    doc.embedding = embedding\n\n            except Exception as e:\n                self.logger.error(f\"Error generating embeddings for batch: {e}\")\n                # Continue with next batch\n\n        self.logger.info(f\"Successfully generated embeddings for {len(documents)} documents\")\n        return documents\n\n    async def _execute_qdrant(self, documents: List[Document]) -&gt; List[Document]:\n        \"\"\"Execute Qdrant upload mode.\n\n        Prepares document data, generates or extracts embeddings, filters out\n        existing documents, and uploads to Qdrant in batches.\n\n        Args:\n            documents (List[Document]): List of Document objects to upload.\n\n        Returns:\n            List[Document]: The same list of documents passed through for\n                pipeline chaining.\n        \"\"\"\n        # Prepare data for upload\n        ids = []\n        chunks = []\n        metadata = []\n        embeddings = [] if self.use_existing_embeddings else None\n\n        for doc in documents:\n            # Create unique ID based on file path and content hash\n            doc_id = (\n                f\"{doc.filename}_{hashlib.md5(doc.content.encode()).hexdigest()[:8]}\"\n            )\n            ids.append(doc_id)\n            chunks.append(doc.content)\n            metadata.append(self._prepare_metadata(doc))\n\n            # Extract existing embeddings if configured\n            if self.use_existing_embeddings:\n                if doc.embedding is None:\n                    self.logger.error(f\"Document {doc.filename} missing embedding\")\n                    # Skip this document\n                    ids.pop()\n                    chunks.pop()\n                    metadata.pop()\n                    continue\n                embeddings.append(doc.embedding)\n\n        # Convert to uint IDs\n        uint_ids = [self._string_to_uint(id_str) for id_str in ids]\n\n        if self.use_existing_embeddings:\n            to_process = list(zip(uint_ids, chunks, metadata, embeddings, ids))\n        else:\n            to_process = list(zip(uint_ids, chunks, metadata, ids))\n\n        # Filter out existing IDs\n        if self.use_existing_embeddings:\n            to_process = [item for item in to_process if item[0] not in self.existing_ids]\n        else:\n            to_process = [item for item in to_process if item[0] not in self.existing_ids]\n\n        skipped = len(uint_ids) - len(to_process)\n        self.logger.info(f\"Skipping {skipped} existing documents\")\n        self.logger.info(f\"Uploading {len(to_process)} new vectors\")\n\n        # Upload in batches\n        for i in tqdm(\n            range(0, len(to_process), self.batch_size),\n            desc=f\"Uploading to {self.collection_name}\",\n        ):\n            batch = to_process[i : i + self.batch_size]\n\n            if self.use_existing_embeddings:\n                batch_ids = [item[0] for item in batch]\n                batch_chunks = [item[1] for item in batch]\n                batch_metadata = [item[2] for item in batch]\n                batch_embeddings = [item[3] for item in batch]\n                self._upload_batch(batch_ids, batch_chunks, batch_metadata, batch_embeddings)\n            else:\n                batch_ids = [item[0] for item in batch]\n                batch_chunks = [item[1] for item in batch]\n                batch_metadata = [item[2] for item in batch]\n                self._upload_batch(batch_ids, batch_chunks, batch_metadata)\n\n        self.logger.info(f\"Successfully uploaded {len(to_process)} documents\")\n\n        # Return documents for potential further processing\n        return documents\n</code></pre>"},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.QdrantUploadStep.__init__","title":"<code>__init__(config: dict, name: str = 'QdrantUpload')</code>","text":"<p>Initialize the Qdrant upload step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary containing:</p> <ul> <li>mode (str, optional): \"qdrant\" or \"local\". Defaults to \"qdrant\".</li> <li>use_existing_embeddings (bool, optional): If True, use embeddings     from document.embedding field. Defaults to False.</li> <li>upload_pipeline_metadata (bool, optional): If True, include     pipeline_metadata in Qdrant payload. Defaults to False.</li> <li>vector_store (dict, required for \"qdrant\" mode):<ul> <li>url (str): Qdrant instance URL.</li> <li>api_key (str, optional): API key for Qdrant authentication.</li> <li>collection_name (str): Target collection name.</li> <li>batch_size (int): Number of documents per batch.</li> <li>vector_size (int): Dimension of embedding vectors.</li> </ul> </li> <li>embedder (dict, required if use_existing_embeddings=False):<ul> <li>url (str): URL of VLLM embedding server.</li> <li>model_name (str): Embedding model identifier.</li> <li>timeout (int, optional): Request timeout in seconds. Defaults to 300.</li> <li>api_key (str, optional): API key for VLLM. Defaults to \"EMPTY\".</li> </ul> </li> <li>batch_size (int, optional): Batch size for local mode. Defaults to 10.</li> </ul> required <code>name</code> <code>str</code> <p>Name for logging purposes. Defaults to \"QdrantUpload\".</p> <code>'QdrantUpload'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If mode is not \"qdrant\" or \"local\".</p> Source code in <code>eve/steps/qdrant/qdrant_step.py</code> <pre><code>def __init__(self, config: dict, name: str = \"QdrantUpload\"):\n    \"\"\"Initialize the Qdrant upload step.\n\n    Args:\n        config (dict): Configuration dictionary containing:\n\n            - mode (str, optional): \"qdrant\" or \"local\". Defaults to \"qdrant\".\n            - use_existing_embeddings (bool, optional): If True, use embeddings\n                from document.embedding field. Defaults to False.\n            - upload_pipeline_metadata (bool, optional): If True, include\n                pipeline_metadata in Qdrant payload. Defaults to False.\n            - vector_store (dict, required for \"qdrant\" mode):\n                - url (str): Qdrant instance URL.\n                - api_key (str, optional): API key for Qdrant authentication.\n                - collection_name (str): Target collection name.\n                - batch_size (int): Number of documents per batch.\n                - vector_size (int): Dimension of embedding vectors.\n            - embedder (dict, required if use_existing_embeddings=False):\n                - url (str): URL of VLLM embedding server.\n                - model_name (str): Embedding model identifier.\n                - timeout (int, optional): Request timeout in seconds. Defaults to 300.\n                - api_key (str, optional): API key for VLLM. Defaults to \"EMPTY\".\n            - batch_size (int, optional): Batch size for local mode. Defaults to 10.\n        name (str, optional): Name for logging purposes. Defaults to \"QdrantUpload\".\n\n    Raises:\n        ValueError: If mode is not \"qdrant\" or \"local\".\n    \"\"\"\n    super().__init__(config, name)\n\n    # Determine mode: \"qdrant\" or \"local\"\n    self.mode = config.get(\"mode\", \"qdrant\").lower()\n\n    if self.mode not in [\"qdrant\", \"local\"]:\n        raise ValueError(f\"Invalid mode '{self.mode}'. Must be 'qdrant' or 'local'\")\n\n    # Check if we should use existing embeddings from document.embedding field\n    self.use_existing_embeddings = config.get(\"use_existing_embeddings\", False)\n\n    # Check if we should upload pipeline_metadata to Qdrant\n    self.upload_pipeline_metadata = config.get(\"upload_pipeline_metadata\", False)\n\n    # Initialize VLLM embedder only if not using existing embeddings\n    if not self.use_existing_embeddings:\n        embedding_cfg = config[\"embedder\"]\n        self.embedder = VLLMEmbedder(\n            url=embedding_cfg[\"url\"],\n            model_name=embedding_cfg[\"model_name\"],\n            timeout=embedding_cfg.get(\"timeout\", 300),\n            api_key=embedding_cfg.get(\"api_key\", \"EMPTY\"),\n        )\n        self.logger.info(f\"Initialized VLLM embedder at {embedding_cfg['url']}\")\n    else:\n        self.embedder = None\n        self.logger.info(\"Using existing embeddings from document metadata\")\n\n    self.logger.info(f\"Mode: {self.mode}\")\n\n    # Initialize Qdrant-specific configuration only if mode is \"qdrant\"\n    if self.mode == \"qdrant\":\n        vector_store_cfg = config.get(\"vector_store\", {})\n        self.qdrant_url = vector_store_cfg.get(\"url\", \"http://localhost:6333\")\n        self.vector_store_api_key = vector_store_cfg.get(\"api_key\")\n\n        # Get collection configuration\n        self.collection_name = vector_store_cfg[\"collection_name\"]\n        self.batch_size = vector_store_cfg[\"batch_size\"]\n        self.vector_size = vector_store_cfg[\"vector_size\"]\n\n        # Initialize Qdrant client\n        self.client = QdrantClient(\n            url=self.qdrant_url, api_key=self.vector_store_api_key\n        )\n\n        # Ensure collection exists\n        self._ensure_collection()\n        self.existing_ids = self._get_existing_ids()\n    else:\n        # Local mode: set batch size for processing\n        self.batch_size = config.get(\"batch_size\", 10)\n        self.client = None\n</code></pre>"},{"location":"pipeline-stages/qdrant/#eve.steps.qdrant.qdrant_step.QdrantUploadStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute the upload step.</p> <p>Routes to appropriate execution method based on configured mode (local or qdrant).</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document objects to process.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List[Document]: The same list of documents passed through for pipeline chaining.</p> Source code in <code>eve/steps/qdrant/qdrant_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute the upload step.\n\n    Routes to appropriate execution method based on configured mode\n    (local or qdrant).\n\n    Args:\n        documents (List[Document]): List of Document objects to process.\n\n    Returns:\n        List[Document]: The same list of documents passed through for\n            pipeline chaining.\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No documents to process\")\n        return documents\n\n    self.logger.info(f\"Processing {len(documents)} documents\")\n\n    if self.mode == \"local\":\n        # Local mode: add embeddings to document metadata\n        return await self._execute_local(documents)\n    else:\n        # Qdrant mode: upload to vector database\n        return await self._execute_qdrant(documents)\n</code></pre>"}]}