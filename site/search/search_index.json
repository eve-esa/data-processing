{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"EVE Pipeline","text":"<p>A high-performance, modular library for extracting, deduplicating, cleaning, anonymizing, and exporting large-scale Earth science and Earth observation datasets. This is part of the Earth Virtual Expert (EVE) initiative\u2014an open-science program funded by the European Space Agency's \u03a6-lab.</p>"},{"location":"#earth-virtual-expert-eve","title":"\ud83c\udf0d Earth Virtual Expert (EVE)","text":"<p>Earth Virtual Expert (EVE) aims to advance the use of Large Language Models (LLMs) within the Earth Observation (EO) and Earth Science (ES) community.</p> <ul> <li>Website: https://eve.philab.esa.int/</li> <li>HuggingFace: https://huggingface.co/eve-esa</li> <li>GitHub: https://github.com/eve-esa</li> </ul>"},{"location":"#features","title":"Features","text":""},{"location":"#extraction","title":"Extraction","text":"<ul> <li>Supports PDF, HTML, XML, Markdown and nested folder structures</li> <li>Automatically detects file formats unless explicitly specified</li> </ul>"},{"location":"#deduplication","title":"Deduplication","text":"<ul> <li>Performs exact matching using SHA-256 checksum</li> <li>Supports LSH based near-duplicate detection with configurable:</li> <li>Shingle size</li> <li>Permutations</li> <li>Similarity threshold</li> </ul>"},{"location":"#cleaning","title":"Cleaning","text":"<ul> <li>Removes irregularities and noise artifacts</li> <li>Corrects LaTeX equations and tables using LLM assistance</li> </ul>"},{"location":"#pii-removal","title":"PII Removal","text":"<ul> <li>Automatically masks Names and Emails using the Presidio framework</li> <li>Configurable detection patterns</li> </ul>"},{"location":"#metadata-extraction","title":"Metadata Extraction","text":"<ul> <li>Extracts Title, Authors, DOI, URL, Year, Journal, and Citation Count</li> <li>PDF-based extraction using MonkeyOCR integration</li> <li>Support for HTML and other formats</li> </ul>"},{"location":"#export","title":"Export","text":"<ul> <li>Saves processed content in multiple formats (default: Markdown)</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<ol> <li> <p>Install the packages</p> <pre><code>uv sync\n</code></pre> </li> <li> <p>Configure the pipeline (<code>config.yaml</code>)</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_dir\"\n  stages:\n    - name: extraction\n      config: { format: \"xml\"}\n    - name: duplication\n      config: { method: \"lsh\", shingle_size: 3, num_perm: 128, threshold: 0.8 }\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: export\n      config: { format: \"md\", destination: \"output/files\"}\n</code></pre> </li> <li> <p>Run the pipeline</p> <pre><code>eve run\n</code></pre> </li> </ol>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Installation Guide</li> <li>Quick Start Tutorial</li> <li>Pipeline Stages</li> <li>API Reference</li> </ul>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please open an issue or submit a pull request on GitHub to help improve the pipeline.</p> <p>See our Contributing Guide for details.</p>"},{"location":"#license","title":"License","text":"<p>This project is released under the Apache 2.0 License - see the LICENSE file for more details.</p>"},{"location":"#acknowledgments","title":"Acknowledgments","text":"<p>This project is supported by the European Space Agency (ESA) \u03a6-lab through the Large Language Model for Earth Observation and Earth Science project, as part of the Foresight Element within FutureEO Block 4 programme.</p>"},{"location":"api/core/","title":"Core Components","text":"<p>This section covers the core components of the EVE Pipeline that form the foundation of the data processing framework.</p>"},{"location":"api/core/#pipeline","title":"Pipeline","text":"<p>The main pipeline orchestrator that coordinates all processing stages.</p>"},{"location":"api/core/#eve.pipeline.pipeline","title":"<code>eve.pipeline.pipeline()</code>  <code>async</code>","text":"Source code in <code>eve/pipeline.py</code> <pre><code>async def pipeline():\n    logger = get_logger(\"pipeline\")\n    cfg = load_config(\"config.yaml\")\n\n    batch_size = cfg.batch_size\n\n    logger.info(\"Starting pipeline execution\")\n\n    start_time = time.perf_counter()\n    input_files = cfg.inputs.get_files()\n\n    logger.info(f\"Processing {len(input_files)} files with batch size {batch_size}\")\n\n    unique_file_formats = {find_format(f) for f in input_files}\n\n    stages_with_extraction_dependency = {\"dedup\", \"cleaning\", \"pii\"}\n\n    # enable extraction only if needed\n    if 'md' not in unique_file_formats:\n        user_stage_names = {stage[\"name\"] for stage in cfg.stages}\n        if not any(stage in user_stage_names for stage in stages_with_extraction_dependency):\n            # no dependency stage, skip extraction\n            pass\n        else:\n            if \"extraction\" not in user_stage_names:\n                cfg.stages.insert(0, {\"name\": \"extraction\"})\n\n    # enable export by default\n    if not any(stage[\"name\"] == \"export\" for stage in cfg.stages):\n        cfg.stages.append({\"name\": \"export\"})\n\n\n    logger.info(f\"Stages: {[stage['name'] for stage in cfg.stages]}\")\n\n    step_mapping = {\n        \"cleaning\": CleaningStep,\n        \"export\": ExportStep,\n        \"duplication\": DuplicationStep,\n        \"extraction\": ExtractionStep,\n        \"pii\": PiiStep,\n        \"metadata\": MetadataStep,\n    }\n\n    batchable_steps = {\"cleaning\", \"extraction\", \"pii\", \"metadata\", \"export\"}\n\n    has_dedup = any(stage[\"name\"] == \"duplication\" for stage in cfg.stages) #TO-DO - is there a way to do dedup with batching?\n\n    if has_dedup: #handle batching seperately\n        logger.info(\"Deduplication detected - collecting all documents before processing\")\n        all_documents = []\n        async for batch in create_batches(input_files, batch_size):\n            batch_docs = batch\n            for stage in cfg.stages:\n                step_name = stage[\"name\"]\n                if step_name == \"duplication\":\n                    break  # stop here, accumulate all docs and run dedup in phase 2\n                if step_name in batchable_steps and step_name in step_mapping:\n                    step_config = stage.get(\"config\", {})\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step on batch: {step_name}\")\n                    batch_docs = await step(batch_docs)\n\n            all_documents.extend(batch_docs)\n\n        documents = all_documents\n        dedup_started = False\n        for stage in cfg.stages:\n            step_name = stage[\"name\"]\n            if step_name == \"duplication\":\n                dedup_started = True\n\n            if dedup_started:\n                step_config = stage.get(\"config\", {})\n                if step_name in step_mapping:\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step: {step_name}\")\n                    documents = await step(documents)\n                else:\n                    logger.error(f\"No implementation found for step: {step_name}\")\n    else:\n        logger.info(\"No deduplication - using streaming batch processing\")\n        all_processed = []\n\n        async for batch in create_batches(input_files, batch_size):\n            batch_docs = batch\n            logger.info(f\"Processing batch of {len(batch_docs)} documents\")\n\n            for stage in cfg.stages:\n                step_name = stage[\"name\"]\n                step_config = stage.get(\"config\", {})\n                if step_name in step_mapping:\n                    step = step_mapping[step_name](config = step_config)\n                    logger.info(f\"Running step on batch: {step_name}\")\n                    batch_docs = await step(batch_docs)\n                else:\n                    logger.error(f\"No implementation found for step: {step_name}\")\n\n            all_processed.extend(batch_docs)\n\n        documents = all_processed\n\n    end_time = time.perf_counter()\n    elapsed_time = end_time - start_time\n    logger.info(f\"Pipeline completed in {elapsed_time:.2f} seconds\")\n    logger.info(f\"Processed {len(documents)} documents successfully\")\n</code></pre>"},{"location":"api/core/#configuration","title":"Configuration","text":"<p>Configuration management for the pipeline using Pydantic models.</p>"},{"location":"api/core/#eve.config","title":"<code>eve.config</code>","text":""},{"location":"api/core/#document-model","title":"Document Model","text":"<p>The unified document object that represents content and metadata throughout the pipeline.</p>"},{"location":"api/core/#eve.model.document","title":"<code>eve.model.document</code>","text":"<p>Unified Document object for the EVE pipeline.</p>"},{"location":"api/core/#eve.model.document.Document","title":"<code>Document(content: str, file_path: Path, file_format: str, metadata: Dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Unified document object that encapsulates content and metadata throughout the pipeline.</p> <p>This replaces the need to pass (Path, str) tuples and provides a consistent interface for document handling across all pipeline stages.</p>"},{"location":"api/core/#eve.model.document.Document.content_length","title":"<code>content_length: int</code>  <code>property</code>","text":"<p>Get the length of the content.</p>"},{"location":"api/core/#eve.model.document.Document.extension","title":"<code>extension: str</code>  <code>property</code>","text":"<p>Get the file extension.</p>"},{"location":"api/core/#eve.model.document.Document.filename","title":"<code>filename: str</code>  <code>property</code>","text":"<p>Get the filename without path.</p>"},{"location":"api/core/#eve.model.document.Document.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Detailed representation.</p> Source code in <code>eve/model/document.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Detailed representation.\"\"\"\n    return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>String representation showing filename and content length.</p> Source code in <code>eve/model/document.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation showing filename and content length.\"\"\"\n    return f\"Document({self.filename}, {self.file_format} format)\"\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.add_metadata","title":"<code>add_metadata(key: str, value: Any) -&gt; None</code>","text":"<p>Add a metadata entry.</p> Source code in <code>eve/model/document.py</code> <pre><code>def add_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Add a metadata entry.\"\"\"\n    self.metadata[key] = value\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.from_path_and_content","title":"<code>from_path_and_content(file_path: Path, content: str, **metadata) -&gt; Document</code>  <code>classmethod</code>","text":"<p>Create a Document from a file path and content string.</p> Source code in <code>eve/model/document.py</code> <pre><code>@classmethod\ndef from_path_and_content(cls, file_path: Path, content: str, **metadata) -&gt; 'Document':\n    \"\"\"Create a Document from a file path and content string.\"\"\"\n    return cls(\n        content=content,\n        file_path=file_path,\n        metadata=metadata\n    )\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.from_tuple","title":"<code>from_tuple(path_content_tuple: tuple[Path, str], **metadata) -&gt; Document</code>  <code>classmethod</code>","text":"<p>Create a Document from a (Path, str) tuple for backwards compatibility.</p> Source code in <code>eve/model/document.py</code> <pre><code>@classmethod\ndef from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; 'Document':\n    \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n    file_path, content = path_content_tuple\n    return cls.from_path_and_content(file_path, content, **metadata)\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.get_metadata","title":"<code>get_metadata(key: str, default: Any = None) -&gt; Any</code>","text":"<p>Get a metadata value with optional default.</p> Source code in <code>eve/model/document.py</code> <pre><code>def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a metadata value with optional default.\"\"\"\n    return self.metadata.get(key, default)\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.is_empty","title":"<code>is_empty() -&gt; bool</code>","text":"<p>Check if the document content is empty.</p> Source code in <code>eve/model/document.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if the document content is empty.\"\"\"\n    return not self.content.strip()\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.to_tuple","title":"<code>to_tuple() -&gt; tuple[Path, str]</code>","text":"<p>Convert to (Path, str) tuple for backwards compatibility.</p> Source code in <code>eve/model/document.py</code> <pre><code>def to_tuple(self) -&gt; tuple[Path, str]:\n    \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n    return (self.file_path, self.content)\n</code></pre>"},{"location":"api/core/#eve.model.document.Document.update_content","title":"<code>update_content(new_content: str) -&gt; None</code>","text":"<p>Update the document content and track the change in metadata.</p> Source code in <code>eve/model/document.py</code> <pre><code>def update_content(self, new_content: str) -&gt; None:\n    \"\"\"Update the document content and track the change in metadata.\"\"\"\n    old_length = self.content_length\n    self.content = new_content\n    new_length = self.content_length\n\n    # Track content changes in metadata\n    changes = self.metadata.get('content_changes', [])\n    changes.append({\n        'old_length': old_length,\n        'new_length': new_length,\n        'size_change': new_length - old_length\n    })\n    self.metadata['content_changes'] = changes\n</code></pre>"},{"location":"api/core/#pipeline-step-base","title":"Pipeline Step Base","text":"<p>Abstract base class that all pipeline stages must implement.</p>"},{"location":"api/core/#eve.base_step","title":"<code>eve.base_step</code>","text":""},{"location":"api/core/#eve.base_step.PipelineStep","title":"<code>PipelineStep(config: Any, name: Optional[str] = None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>abstract base class for all pipeline steps.</p> <p>initialize the pipeline step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Any</code> <p>Configuration specific to the step.</p> required <code>name</code> <code>Optional[str]</code> <p>Optional name for the step (used for logging).</p> <code>None</code> Source code in <code>eve/base_step.py</code> <pre><code>def __init__(self, config: Any, name: Optional[str] = None):\n    \"\"\"initialize the pipeline step.\n\n    Args:\n        config: Configuration specific to the step.\n        name: Optional name for the step (used for logging).\n    \"\"\"\n    self.config = config\n    self.debug = config.get(\"debug\", False) if isinstance(config, dict) else False\n    self.logger = get_logger(name or self.__class__.__name__)\n</code></pre>"},{"location":"api/core/#eve.base_step.PipelineStep.__call__","title":"<code>__call__(input_data: Any) -&gt; Any</code>  <code>async</code>","text":"<p>shortway of calling <code>execute</code> method.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data to process.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Processed data or result of the step.</p> Source code in <code>eve/base_step.py</code> <pre><code>async def __call__(self, input_data: Any) -&gt; Any:\n    \"\"\"shortway of calling `execute` method.\n\n    Args:\n        input_data: Input data to process.\n\n    Returns:\n        Processed data or result of the step.\n    \"\"\"\n    return await self.execute(input_data)\n</code></pre>"},{"location":"api/core/#eve.base_step.PipelineStep.execute","title":"<code>execute(input_data: Any) -&gt; Any</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Execute the pipeline step.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>Any</code> <p>Input data to process.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Processed data or result of the step.</p> Source code in <code>eve/base_step.py</code> <pre><code>@abstractmethod\nasync def execute(self, input_data: Any) -&gt; Any: # TBD\n    \"\"\"Execute the pipeline step.\n\n    Args:\n        input_data: Input data to process.\n\n    Returns:\n        Processed data or result of the step.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models/","title":"Data Models","text":"<p>This section documents the data models and structures used throughout the EVE Pipeline.</p>"},{"location":"api/models/#document-model","title":"Document Model","text":"<p>The primary data structure representing documents in the pipeline.</p>"},{"location":"api/models/#eve.model.document.Document","title":"<code>eve.model.document.Document(content: str, file_path: Path, file_format: str, metadata: Dict[str, Any] = dict())</code>  <code>dataclass</code>","text":"<p>Unified document object that encapsulates content and metadata throughout the pipeline.</p> <p>This replaces the need to pass (Path, str) tuples and provides a consistent interface for document handling across all pipeline stages.</p>"},{"location":"api/models/#eve.model.document.Document.content_length","title":"<code>content_length: int</code>  <code>property</code>","text":"<p>Get the length of the content.</p>"},{"location":"api/models/#eve.model.document.Document.extension","title":"<code>extension: str</code>  <code>property</code>","text":"<p>Get the file extension.</p>"},{"location":"api/models/#eve.model.document.Document.filename","title":"<code>filename: str</code>  <code>property</code>","text":"<p>Get the filename without path.</p>"},{"location":"api/models/#eve.model.document.Document.__repr__","title":"<code>__repr__() -&gt; str</code>","text":"<p>Detailed representation.</p> Source code in <code>eve/model/document.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"Detailed representation.\"\"\"\n    return f\"Document(file_path={self.file_path}, format={self.file_format}, metadata_keys={list(self.metadata.keys())})\"\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>String representation showing filename and content length.</p> Source code in <code>eve/model/document.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation showing filename and content length.\"\"\"\n    return f\"Document({self.filename}, {self.file_format} format)\"\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.add_metadata","title":"<code>add_metadata(key: str, value: Any) -&gt; None</code>","text":"<p>Add a metadata entry.</p> Source code in <code>eve/model/document.py</code> <pre><code>def add_metadata(self, key: str, value: Any) -&gt; None:\n    \"\"\"Add a metadata entry.\"\"\"\n    self.metadata[key] = value\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.from_path_and_content","title":"<code>from_path_and_content(file_path: Path, content: str, **metadata) -&gt; Document</code>  <code>classmethod</code>","text":"<p>Create a Document from a file path and content string.</p> Source code in <code>eve/model/document.py</code> <pre><code>@classmethod\ndef from_path_and_content(cls, file_path: Path, content: str, **metadata) -&gt; 'Document':\n    \"\"\"Create a Document from a file path and content string.\"\"\"\n    return cls(\n        content=content,\n        file_path=file_path,\n        metadata=metadata\n    )\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.from_tuple","title":"<code>from_tuple(path_content_tuple: tuple[Path, str], **metadata) -&gt; Document</code>  <code>classmethod</code>","text":"<p>Create a Document from a (Path, str) tuple for backwards compatibility.</p> Source code in <code>eve/model/document.py</code> <pre><code>@classmethod\ndef from_tuple(cls, path_content_tuple: tuple[Path, str], **metadata) -&gt; 'Document':\n    \"\"\"Create a Document from a (Path, str) tuple for backwards compatibility.\"\"\"\n    file_path, content = path_content_tuple\n    return cls.from_path_and_content(file_path, content, **metadata)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.get_metadata","title":"<code>get_metadata(key: str, default: Any = None) -&gt; Any</code>","text":"<p>Get a metadata value with optional default.</p> Source code in <code>eve/model/document.py</code> <pre><code>def get_metadata(self, key: str, default: Any = None) -&gt; Any:\n    \"\"\"Get a metadata value with optional default.\"\"\"\n    return self.metadata.get(key, default)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.is_empty","title":"<code>is_empty() -&gt; bool</code>","text":"<p>Check if the document content is empty.</p> Source code in <code>eve/model/document.py</code> <pre><code>def is_empty(self) -&gt; bool:\n    \"\"\"Check if the document content is empty.\"\"\"\n    return not self.content.strip()\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.to_tuple","title":"<code>to_tuple() -&gt; tuple[Path, str]</code>","text":"<p>Convert to (Path, str) tuple for backwards compatibility.</p> Source code in <code>eve/model/document.py</code> <pre><code>def to_tuple(self) -&gt; tuple[Path, str]:\n    \"\"\"Convert to (Path, str) tuple for backwards compatibility.\"\"\"\n    return (self.file_path, self.content)\n</code></pre>"},{"location":"api/models/#eve.model.document.Document.update_content","title":"<code>update_content(new_content: str) -&gt; None</code>","text":"<p>Update the document content and track the change in metadata.</p> Source code in <code>eve/model/document.py</code> <pre><code>def update_content(self, new_content: str) -&gt; None:\n    \"\"\"Update the document content and track the change in metadata.\"\"\"\n    old_length = self.content_length\n    self.content = new_content\n    new_length = self.content_length\n\n    # Track content changes in metadata\n    changes = self.metadata.get('content_changes', [])\n    changes.append({\n        'old_length': old_length,\n        'new_length': new_length,\n        'size_change': new_length - old_length\n    })\n    self.metadata['content_changes'] = changes\n</code></pre>"},{"location":"api/models/#configuration-models","title":"Configuration Models","text":"<p>Data models for pipeline configuration.</p>"},{"location":"api/models/#inputs-configuration","title":"Inputs Configuration","text":""},{"location":"api/models/#eve.config.Inputs","title":"<code>eve.config.Inputs</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/models/#pipeline-configuration","title":"Pipeline Configuration","text":""},{"location":"api/models/#eve.config.PipelineConfig","title":"<code>eve.config.PipelineConfig</code>","text":"<p>               Bases: <code>BaseModel</code></p>"},{"location":"api/stages/","title":"Pipeline Stages","text":"<p>This section documents all the available pipeline stages for processing documents.</p>"},{"location":"api/stages/#extraction-stage","title":"Extraction Stage","text":"<p>Extracts content from various document formats.</p>"},{"location":"api/stages/#eve.steps.extraction.extract_step","title":"<code>eve.steps.extraction.extract_step</code>","text":""},{"location":"api/stages/#eve.steps.extraction.extract_step.ExtractionStep","title":"<code>ExtractionStep(config: Any, name: Optional[str] = None)</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>eve/base_step.py</code> <pre><code>def __init__(self, config: Any, name: Optional[str] = None):\n    \"\"\"initialize the pipeline step.\n\n    Args:\n        config: Configuration specific to the step.\n        name: Optional name for the step (used for logging).\n    \"\"\"\n    self.config = config\n    self.debug = config.get(\"debug\", False) if isinstance(config, dict) else False\n    self.logger = get_logger(name or self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.extract_step.ExtractionStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute text extraction on input files or documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>List of file paths or Document objects to extract text from</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with extracted text</p> Source code in <code>eve/steps/extraction/extract_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute text extraction on input files or documents.\n\n    Args:\n        input_data: List of file paths or Document objects to extract text from\n\n    Returns:\n        List of Document objects with extracted text\n    \"\"\"\n    format = self.config.get(\"format\", None)  # write a wrapper to find out the extension\n    if not format:\n        unique_formats = set()\n\n    unique_formats = {document.file_format for document in documents}\n\n    self.logger.info(f\"Extracting text from {unique_formats} files. File count: {len(documents)}\")\n\n    result = []\n    for document in documents:\n        try:\n            if document.file_format == \"html\":\n                document_with_text = await self._html_extraction(document)\n            elif document.file_format == \"pdf\":\n                url = self.config.get(\"url\", None)\n                if not url:\n                    self.logger.error(\"No URL provided for PDF extraction service\")\n                document_with_text = await self._pdf_extraction(document, url)\n            elif document.file_format == \"xml\":\n                document_with_text = await self._xml_extraction(document)\n            elif document.file_format == 'md':\n                document_with_text = await self._md_extraction(document)\n            else:\n                self.logger.error(f\"Unsupported format: {document.file_format}\")\n                continue\n\n            if document_with_text and hasattr(document_with_text, 'content_length') and document_with_text.content_length &gt; 1:\n                result.append(document_with_text)\n                self.logger.info(f\"Successfully extracted {document_with_text.content_length} characters from {document_with_text.filename}\")\n            else:\n                self.logger.warning(f\"No text extracted from {document.filename}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to extract text from {document.filename}: {str(e)}\")\n            continue\n    return result\n</code></pre>"},{"location":"api/stages/#extractors","title":"Extractors","text":""},{"location":"api/stages/#pdf-extractor","title":"PDF Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.pdfs","title":"<code>eve.steps.extraction.pdfs</code>","text":""},{"location":"api/stages/#eve.steps.extraction.pdfs.PdfExtractor","title":"<code>PdfExtractor(document: Document, endpoint: str)</code>","text":"Source code in <code>eve/steps/extraction/pdfs.py</code> <pre><code>def __init__(self, document: Document, endpoint: str):\n    self.document = document\n    self.endpoint = f\"{endpoint}/predict\"\n    self.extraction = None\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.pdfs.PdfExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single PDF file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/pdfs.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single PDF file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            content = await self._call_nougat(session)\n            if not content:\n                logger.error(f\"Failed to extract content from {self.document.file_path}\")\n                return None\n            self.document.content = content\n            return self.document\n    except Exception as e:\n        logger.error(f\"Error in PDF extraction for {self.document.file_path}: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/stages/#html-extractor","title":"HTML Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.htmls","title":"<code>eve.steps.extraction.htmls</code>","text":""},{"location":"api/stages/#eve.steps.extraction.htmls.HtmlExtractor","title":"<code>HtmlExtractor(document: Document)</code>","text":"Source code in <code>eve/steps/extraction/htmls.py</code> <pre><code>def __init__(self, document: Document):\n    self.document = document\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.htmls.HtmlExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single HTML file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/htmls.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single HTML file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        def parse_html():\n            return extract(content, include_comments = False, include_tables = True)\n\n        self.document.content = await asyncio.to_thread(parse_html)\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/stages/#xml-extractor","title":"XML Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.xmls","title":"<code>eve.steps.extraction.xmls</code>","text":""},{"location":"api/stages/#eve.steps.extraction.xmls.XmlExtractor","title":"<code>XmlExtractor(document: Document)</code>","text":"Source code in <code>eve/steps/extraction/xmls.py</code> <pre><code>def __init__(self, document: Document):\n    self.document = document\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.xmls.XmlExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single XML file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/xmls.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single XML file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        def parse_and_extract():\n            root = ET.fromstring(content)\n\n            def extract_text_from_tree(element):\n                texts = []\n                if element.text:\n                    texts.append(element.text)\n                for child in element:\n                    texts.extend(extract_text_from_tree(child))\n                if element.tail:\n                    texts.append(element.tail)\n                return texts\n\n            extracted_texts = extract_text_from_tree(root)\n            full_text = ''.join(extracted_texts)\n            cleaned_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n            return cleaned_text.strip()\n\n        self.document.content = await asyncio.to_thread(parse_and_extract)\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing XML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/stages/#markdown-extractor","title":"Markdown Extractor","text":""},{"location":"api/stages/#eve.steps.extraction.markdown","title":"<code>eve.steps.extraction.markdown</code>","text":""},{"location":"api/stages/#eve.steps.extraction.markdown.MarkdownExtractor","title":"<code>MarkdownExtractor(document: Document)</code>","text":"Source code in <code>eve/steps/extraction/markdown.py</code> <pre><code>def __init__(self, document: Document):\n    self.document = document\n</code></pre>"},{"location":"api/stages/#eve.steps.extraction.markdown.MarkdownExtractor.extract_text","title":"<code>extract_text() -&gt; Optional[Document]</code>  <code>async</code>","text":"<p>Extract text from a single markdown file.</p> <p>Returns:</p> Type Description <code>Optional[Document]</code> <p>Document object with extracted text if successful, None otherwise</p> Source code in <code>eve/steps/extraction/markdown.py</code> <pre><code>async def extract_text(self) -&gt; Optional[Document]:\n    \"\"\"Extract text from a single markdown file.\n\n    Returns:\n        Document object with extracted text if successful, None otherwise\n    \"\"\"\n    try:\n        content = await read_file(self.document.file_path, 'r')\n        if not content:\n            logger.error(f\"Failed to read file: {self.document.file_path}\")\n            return None\n\n        self.document.content = content\n        return self.document\n    except Exception as e:\n        logger.error(f\"Error processing HTML file {self.document.file_path}: {e}\")\n        return None\n</code></pre>"},{"location":"api/stages/#deduplication-stage","title":"Deduplication Stage","text":"<p>Removes duplicate and near-duplicate documents.</p>"},{"location":"api/stages/#eve.steps.dedup.dedup_step","title":"<code>eve.steps.dedup.dedup_step</code>","text":""},{"location":"api/stages/#eve.steps.dedup.dedup_step.DuplicationStep","title":"<code>DuplicationStep(config: Any, name: Optional[str] = None)</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>eve/base_step.py</code> <pre><code>def __init__(self, config: Any, name: Optional[str] = None):\n    \"\"\"initialize the pipeline step.\n\n    Args:\n        config: Configuration specific to the step.\n        name: Optional name for the step (used for logging).\n    \"\"\"\n    self.config = config\n    self.debug = config.get(\"debug\", False) if isinstance(config, dict) else False\n    self.logger = get_logger(name or self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.dedup_step.DuplicationStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute deduplication on input files or documents.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <p>List of file paths or Document objects to deduplicate</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with duplicates removed</p> Source code in <code>eve/steps/dedup/dedup_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute deduplication on input files or documents.\n\n    Args:\n        input_data: List of file paths or Document objects to deduplicate\n\n    Returns:\n        List of Document objects with duplicates removed\n    \"\"\"\n    method = self.config.get(\"method\", \"exact\")  # default to exact\n\n    self.logger.info(f\"Executing duplication step with method: {method} file count: {len(documents)}\")\n\n    if method == \"exact\":\n        duplicates = await self._exact_deduplication(documents)\n    elif method == \"lsh\":\n        duplicates = await self._lsh_deduplication(documents)\n    else:\n        self.logger.error(f\"Invalid deduplication method: {method}\")\n        raise ValueError(f\"Invalid deduplication method: {method}\")\n\n    # Remove duplicates from documents\n    duplicate_docs = set()\n    duplicates_removed = 0\n    for group in duplicates:\n        # Keep the first doc in each group, mark the rest as duplicates\n        for doc in group[1:]:\n            duplicate_docs.add(doc)\n            duplicates_removed += 1\n\n    # Filter out duplicates, keeping the first occurrence\n    result_documents = []\n    for doc in documents:\n        if doc not in duplicate_docs:\n            result_documents.append(doc)\n\n    self.logger.info(\n        f\"Deduplication complete: {len(result_documents)} files remaining, {duplicates_removed} duplicates removed\"\n    )\n    return result_documents\n</code></pre>"},{"location":"api/stages/#deduplication-methods","title":"Deduplication Methods","text":""},{"location":"api/stages/#exact-duplicates","title":"Exact Duplicates","text":""},{"location":"api/stages/#eve.steps.dedup.exact_duplicates","title":"<code>eve.steps.dedup.exact_duplicates</code>","text":""},{"location":"api/stages/#eve.steps.dedup.exact_duplicates.ExactDuplication","title":"<code>ExactDuplication(documents: List[Document])</code>","text":"<p>this class does exact duplication by - 1. calculate size as a first filter to save computation. 2. calcuates checksum and finds the duplicates.</p> Source code in <code>eve/steps/dedup/exact_duplicates.py</code> <pre><code>def __init__(self, documents: List[Document]):\n    self.documents = documents\n    self.duplicates = []\n\n    self._validate()\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.exact_duplicates.ExactDuplication.find_duplicates","title":"<code>find_duplicates() -&gt; list[list[Document]]</code>  <code>async</code>","text":"<p>Find duplicate files based on size and SHA-256 checksum.</p> Source code in <code>eve/steps/dedup/exact_duplicates.py</code> <pre><code>async def find_duplicates(self) -&gt; list[list[Document]]:\n    \"\"\"Find duplicate files based on size and SHA-256 checksum.\"\"\"\n\n    # stage 1: group files by size\n    size_tasks = [self._calculate_size(doc.file_path) for doc in self.documents]\n    sizes = await asyncio.gather(*size_tasks)\n\n    size_groups = defaultdict(list)\n    for doc, size in zip(self.documents, sizes):\n        size_groups[size].append(doc)\n\n    # stage 2: calculate checksums for potential duplicates\n    checksum_tasks = []\n    file_info = []\n\n    for size, docs in size_groups.items():\n        if len(docs) &gt;= 2:  # Only consider docs with matching sizes\n            for doc in docs:\n                checksum_tasks.append(self._calculate_sha256(doc.file_path))\n                file_info.append((doc, size))\n\n    if not checksum_tasks:\n        return []\n\n    checksums = await asyncio.gather(*checksum_tasks)\n\n    file_map = defaultdict(list)\n    for (doc, size), checksum in zip(file_info, checksums):\n        key = (size, checksum)\n        file_map[key].append(doc)\n\n    self.duplicates = {key: docs for key, docs in file_map.items() if len(docs) &gt; 1}\n    return list(self.duplicates.values())\n</code></pre>"},{"location":"api/stages/#minhash-lsh","title":"MinHash LSH","text":""},{"location":"api/stages/#eve.steps.dedup.minhash","title":"<code>eve.steps.dedup.minhash</code>","text":"<p>Adjust NUM_PERM: Higher values increase accuracy but use more memory. Adjust THRESHOLD: Higher values find closer duplicates but may miss some. Adjust SHINGLE_SIZE: Larger shingles are more specific but increase computation.</p>"},{"location":"api/stages/#eve.steps.dedup.minhash.LSH","title":"<code>LSH(documents: List[Document], shingle_size: int = 3, num_perm: int = 128, threshold: float = 0.8)</code>","text":"Source code in <code>eve/steps/dedup/minhash.py</code> <pre><code>def __init__(\n    self,\n    documents: List[Document],\n    shingle_size: int = 3,\n    num_perm: int = 128,\n    threshold: float = 0.8,\n):\n    self.documents = documents\n    self.shingle_size = shingle_size\n    self.num_perm = num_perm\n    self.threshold = threshold\n    self.doc_hashes = {}   # map: document -&gt; minHash\n    self.duplicates = []\n\n    self._validate()\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.minhash.LSH.create_shingles","title":"<code>create_shingles(text: str) -&gt; set[str]</code>","text":"<p>Create shingles (word n-grams) from text.</p> Source code in <code>eve/steps/dedup/minhash.py</code> <pre><code>def create_shingles(self, text: str) -&gt; set[str]:\n    \"\"\"Create shingles (word n-grams) from text.\"\"\"\n    words = text.lower().split()\n    return {\" \".join(gram) for gram in ngrams(words, self.shingle_size)}\n</code></pre>"},{"location":"api/stages/#eve.steps.dedup.minhash.LSH.find_duplicates","title":"<code>find_duplicates() -&gt; list[list[Document]]</code>","text":"<p>Find near-duplicate documents using LSH.</p> Source code in <code>eve/steps/dedup/minhash.py</code> <pre><code>def find_duplicates(self) -&gt; list[list[Document]]:\n    \"\"\"Find near-duplicate documents using LSH.\"\"\"\n    file_hashes = self._do_lsh()\n    processed = set()\n\n    for doc in self.documents:\n        if doc in processed:\n            continue\n\n        m = self.doc_hashes[doc]\n        candidates = file_hashes.query(m)\n\n        # Convert LSH string keys back to Document objects\n        candidate_docs = [\n            d for d in self.documents if str(d.file_path) in candidates and d != doc\n        ]\n\n        if candidate_docs:\n            group = [doc, *candidate_docs]\n            group = sorted(group, key = lambda d: str(d.file_path))  # consistent ordering\n            if group not in self.duplicates:\n                self.duplicates.append(group)\n            processed.update(group)\n\n    return self.duplicates\n</code></pre>"},{"location":"api/stages/#cleaning-stage","title":"Cleaning Stage","text":"<p>Cleans and improves document quality.</p>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step","title":"<code>eve.steps.cleaning.cleaning_step</code>","text":"<p>Comprehensive cleaning step that applies all data cleaning components.</p>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep","title":"<code>CleaningStep(config: dict)</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Comprehensive cleaning step that applies multiple data cleaning components.</p> <p>This step processes extracted text through various cleaning components to: - Fix OCR-induced errors - Remove OCR duplicates - Apply Nougat corrections - Apply rule-based corrections - Remove Nougat artifacts - Correct LaTeX syntax errors (optional)</p> <p>Initialize the cleaning step with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with component settings.    Expected keys:    - ocr_threshold: float (default 0.99) - OCR duplicate threshold    - min_words: int (default 2) - Minimum words for processing    - enable_latex_correction: bool (default False) - Enable LaTeX correction    - openrouter_api_key: str (optional) - API key for LaTeX correction    - openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections    - debug: bool (default False) - Enable debug output</p> required Source code in <code>eve/steps/cleaning/cleaning_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"Initialize the cleaning step with configuration.\n\n    Args:\n        config: Configuration dictionary with component settings.\n               Expected keys:\n               - ocr_threshold: float (default 0.99) - OCR duplicate threshold\n               - min_words: int (default 2) - Minimum words for processing\n               - enable_latex_correction: bool (default False) - Enable LaTeX correction\n               - openrouter_api_key: str (optional) - API key for LaTeX correction\n               - openrouter_model: str (default \"anthropic/claude-3-haiku\") - Model for corrections\n               - debug: bool (default False) - Enable debug output\n    \"\"\"\n    super().__init__(config, name=\"CleaningStep\")\n\n    ocr_threshold = config.get(\"ocr_threshold\", 0.99)\n    min_words = config.get(\"min_words\", 2)\n    enable_latex = config.get(\"enable_latex_correction\", False)\n    openrouter_key = config.get(\"openrouter_api_key\")\n    openrouter_model = config.get(\"openrouter_model\", \"anthropic/claude-3-haiku\")\n\n    self.processors = [\n        OCRProcessor(debug=self.debug),\n        DuplicateRemovalProcessor(threshold=ocr_threshold, min_words=min_words, debug=self.debug),\n        NougatProcessor(debug=self.debug),\n        RuleBasedProcessor(debug=self.debug),\n    ]\n\n    if enable_latex:\n        self.processors.append(\n            LaTeXProcessor(\n                debug=self.debug,\n                api_key=openrouter_key,\n                model=openrouter_model\n            )\n        )\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute the cleaning step on input data.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Documents.</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of cleaned Documents.</p> Source code in <code>eve/steps/cleaning/cleaning_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"Execute the cleaning step on input data.\n\n    Args:\n        documents: List of Documents.\n\n    Returns:\n        List of cleaned Documents.\n    \"\"\"\n    self.logger.info(f\"Executing cleaning step on {len(documents)} documents\")\n\n    if not documents:\n        self.logger.warning(\"No input data provided to cleaning step\")\n        return []\n\n    result = []\n    processed_count = 0\n    failed_count = 0\n\n    for document in documents:\n        if document.is_empty():\n            self.logger.warning(f\"{document.filename} - Empty content, skipping cleaning\")\n            result.append(document)\n            failed_count += 1\n            continue\n\n        try:\n            processed_document = document\n            original_length = document.content_length\n\n            for processor in self.processors:\n                try:\n                    processed_document = await processor.process(processed_document)\n\n                    if processed_document is None:\n                        self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} returned None\")\n                        processed_document = document\n                        break\n\n                except Exception as e:\n                    self.logger.error(f\"{document.filename} - Processor {processor.__class__.__name__} failed: {str(e)}\")\n                    continue\n\n            if original_length &gt; 0 and processed_document.content_length != original_length:\n                reduction_percent = ((original_length - processed_document.content_length) / original_length) * 100\n\n                if reduction_percent &gt; 0:\n                    self.logger.info(f\"{document.filename} - Cleaned: {reduction_percent:.2f}% text removed ({original_length} -&gt; {processed_document.content_length} chars)\")\n                else:\n                    self.logger.info(f\"{document.filename} - Cleaned: No significant changes\")\n\n            result.append(processed_document)\n            processed_count += 1\n\n        except Exception as e:\n            self.logger.error(f\"{document.filename} - Cleaning failed: {str(e)}\")\n            result.append(document)\n            failed_count += 1\n\n    self.logger.info(f\"Cleaning step completed: {processed_count} processed, {failed_count} failed\")\n    return result\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.cleaning_step.CleaningStep.get_component_info","title":"<code>get_component_info() -&gt; dict</code>","text":"<p>Get information about enabled cleaning processors.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with processor information.</p> Source code in <code>eve/steps/cleaning/cleaning_step.py</code> <pre><code>def get_component_info(self) -&gt; dict:\n    \"\"\"Get information about enabled cleaning processors.\n\n    Returns:\n        Dictionary with processor information.\n    \"\"\"\n    component_info = {\n        \"total_processors\": len(self.processors),\n        \"processors\": [processor.__class__.__name__ for processor in self.processors],\n        \"applicable_formats\": self._get_applicable_formats(),\n        \"debug_enabled\": self.debug\n    }\n\n    latex_enabled = any(isinstance(proc, LaTeXProcessor) for proc in self.processors)\n    component_info[\"latex_correction_enabled\"] = latex_enabled\n\n    return component_info\n</code></pre>"},{"location":"api/stages/#cleaning-components","title":"Cleaning Components","text":""},{"location":"api/stages/#processors","title":"Processors","text":""},{"location":"api/stages/#eve.steps.cleaning.processors","title":"<code>eve.steps.cleaning.processors</code>","text":"<p>Consolidated text processing components for the cleaning pipeline.</p> <p>This module combines all the individual cleaning components into a unified structure for better organization and maintainability.</p>"},{"location":"api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor","title":"<code>DuplicateRemovalProcessor(threshold: float = 0.99, min_words: int = 2, debug: bool = False)</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for removing OCR-induced duplicate text segments.</p> <p>Initialize the duplicate removal processor.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>Similarity threshold for duplicates.</p> <code>0.99</code> <code>min_words</code> <code>int</code> <p>Minimum words required for a unit to be processed.</p> <code>2</code> <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, threshold: float = 0.99, min_words: int = 2, debug: bool = False):\n    \"\"\"\n    Initialize the duplicate removal processor.\n\n    Args:\n        threshold: Similarity threshold for duplicates.\n        min_words: Minimum words required for a unit to be processed.\n        debug: Enable debug output.\n    \"\"\"\n    super().__init__(debug=debug)\n    self.threshold = threshold\n    self.min_words = min_words\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.DuplicateRemovalProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Remove duplicate content from the document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Remove duplicate content from the document.\"\"\"\n    if self.debug:\n        self.logger.info(f\"Before duplicate removal ({document.filename}): {document.content[:200]}...\")\n\n    if document.is_empty():\n        self.logger.warning(f\"{document.filename} - Empty content in duplicate removal\")\n        return document\n\n    try:\n        cleaned_content, removed = self._remove_near_adjacent_duplicates(\n            document.content, document.filename\n        )\n\n        percent_removed = 0.0\n        if document.content:\n            percent_removed = (len(document.content) - len(cleaned_content)) / len(document.content) * 100\n\n        document.update_content(cleaned_content)\n        document.add_metadata('duplicates_removed', len(removed))\n        document.add_metadata('duplicate_removal_percent', percent_removed)\n\n        self.logger.info(f\"{document.filename} - Duplicate removal: {len(removed)} segments, {percent_removed:.2f}% text removed\")\n\n        if self.debug:\n            self.logger.info(f\"After duplicate removal ({document.filename}): {document.content[:200]}...\")\n\n        return document\n\n    except Exception as e:\n        self.logger.error(f\"{document.filename} - Duplicate removal failed: {str(e)}\")\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.LaTeXProcessor","title":"<code>LaTeXProcessor(debug: bool = False, api_key: Optional[str] = None, model: str = 'anthropic/claude-3-haiku')</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for detecting and correcting LaTeX syntax errors.</p> <p>Initialize the LaTeX processor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> <code>api_key</code> <code>Optional[str]</code> <p>OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.</p> <code>None</code> <code>model</code> <code>str</code> <p>OpenRouter model to use for corrections.</p> <code>'anthropic/claude-3-haiku'</code> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, debug: bool = False, api_key: Optional[str] = None, model: str = \"anthropic/claude-3-haiku\"):\n    \"\"\"Initialize the LaTeX processor.\n\n    Args:\n        debug: Enable debug output.\n        api_key: OpenRouter API key. If None, will use OPENROUTER_API_KEY environment variable.\n        model: OpenRouter model to use for corrections.\n    \"\"\"\n    super().__init__(debug=debug)\n    self.api_key = api_key or os.getenv('OPENROUTER_API_KEY')\n    self.model = model\n    self.formula_patterns = get_latex_formula_patterns()\n\n    if not self.api_key and debug:\n        self.logger.warning(\"No OPENROUTER_API_KEY found. LaTeX correction will only detect errors.\")\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.LaTeXProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Process document to detect and correct LaTeX syntax errors.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Process document to detect and correct LaTeX syntax errors.\"\"\"\n    if self.debug:\n        self.logger.info(f\"Before LaTeX processing ({document.filename}): {document.content[:200]}...\")\n\n    if document.is_empty():\n        self.logger.warning(f\"{document.filename} - Empty content in LaTeX processing\")\n        return document\n\n    try:\n        formulas = self._extract_latex_formulas(document.content)\n\n        if not formulas:\n            self.logger.info(f\"{document.filename} - No LaTeX formulas found\")\n            document.add_metadata('latex_processed', True)\n            return document\n\n        errors_found = 0\n        corrections_made = 0\n        modified_content = document.content\n\n        for formula_type, formula in formulas:\n            is_valid, error_message = await self._check_formula_syntax(formula, formula_type)\n\n            if not is_valid:\n                errors_found += 1\n                self.logger.warning(f\"{document.filename} - Invalid LaTeX formula: {formula[:10]}... Error: {error_message}\")\n\n                if self.api_key:\n                    prompt = get_latex_correction_prompt(formula_type, error_message, formula, document.content)\n                    corrected_formula = await make_openrouter_request(\n                        self.api_key, self.model, prompt\n                    )\n\n                    if corrected_formula and corrected_formula != formula:\n                        is_corrected_valid, _ = await self._check_formula_syntax(corrected_formula, formula_type)\n\n                        if is_corrected_valid:\n                            modified_content = self._replace_formula_in_content(\n                                modified_content, formula, corrected_formula, formula_type\n                            )\n                            corrections_made += 1\n                            self.logger.info(f\"{document.filename} - Corrected LaTeX formula: {formula[:30]}... -&gt; {corrected_formula[:30]}...\")\n                        else:\n                            self.logger.warning(f\"{document.filename} - LLM correction still invalid: {corrected_formula[:50]}...\")\n\n        document.update_content(modified_content)\n        document.add_metadata('latex_errors_found', errors_found)\n        document.add_metadata('latex_corrections_made', corrections_made)\n        document.add_metadata('latex_processed', True)\n\n        if errors_found &gt; 0:\n            self.logger.info(f\"{document.filename} - LaTeX processing complete: {errors_found} errors found, {corrections_made} corrected\")\n        else:\n            self.logger.info(f\"{document.filename} - All LaTeX formulas are valid\")\n\n        if self.debug:\n            self.logger.info(f\"After LaTeX processing ({document.filename}): {errors_found} errors, {corrections_made} fixed\")\n\n        return document\n\n    except Exception as e:\n        self.logger.error(f\"{document.filename} - LaTeX processing failed: {str(e)}\")\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.NougatProcessor","title":"<code>NougatProcessor(debug: bool = False)</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for fixing Nougat-related issues and artifacts.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"Initialize the text processor.\n\n    Args:\n        debug: Enable debug output.\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.NougatProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Process Nougat-specific issues in the document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Process Nougat-specific issues in the document.\"\"\"\n    if self.debug:\n        self.logger.info(f\"Before Nougat processing ({document.filename}): {document.content[:200]}...\")\n\n    if document.is_empty():\n        self.logger.warning(f\"{document.filename} - Empty content in Nougat processing\")\n        return document\n\n    try:\n        # Apply Nougat postprocessing\n        cleaned = postprocess_single(document.content, markdown_fix=True)\n\n        # Clean LaTeX table formatting\n        cleaned = clean_doubled_backslashes(cleaned)\n\n        # Remove Nougat artifacts\n        cleaned = remove_nougat_artifacts(cleaned)\n\n        # Convert escaped newlines\n        cleaned = cleaned.replace('\\\\n', '\\n')\n\n        # Remove surrounding quotes\n        cleaned = cleaned.strip('\"')\n\n        document.update_content(cleaned)\n        document.add_metadata('nougat_processed', True)\n\n        self.logger.info(f\"{document.filename} - Nougat processing completed\")\n\n        if self.debug:\n            self.logger.info(f\"After Nougat processing ({document.filename}): {document.content[:200]}...\")\n\n        return document\n\n    except Exception as e:\n        self.logger.error(f\"{document.filename} - Nougat processing failed: {str(e)}\")\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.OCRProcessor","title":"<code>OCRProcessor(debug: bool = False)</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for fixing OCR-induced text issues.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"Initialize the text processor.\n\n    Args:\n        debug: Enable debug output.\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.OCRProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Fix OCR issues in the document content.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Fix OCR issues in the document content.\"\"\"\n    if self.debug:\n        self.logger.info(f\"Before OCR processing ({document.filename}): {document.content[:200]}...\")\n\n    if document.is_empty():\n        self.logger.warning(f\"{document.filename} - Empty content in OCR processing\")\n        return document\n\n    try:\n        # Fix digit-letter spacing issues\n        cleaned_content = fix_ocr_digit_letter_spacing(document.content)\n\n        document.update_content(cleaned_content)\n        document.add_metadata('ocr_processed', True)\n\n        self.logger.info(f\"{document.filename} - OCR processing completed\")\n\n        if self.debug:\n            self.logger.info(f\"After OCR processing ({document.filename}): {document.content[:200]}...\")\n\n        return document\n\n    except Exception as e:\n        self.logger.error(f\"{document.filename} - OCR processing failed: {str(e)}\")\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.RuleBasedProcessor","title":"<code>RuleBasedProcessor(debug: bool = False)</code>","text":"<p>               Bases: <code>TextProcessor</code></p> <p>Processor for applying rule-based text corrections.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"Initialize the text processor.\n\n    Args:\n        debug: Enable debug output.\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.RuleBasedProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>async</code>","text":"<p>Apply rule-based corrections to the document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>async def process(self, document: Document) -&gt; Document:\n    \"\"\"Apply rule-based corrections to the document.\"\"\"\n    if self.debug:\n        self.logger.info(f\"Before rule-based processing ({document.filename}): {document.content[:200]}...\")\n\n    if document.is_empty():\n        self.logger.warning(f\"{document.filename} - Empty content in rule-based processing\")\n        return document\n\n    try:\n        # Remove single symbol lines\n        cleaned = remove_single_symbol_lines(document.content)\n\n        # Normalize excessive newlines\n        cleaned = normalize_excessive_newlines(cleaned)\n\n        # Trim whitespace\n        cleaned = cleaned.strip()\n\n        document.update_content(cleaned)\n        document.add_metadata('rule_based_processed', True)\n\n        self.logger.info(f\"{document.filename} - Rule-based processing completed\")\n\n        if self.debug:\n            self.logger.info(f\"After rule-based processing ({document.filename}): {document.content[:200]}...\")\n\n        return document\n\n    except Exception as e:\n        self.logger.error(f\"{document.filename} - Rule-based processing failed: {str(e)}\")\n        return document\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.TextProcessor","title":"<code>TextProcessor(debug: bool = False)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for text processing components.</p> <p>Initialize the text processor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug output.</p> <code>False</code> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"Initialize the text processor.\n\n    Args:\n        debug: Enable debug output.\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.processors.TextProcessor.process","title":"<code>process(document: Document) -&gt; Document</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Process a document and return the cleaned result.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>The document to process.</p> required <p>Returns:</p> Type Description <code>Document</code> <p>Processed document.</p> Source code in <code>eve/steps/cleaning/processors.py</code> <pre><code>@abstractmethod\nasync def process(self, document: Document) -&gt; Document:\n    \"\"\"Process a document and return the cleaned result.\n\n    Args:\n        document: The document to process.\n\n    Returns:\n        Processed document.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/stages/#nougat-helpers","title":"Nougat Helpers","text":""},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers","title":"<code>eve.steps.cleaning.nougat_helpers</code>","text":"<p>Copyright \u00a9 Meta Platforms, Inc. and affiliates.</p> <p>This source code is licensed under the MIT license found in the LICENSE file in the root directory of this source tree.</p> <p>Script from here - https://github.com/facebookresearch/nougat/blob/main/nougat/postprocessing.py</p>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.get_slices","title":"<code>get_slices(lines: List[str], clean_lines: List[str]) -&gt; List[slice]</code>","text":"<p>Get slices of potentially hallucinated reference sections.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def get_slices(lines: List[str], clean_lines: List[str]) -&gt; List[slice]:\n    \"\"\"Get slices of potentially hallucinated reference sections.\"\"\"\n    slices = []\n    for i, line in enumerate(lines):\n        if line.strip().lower().startswith('## references'):\n            j = i + 1\n            while j &lt; len(lines) and not lines[j].strip().startswith('##'):\n                j += 1\n            slices.append(slice(i, j))\n    return slices\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.markdown_compatible","title":"<code>markdown_compatible(s: str) -&gt; str</code>","text":"<p>Make text compatible with Markdown formatting.</p> <p>This function makes various text formatting adjustments to make it compatible with Markdown.</p> <p>Parameters:</p> Name Type Description Default <code>s</code> <code>str</code> <p>The input text to be made Markdown-compatible.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Markdown-compatible text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def markdown_compatible(s: str) -&gt; str:\n    \"\"\"\n    Make text compatible with Markdown formatting.\n\n    This function makes various text formatting adjustments to make it compatible with Markdown.\n\n    Args:\n        s (str): The input text to be made Markdown-compatible.\n\n    Returns:\n        str: The Markdown-compatible text.\n    \"\"\"\n    s = re.sub(\n        r\"^\\(([\\d.]+[a-zA-Z]?)\\) \\\\\\[(.+?)\\\\\\]$\", r\"\\[\\2 \\\\tag{\\1}\\]\", s, flags=re.M\n    )\n    s = re.sub(\n        r\"^\\\\\\[(.+?)\\\\\\] \\(([\\d.]+[a-zA-Z]?)\\)$\", r\"\\[\\1 \\\\tag{\\2}\\]\", s, flags=re.M\n    )\n    s = re.sub(\n        r\"^\\\\\\[(.+?)\\\\\\] \\(([\\d.]+[a-zA-Z]?)\\) (\\\\\\[.+?\\\\\\])$\",\n        r\"\\[\\1 \\\\tag{\\2}\\] \\3\",\n        s,\n        flags=re.M,\n    )\n    s = s.replace(r\"\\. \", \". \")\n    s = s.replace(r\"\\.}\", \".}\")\n    s = s.replace(r\"\\. }\", \". }\")\n    s = s.replace(r\"\\.\\]\", \".]\")\n    s = s.replace(r\"\\. ]\", \". ]\")\n    s = re.sub(r\"\\\\begin\\{table\\}\\s*\\\\begin\\{tabular\\}(.*?)\\\\end\\{tabular\\}\\s*\\\\end\\{table\\}\", r\"\\n\\\\begin{table}\\n\\\\begin{tabular}\\1\\\\end{tabular}\\n\\\\end{table}\\n\", s, flags=re.DOTALL)\n\n    s = re.sub(r\"([^\\s])\\$([^\\$]*)\\$\", r\"\\1 $\\2$\", s)\n    s = re.sub(r\"\\$([^\\$]*)\\$([^\\s])\", r\"$\\1$ \\2\", s)\n\n    return s\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.postprocess","title":"<code>postprocess(generation: Union[str, List[str]], markdown_fix: bool = True) -&gt; Union[str, List[str]]</code>","text":"<p>Postprocess generated text or a list of generated texts.</p> <p>This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>Union[str, List[str]]</code> <p>The generated text or a list of generated texts.</p> required <code>markdown_fix</code> <code>bool</code> <p>Whether to perform Markdown formatting fixes. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[str, List[str]]</code> <p>Union[str, List[str]]: The postprocessed text or list of postprocessed texts.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def postprocess(\n    generation: Union[str, List[str]], markdown_fix: bool = True\n) -&gt; Union[str, List[str]]:\n    \"\"\"\n    Postprocess generated text or a list of generated texts.\n\n    This function can be used to perform postprocessing on generated text, such as fixing Markdown formatting.\n\n    Args:\n        generation (Union[str, List[str]]): The generated text or a list of generated texts.\n        markdown_fix (bool, optional): Whether to perform Markdown formatting fixes. Default is True.\n\n    Returns:\n        Union[str, List[str]]: The postprocessed text or list of postprocessed texts.\n    \"\"\"\n    if type(generation) == list:\n        if os.environ.get(\"NOUGAT_MULTIPROCESSING\"):\n            with Pool(int(os.environ.get(\"NOUGAT_MULTIPROCESSING\"))) as p:\n                return p.map(\n                    partial(postprocess_single, markdown_fix=markdown_fix), generation\n                )\n        else:\n            return [\n                postprocess_single(s, markdown_fix=markdown_fix) for s in generation\n            ]\n    else:\n        return postprocess_single(generation, markdown_fix=markdown_fix)\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.postprocess_single","title":"<code>postprocess_single(generation: str, markdown_fix: bool = True) -&gt; str</code>","text":"<p>Postprocess a single generated text.</p> <p>Parameters:</p> Name Type Description Default <code>generation</code> <code>str</code> <p>The generated text to be postprocessed.</p> required <code>markdown_fix</code> <code>bool</code> <p>Whether to perform Markdown formatting fixes. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The postprocessed text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def postprocess_single(generation: str, markdown_fix: bool = True) -&gt; str:\n    \"\"\"\n    Postprocess a single generated text.\n\n    Args:\n        generation (str): The generated text to be postprocessed.\n        markdown_fix (bool, optional): Whether to perform Markdown formatting fixes. Default is True.\n\n    Returns:\n        str: The postprocessed text.\n    \"\"\"\n    generation = re.sub(\n        r\"(?:\\n|^)#+ \\d*\\W? ?(.{100,})\", r\"\\n\\1\", generation\n    )\n    generation = generation.strip()\n    generation = generation.replace(\"\\n* [leftmargin=*]\\n\", \"\\n\")\n    generation = re.sub(\n        r\"^#+ (?:\\.?(?:\\d|[ixv])+)*\\s*(?:$|\\n\\s*)\", \"\", generation, flags=re.M\n    )\n    lines = generation.split(\"\\n\")\n    if (\n        lines[-1].startswith(\"#\")\n        and lines[-1].lstrip(\"#\").startswith(\" \")\n        and len(lines) &gt; 1\n    ):\n        print(\"INFO: likely hallucinated title at the end of the page: \" + lines[-1])\n        generation = \"\\n\".join(lines[:-1])\n    generation = truncate_repetitions(generation)\n    generation = remove_hallucinated_references(generation)\n    generation = re.sub(\n        r\"^\\* \\[\\d+\\](\\s?[A-W]\\.+\\s?){10,}.*$\", \"\", generation, flags=re.M\n    )\n    generation = re.sub(r\"^(\\* \\[\\d+\\])\\[\\](.*)$\", r\"\\1\\2\", generation, flags=re.M)\n    generation = re.sub(r\"(^\\w\\n\\n|\\n\\n\\w$)\", \"\", generation)\n    generation = re.sub(\n        r\"([\\s.,()])_([a-zA-Z0-9])__([a-zA-Z0-9]){1,3}_([\\s.,:()])\",\n        r\"\\1\\(\\2_{\\3}\\)\\4\",\n        generation,\n    )\n    generation = re.sub(\n        r\"([\\s.,\\d])_([a-zA-Z0-9])_([\\s.,\\d;])\", r\"\\1\\(\\2\\)\\3\", generation\n    )\n    generation = re.sub(\n        r\"(\\nFootnote .*?:) (?:footnotetext|thanks):\\W*(.*(?:\\n\\n|$))\",\n        r\"\\1 \\2\",\n        generation,\n    )\n    generation = re.sub(r\"\\[FOOTNOTE:.+?\\](.*?)\\[ENDFOOTNOTE\\]\", \"\", generation)\n    for match in reversed(\n        list(\n            re.finditer(\n                r\"(?:^)(-|\\*)?(?!-|\\*) ?((?:\\d|[ixv])+ )?.+? (-|\\*) (((?:\\d|[ixv])+)\\.(\\d|[ixv]) )?.*(?:$)\",\n                generation,\n                flags=re.I | re.M,\n            )\n        )\n    ):\n        start, stop = match.span()\n        delim = match.group(3) + \" \"\n        splits = match.group(0).split(delim)\n        replacement = \"\"\n        if match.group(1) is not None:\n            splits = splits[1:]\n            delim1 = match.group(1) + \" \"\n        else:\n            delim1 = \"\"\n            continue\n        pre, post = generation[:start], generation[stop:]\n        for i, item in enumerate(splits):\n            level = 0\n            potential_numeral, _, rest = item.strip().partition(\" \")\n            if not rest:\n                continue\n            if re.match(\n                r\"^[\\dixv]+((?:\\.[\\dixv])?)+$\", potential_numeral, flags=re.I | re.M\n            ):\n                level = potential_numeral.count(\".\")\n\n            replacement += (\n                (\"\\n\" if i &gt; 0 else \"\")\n                + (\"\\t\" * level)\n                + (delim if i &gt; 0 or start == 0 else delim1)\n                + item.strip()\n            )\n        if post == \"\":\n            post = \"\\n\"\n        generation = pre + replacement + post\n\n    if generation.endswith((\".\", \"}\")):\n        generation += \"\\n\\n\"\n    if re.match(r\"[A-Z0-9,;:]$\", generation):\n        generation += \" \"\n    elif generation.startswith((\"#\", \"**\", \"\\\\begin\")):\n        generation = \"\\n\\n\" + generation\n    elif generation.split(\"\\n\")[-1].startswith((\"#\", \"Figure\", \"Table\")):\n        generation = generation + \"\\n\\n\"\n    else:\n        try:\n            last_word = generation.split(\" \")[-1]\n            if last_word in words.words():\n                generation += \" \"\n        except LookupError:\n            generation += \" \"\n            import nltk\n\n            nltk.download(\"words\")\n    for l in generation.split(\"\\n\"):\n        if (\n            l.count(\"\\\\begin{tabular}\") &gt; 15\n            or l.count(\"\\\\multicolumn\") &gt; 60\n            or l.count(\"&amp;\") &gt; 400\n        ):\n            generation = generation.replace(l, \"\")\n    generation = generation.replace(\n        \"\\\\begin{table} \\\\begin{tabular}\", \"\\\\begin{table}\\n\\\\begin{tabular}\"\n    )\n    generation = generation.replace(\n        \"\\\\end{tabular} \\\\end{table}\", \"\\\\end{tabular}\\n\\\\end{table}\"\n    )\n    generation = generation.replace(\"\\\\end{table} Tab\", \"\\\\end{table}\\nTab\")\n    generation = re.sub(r\"(^.+)\\\\begin{tab\", r\"\\1\\n\\\\begin{tab\", generation, flags=re.M)\n\n    generation = generation.replace(\n        r\"\\begin{tabular}{l l}  &amp; \\\\ \\end{tabular}\", \"\"\n    ).replace(\"\\\\begin{tabular}{}\\n\\n\\\\end{tabular}\", \"\")\n    generation = generation.replace(\"\\\\begin{array}[]{\", \"\\\\begin{array}{\")\n    generation = re.sub(\n        r\"\\\\begin{tabular}{([clr ]){2,}}\\s*[&amp; ]*\\s*(\\\\\\\\)? \\\\end{tabular}\",\n        \"\",\n        generation,\n    )\n    generation = re.sub(r\"(\\*\\*S\\. A\\. B\\.\\*\\*\\n+){2,}\", \"\", generation)\n    generation = re.sub(r\"^#+( [\\[\\d\\w])?$\", \"\", generation, flags=re.M)\n    generation = re.sub(r\"^\\.\\s*$\", \"\", generation, flags=re.M)\n    generation = re.sub(r\"\\n{3,}\", \"\\n\\n\", generation)\n    if markdown_fix:\n        return markdown_compatible(generation)\n    else:\n        return generation\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.remove_hallucinated_references","title":"<code>remove_hallucinated_references(text: str) -&gt; str</code>","text":"<p>Remove hallucinated or missing references from the text.</p> <p>This function identifies and removes references that are marked as missing or hallucinated from the input text.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The input text containing references.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The text with hallucinated references removed.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_hallucinated_references(text: str) -&gt; str:\n    \"\"\"\n    Remove hallucinated or missing references from the text.\n\n    This function identifies and removes references that are marked as missing or hallucinated\n    from the input text.\n\n    Args:\n        text (str): The input text containing references.\n\n    Returns:\n        str: The text with hallucinated references removed.\n    \"\"\"\n    lines = text.split(\"\\n\")\n    if len(lines) == 0:\n        return \"\"\n    clean_lines = remove_numbers(lines)\n    slices = get_slices(lines, clean_lines)\n    to_delete = []\n    for sli in slices:\n        to_delete.append(remove_slice_from_lines(lines, clean_lines, sli))\n    for to_delete in reversed(to_delete):\n        text = text.replace(to_delete, \"\\n\\n[MISSING_PAGE_POST]\\n\\n\")\n    text = re.sub(\n        r\"## References\\n+\\[MISSING_PAGE_POST(:\\d+)?\\]\",\n        \"\\n\\n[MISSING_PAGE_POST\\\\1]\",\n        text,\n    )\n    return text\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.remove_numbers","title":"<code>remove_numbers(lines: List[str]) -&gt; List[str]</code>","text":"<p>Remove number patterns from lines.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_numbers(lines: List[str]) -&gt; List[str]:\n    \"\"\"Remove number patterns from lines.\"\"\"\n    clean_lines = []\n    for line in lines:\n        clean_line = re.sub(r'\\[\\d+\\]', '', line)\n        clean_line = re.sub(r'\\d+\\.', '', clean_line)\n        clean_lines.append(clean_line.strip())\n    return clean_lines\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.remove_slice_from_lines","title":"<code>remove_slice_from_lines(lines: List[str], clean_lines: List[str], sli: slice) -&gt; str</code>","text":"<p>Remove slice from lines and return the removed text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def remove_slice_from_lines(lines: List[str], clean_lines: List[str], sli: slice) -&gt; str:\n    \"\"\"Remove slice from lines and return the removed text.\"\"\"\n    removed_text = '\\n'.join(lines[sli])\n    return removed_text\n</code></pre>"},{"location":"api/stages/#eve.steps.cleaning.nougat_helpers.truncate_repetitions","title":"<code>truncate_repetitions(generation: str, score_cutoff: float = 0.5, min_len: int = 30)</code>","text":"<p>Truncate repetitions in the given generation.</p> <p>This function identifies and truncates repetitive content in the text.</p> Source code in <code>eve/steps/cleaning/nougat_helpers.py</code> <pre><code>def truncate_repetitions(generation: str, score_cutoff: float = 0.5, min_len: int = 30):\n    \"\"\"\n    Truncate repetitions in the given generation.\n\n    This function identifies and truncates repetitive content in the text.\n    \"\"\"\n    try:\n        sentences = generation.split(\".\")\n        if len(sentences) &lt; 3:\n            return generation\n\n        to_delete = set()\n        for i in range(len(sentences)):\n            for j in range(i + 1, len(sentences)):\n                sent_i = sentences[i].strip()\n                sent_j = sentences[j].strip()\n\n                if len(sent_i) &lt; min_len or len(sent_j) &lt; min_len:\n                    continue\n\n                if ratio(sent_i, sent_j) &gt; score_cutoff:\n                    to_delete.add(j)\n\n        new_sentences = [sent for i, sent in enumerate(sentences) if i not in to_delete]\n        return \".\".join(new_sentences)\n    except Exception:\n        return generation\n</code></pre>"},{"location":"api/stages/#pii-removal-stage","title":"PII Removal Stage","text":"<p>Removes personally identifiable information from documents.</p>"},{"location":"api/stages/#eve.steps.pii.pii_step","title":"<code>eve.steps.pii.pii_step</code>","text":""},{"location":"api/stages/#eve.steps.pii.pii_step.PiiStep","title":"<code>PiiStep(config: Any, name: Optional[str] = None)</code>","text":"<p>               Bases: <code>PipelineStep</code></p> Source code in <code>eve/base_step.py</code> <pre><code>def __init__(self, config: Any, name: Optional[str] = None):\n    \"\"\"initialize the pipeline step.\n\n    Args:\n        config: Configuration specific to the step.\n        name: Optional name for the step (used for logging).\n    \"\"\"\n    self.config = config\n    self.debug = config.get(\"debug\", False) if isinstance(config, dict) else False\n    self.logger = get_logger(name or self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#metadata-extraction-stage","title":"Metadata Extraction Stage","text":"<p>Extracts structured metadata from documents.</p>"},{"location":"api/stages/#eve.steps.metadata.metadata_step","title":"<code>eve.steps.metadata.metadata_step</code>","text":"<p>Metadata extraction step for the EVE pipeline.</p>"},{"location":"api/stages/#eve.steps.metadata.metadata_step.MetadataStep","title":"<code>MetadataStep(config: dict)</code>","text":"<p>               Bases: <code>PipelineStep</code></p> <p>Metadata extraction step that extracts metadata from PDF and HTML documents.</p> <p>Initialize the metadata extraction step.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary with options: - enabled_formats: List of file formats to process (pdf, html, txt, md) - fallback_to_filename: Use filename as title fallback - debug: Enable debug logging - export_metadata: Whether to export metadata to JSON file - metadata_destination: Directory to save metadata file - metadata_filename: Name of the metadata JSON file   Note: Text formats (txt, md) automatically enable this feature</p> required Source code in <code>eve/steps/metadata/metadata_step.py</code> <pre><code>def __init__(self, config: dict):\n    \"\"\"\n    Initialize the metadata extraction step.\n\n    Args:\n        config: Configuration dictionary with options:\n            - enabled_formats: List of file formats to process (pdf, html, txt, md)\n            - fallback_to_filename: Use filename as title fallback\n            - debug: Enable debug logging\n            - export_metadata: Whether to export metadata to JSON file\n            - metadata_destination: Directory to save metadata file\n            - metadata_filename: Name of the metadata JSON file\n              Note: Text formats (txt, md) automatically enable this feature\n    \"\"\"\n    super().__init__(config)\n\n    self.enabled_formats = set(config.get(\"enabled_formats\", [\"pdf\", \"html\", \"txt\", \"md\"]))\n    self.fallback_to_filename = config.get(\"fallback_to_filename\", True)\n    self.export_metadata = config.get(\"export_metadata\", True)\n    self.metadata_destination = Path(config.get(\"metadata_destination\", \"./output\"))\n    self.metadata_filename = config.get(\"metadata_filename\", \"metadata.jsonl\")\n\n    self.extractors = {\n        \"pdf\": PdfMetadataExtractor(debug=self.debug),\n        \"html\": HtmlMetadataExtractor(debug=self.debug)\n    }\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.metadata_step.MetadataStep.execute","title":"<code>execute(documents: List[Document]) -&gt; List[Document]</code>  <code>async</code>","text":"<p>Execute metadata extraction on input documents.</p> <p>Parameters:</p> Name Type Description Default <code>documents</code> <code>List[Document]</code> <p>List of Document objects to extract metadata from</p> required <p>Returns:</p> Type Description <code>List[Document]</code> <p>List of Document objects with metadata added</p> Source code in <code>eve/steps/metadata/metadata_step.py</code> <pre><code>async def execute(self, documents: List[Document]) -&gt; List[Document]:\n    \"\"\"\n    Execute metadata extraction on input documents.\n\n    Args:\n        documents: List of Document objects to extract metadata from\n\n    Returns:\n        List of Document objects with metadata added\n    \"\"\"\n    if not documents:\n        self.logger.warning(\"No input documents provided to metadata step\")\n        return []\n\n    supported_documents = [\n        doc for doc in documents \n        if doc.file_format in self.enabled_formats\n    ]\n\n    unsupported_documents = [\n        doc for doc in documents \n        if doc.file_format not in self.enabled_formats\n    ]\n\n    if unsupported_documents:\n        self.logger.info(f\"Skipping {len(unsupported_documents)} documents with unsupported formats\")\n\n    if not supported_documents:\n        self.logger.warning(\"No documents with supported formats found\")\n        return documents\n\n    self.logger.info(f\"Extracting metadata from {len(supported_documents)} documents\")\n\n    tasks = [\n        self._extract_metadata_for_document(document) \n        for document in supported_documents\n    ]\n\n    processed_supported = await asyncio.gather(*tasks, return_exceptions=True)\n\n    result_supported = []\n    for i, result in enumerate(processed_supported):\n        if isinstance(result, Exception):\n            self.logger.error(f\"Exception processing {supported_documents[i].filename}: {result}\")\n            result_supported.append(supported_documents[i])\n        else:\n            result_supported.append(result)\n\n    final_result = result_supported + unsupported_documents\n\n    successful_count = sum(1 for doc in result_supported if doc.get_metadata(\"extracted_metadata\"))\n    self.logger.info(f\"Successfully extracted metadata from {successful_count}/{len(supported_documents)} supported documents\")\n\n    if self.export_metadata:\n        await self._export_metadata_to_json(final_result)\n\n    return final_result\n</code></pre>"},{"location":"api/stages/#metadata-extractors","title":"Metadata Extractors","text":""},{"location":"api/stages/#html-metadata-extractor","title":"HTML Metadata Extractor","text":""},{"location":"api/stages/#eve.steps.metadata.extractors.html_extractor","title":"<code>eve.steps.metadata.extractors.html_extractor</code>","text":""},{"location":"api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor","title":"<code>HtmlMetadataExtractor(debug: bool = False)</code>","text":"<p>Metadata extractor for HTML files and web pages.</p> <p>Initialize the HTML metadata extractor.</p> <p>The HTML extractor relies on regex patterns defined in eve.common.regex_patterns for parsing HTML content efficiently without requiring a full HTML parser.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug logging for detailed extraction information</p> <code>False</code> Source code in <code>eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"\n    Initialize the HTML metadata extractor.\n\n    The HTML extractor relies on regex patterns defined in eve.common.regex_patterns\n    for parsing HTML content efficiently without requiring a full HTML parser.\n\n    Args:\n        debug: Enable debug logging for detailed extraction information\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.extractors.html_extractor.HtmlMetadataExtractor.extract_metadata","title":"<code>extract_metadata(document: Document) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Extract metadata from an HTML document using multi-source approach.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>HTML document to extract metadata from</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing extracted metadata with fields:</p> <code>Optional[Dict[str, Any]]</code> <ul> <li>title: Page title (from various sources, with title_source indicator)</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>title_source: Source of title ('html_tag', 'meta_tag', 'filename')</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>url: Source URL if available</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>domain: Domain name from URL</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>scheme: URL scheme (http/https)</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>content_length: Length of HTML content</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>has_content: Boolean indicating content exists</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>extraction_methods: List containing 'html_parsing'</li> </ul> <code>Optional[Dict[str, Any]]</code> <p>Returns None if document format is invalid</p> Source code in <code>eve/steps/metadata/extractors/html_extractor.py</code> <pre><code>async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract metadata from an HTML document using multi-source approach.\n\n    Args:\n        document: HTML document to extract metadata from\n\n    Returns:\n        Dictionary containing extracted metadata with fields:\n        - title: Page title (from various sources, with title_source indicator)\n        - title_source: Source of title ('html_tag', 'meta_tag', 'filename')\n        - url: Source URL if available\n        - domain: Domain name from URL\n        - scheme: URL scheme (http/https)\n        - content_length: Length of HTML content\n        - has_content: Boolean indicating content exists\n        - extraction_methods: List containing 'html_parsing'\n\n        Returns None if document format is invalid\n    \"\"\"\n\n    metadata = {}\n\n    document = self._extract_content_with_tags(document) # do this because extraction from previous step removes tag\n\n    extracted_title = self._extract_title_from_html(document.content)\n    metadata['title'] = extracted_title\n    metadata['content_length'] = len(document.content)\n\n    return metadata\n</code></pre>"},{"location":"api/stages/#pdf-metadata-extractor","title":"PDF Metadata Extractor","text":""},{"location":"api/stages/#eve.steps.metadata.extractors.pdf_extractor","title":"<code>eve.steps.metadata.extractors.pdf_extractor</code>","text":"<p>PDF metadata extractor is a two stage process. 1. Extract content using monkeyocr. 2. Use crossref to extract metadata from the content.</p>"},{"location":"api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor","title":"<code>PdfMetadataExtractor(debug: bool = False)</code>","text":"<p>Initialize the PDF metadata extractor.</p> <p>Parameters:</p> Name Type Description Default <code>debug</code> <code>bool</code> <p>Enable debug logging for detailed extraction information</p> <code>False</code> Source code in <code>eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>def __init__(self, debug: bool = False):\n    \"\"\"\n    Initialize the PDF metadata extractor.\n\n    Args:\n        debug: Enable debug logging for detailed extraction information\n    \"\"\"\n    self.debug = debug\n    self.logger = get_logger(self.__class__.__name__)\n</code></pre>"},{"location":"api/stages/#eve.steps.metadata.extractors.pdf_extractor.PdfMetadataExtractor.extract_metadata","title":"<code>extract_metadata(document: Document) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Extract metadata from a PDF document using crossref.</p> <p>Parameters:</p> Name Type Description Default <code>document</code> <code>Document</code> <p>PDF document to extract metadata from</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Dictionary containing extracted metadata with fields:</p> <code>Optional[Dict[str, Any]]</code> <ul> <li>title: Document title</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>authors: List of author names </li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>year: Publication year</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>journal: Publication venue</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>doi: Digital Object Identifier</li> </ul> <code>Optional[Dict[str, Any]]</code> <ul> <li>identifier: Document identifier (DOI, arXiv, etc.)</li> </ul> <code>Optional[Dict[str, Any]]</code> <p>Returns None if document format is invalid</p> Source code in <code>eve/steps/metadata/extractors/pdf_extractor.py</code> <pre><code>async def extract_metadata(self, document: Document) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Extract metadata from a PDF document using crossref.\n\n    Args:\n        document: PDF document to extract metadata from\n\n    Returns:\n        Dictionary containing extracted metadata with fields:\n        - title: Document title\n        - authors: List of author names \n        - year: Publication year\n        - journal: Publication venue\n        - doi: Digital Object Identifier\n        - identifier: Document identifier (DOI, arXiv, etc.)\n\n        Returns None if document format is invalid\n    \"\"\"\n\n    metadata = {}\n    done = set()\n\n    subdirs = [d for d in os.listdir(main_dir)\n            if os.path.isdir(os.path.join(main_dir, d)) and d not in done]\n\n    if not subdirs:\n        print(\"All subdirectories already processed.\")\n        return\n\n    sem = asyncio.Semaphore(MAX_CONCURRENT)\n    async with httpx.AsyncClient() as client:\n        tasks = [self._extract_metadata(d, main_dir, client, sem) for d in subdirs]\n\n        for coro in tqdm(asyncio.as_completed(tasks), total=len(tasks)):\n            try:\n                r = await coro\n                metadata[r.get(\"sub_dir\")] = r.get(\"meta\")\n            except Exception as e:\n                print(f\"Failed on {coro}: {e}\")\n\n    return metadata\n</code></pre>"},{"location":"api/stages/#export-stage","title":"Export Stage","text":"<p>Saves processed documents to output formats.</p>"},{"location":"api/stages/#eve.steps.export.export_step","title":"<code>eve.steps.export.export_step</code>","text":""},{"location":"api/utilities/","title":"Utilities","text":"<p>This section covers utility functions and helper modules used throughout the pipeline.</p>"},{"location":"api/utilities/#common-utilities","title":"Common Utilities","text":"<p>General-purpose utility functions.</p>"},{"location":"api/utilities/#eve.utils","title":"<code>eve.utils</code>","text":""},{"location":"api/utilities/#eve.utils.read_in_chunks","title":"<code>read_in_chunks(file_path: Path, mode: str, chunk_size: int = 4096) -&gt; AsyncGenerator[bytes, None]</code>  <code>async</code>","text":"<p>read a binary file in chunks.</p> Source code in <code>eve/utils.py</code> <pre><code>async def read_in_chunks(file_path: Path, mode: str, chunk_size: int = 4096) -&gt; AsyncGenerator[bytes, None]:\n    \"\"\"\n    read a binary file in chunks.\n    \"\"\"\n    async with aiofiles.open(file_path, mode) as f:\n        while chunk := await f.read(chunk_size):\n            yield chunk\n</code></pre>"},{"location":"api/utilities/#http-utils","title":"HTTP Utils","text":"<p>HTTP client utilities for server-based processing.</p>"},{"location":"api/utilities/#eve.common.http_utils","title":"<code>eve.common.http_utils</code>","text":"<p>Common HTTP utilities for making API calls across the pipeline.</p>"},{"location":"api/utilities/#eve.common.http_utils.make_openrouter_request","title":"<code>make_openrouter_request(api_key: str, model: str, prompt: str, max_tokens: int = 1000, temperature: float = 0.1) -&gt; Optional[str]</code>  <code>async</code>","text":"<p>Make a request to OpenRouter API for LLM completion.</p> <p>Parameters:</p> Name Type Description Default <code>api_key</code> <code>str</code> <p>OpenRouter API key</p> required <code>model</code> <code>str</code> <p>Model name to use</p> required <code>prompt</code> <code>str</code> <p>The prompt to send</p> required <code>max_tokens</code> <code>int</code> <p>Maximum tokens in response</p> <code>1000</code> <code>temperature</code> <code>float</code> <p>Temperature for response generation</p> <code>0.1</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Response content or None if request failed</p> Source code in <code>eve/common/http_utils.py</code> <pre><code>async def make_openrouter_request(\n    api_key: str,\n    model: str,\n    prompt: str,\n    max_tokens: int = 1000,\n    temperature: float = 0.1\n) -&gt; Optional[str]:\n    \"\"\"\n    Make a request to OpenRouter API for LLM completion.\n\n    Args:\n        api_key: OpenRouter API key\n        model: Model name to use\n        prompt: The prompt to send\n        max_tokens: Maximum tokens in response\n        temperature: Temperature for response generation\n\n    Returns:\n        Response content or None if request failed\n    \"\"\"\n    url = \"https://openrouter.ai/api/v1/chat/completions\"\n    headers = {\n        \"Authorization\": f\"Bearer {api_key}\",\n        \"Content-Type\": \"application/json\"\n    }\n    data = {\n        \"model\": model,\n        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n        \"max_tokens\": max_tokens,\n        \"temperature\": temperature\n    }\n\n    response = await post_request(url, headers, data)\n    if response and \"choices\" in response:\n        content = response[\"choices\"][0][\"message\"][\"content\"].strip()\n        import re\n        content = re.sub(r'^```latex\\n?', '', content)\n        content = re.sub(r'\\n?```$', '', content)\n        return content.strip()\n\n    return None\n</code></pre>"},{"location":"api/utilities/#eve.common.http_utils.post_request","title":"<code>post_request(url: str, headers: Dict[str, str], data: Dict[str, Any], timeout: int = 30) -&gt; Optional[Dict[str, Any]]</code>  <code>async</code>","text":"<p>Make an async POST request and return JSON response.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to make the request to</p> required <code>headers</code> <code>Dict[str, str]</code> <p>Request headers</p> required <code>data</code> <code>Dict[str, Any]</code> <p>Request data to send as JSON</p> required <code>timeout</code> <code>int</code> <p>Request timeout in seconds</p> <code>30</code> <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Response JSON data or None if request failed</p> Source code in <code>eve/common/http_utils.py</code> <pre><code>async def post_request(\n    url: str,\n    headers: Dict[str, str],\n    data: Dict[str, Any],\n    timeout: int = 30\n) -&gt; Optional[Dict[str, Any]]:\n    \"\"\"\n    Make an async POST request and return JSON response.\n\n    Args:\n        url: The URL to make the request to\n        headers: Request headers\n        data: Request data to send as JSON\n        timeout: Request timeout in seconds\n\n    Returns:\n        Response JSON data or None if request failed\n    \"\"\"\n    try:\n        async with aiohttp.ClientSession() as session:\n            async with session.post(\n                url, \n                headers=headers, \n                json=data,\n                timeout=aiohttp.ClientTimeout(total=timeout)\n            ) as response:\n                if response.status == 200:\n                    return await response.json()\n                else:\n                    logger.error(f\"HTTP request failed with status {response.status}\")\n                    return None\n    except Exception as e:\n        logger.error(f\"HTTP request failed: {str(e)}\")\n        return None\n</code></pre>"},{"location":"api/utilities/#regex-patterns","title":"Regex Patterns","text":"<p>Common regular expression patterns used throughout the pipeline.</p>"},{"location":"api/utilities/#eve.common.regex_patterns","title":"<code>eve.common.regex_patterns</code>","text":"<p>Common regex patterns used across the pipeline.</p>"},{"location":"api/utilities/#eve.common.regex_patterns.clean_doubled_backslashes","title":"<code>clean_doubled_backslashes(text: str) -&gt; str</code>","text":"<p>Clean up doubled backslashes in LaTeX content.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def clean_doubled_backslashes(text: str) -&gt; str:\n    \"\"\"Clean up doubled backslashes in LaTeX content.\"\"\"\n    return DOUBLED_BACKSLASH_PATTERN.sub(lambda m: '\\\\' * (len(m.group()) // 2), text)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.extract_html_meta_tags","title":"<code>extract_html_meta_tags(html_content: str) -&gt; dict[str, str]</code>","text":"<p>Extract metadata from HTML meta tags.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dictionary containing extracted meta tag information</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def extract_html_meta_tags(html_content: str) -&gt; dict[str, str]:\n    \"\"\"\n    Extract metadata from HTML meta tags.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Dictionary containing extracted meta tag information\n    \"\"\"\n    meta_data = {}\n\n    if not html_content:\n        return meta_data\n\n    meta_patterns = {\n        'description': r'&lt;meta[^&gt;]*name=[\"\\']description[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'keywords': r'&lt;meta[^&gt;]*name=[\"\\']keywords[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'author': r'&lt;meta[^&gt;]*name=[\"\\']author[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'og_title': r'&lt;meta[^&gt;]*property=[\"\\']og:title[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'og_description': r'&lt;meta[^&gt;]*property=[\"\\']og:description[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n        'twitter_title': r'&lt;meta[^&gt;]*name=[\"\\']twitter:title[\"\\'][^&gt;]*content=[\"\\']([^\"\\']*)[\"\\']',\n    }\n\n    for key, pattern in meta_patterns.items():\n        match = re.search(pattern, html_content, re.IGNORECASE)\n        if match:\n            value = match.group(1).strip()\n            if value:\n                meta_data[key] = value\n\n    return meta_data\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.extract_html_title","title":"<code>extract_html_title(html_content: str) -&gt; str</code>","text":"<p>Extract title from HTML content.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>str</code> <p>Extracted and cleaned title, or None if not found</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def extract_html_title(html_content: str) -&gt; str:\n    \"\"\"\n    Extract title from HTML content.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Extracted and cleaned title, or None if not found\n    \"\"\"\n    if not html_content:\n        return None\n\n    title_match = HTML_TITLE_PATTERN.search(html_content)\n\n    if title_match:\n        title = title_match.group(1)\n\n        title = HTML_TAG_PATTERN.sub('', title)\n        title = HTML_ENTITY_PATTERN.sub(' ', title)\n        title = HTML_NUMERIC_ENTITY_PATTERN.sub(' ', title)\n\n        return title.strip()\n\n    return None\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.extract_json_ld_count","title":"<code>extract_json_ld_count(html_content: str) -&gt; int</code>","text":"<p>Count JSON-LD structured data blocks in HTML.</p> <p>Parameters:</p> Name Type Description Default <code>html_content</code> <code>str</code> <p>HTML content as string</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of JSON-LD script blocks found</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def extract_json_ld_count(html_content: str) -&gt; int:\n    \"\"\"\n    Count JSON-LD structured data blocks in HTML.\n\n    Args:\n        html_content: HTML content as string\n\n    Returns:\n        Number of JSON-LD script blocks found\n    \"\"\"\n    if not html_content:\n        return 0\n\n    json_ld_matches = JSON_LD_SCRIPT_PATTERN.findall(html_content)\n    return len(json_ld_matches)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.fix_ocr_digit_letter_spacing","title":"<code>fix_ocr_digit_letter_spacing(text: str) -&gt; str</code>","text":"<p>Fix OCR issues where digits are concatenated with letters.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def fix_ocr_digit_letter_spacing(text: str) -&gt; str:\n    \"\"\"Fix OCR issues where digits are concatenated with letters.\"\"\"\n    return DIGIT_LETTER_PATTERN.sub(r'\\1 \\2', text)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.get_latex_formula_patterns","title":"<code>get_latex_formula_patterns() -&gt; dict[str, Pattern[str]]</code>","text":"<p>Get all LaTeX formula patterns in a dictionary.</p> <p>Returns:</p> Type Description <code>dict[str, Pattern[str]]</code> <p>Dictionary mapping pattern names to compiled regex patterns</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def get_latex_formula_patterns() -&gt; dict[str, Pattern[str]]:\n    \"\"\"\n    Get all LaTeX formula patterns in a dictionary.\n\n    Returns:\n        Dictionary mapping pattern names to compiled regex patterns\n    \"\"\"\n    return {\n        'inline': INLINE_MATH_PATTERN,\n        'display': DISPLAY_MATH_PATTERN,\n        'bracket': BRACKET_MATH_PATTERN,\n        'square_bracket': SQUARE_BRACKET_MATH_PATTERN,\n        'environment': LATEX_ENV_PATTERN\n    }\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.normalize_excessive_newlines","title":"<code>normalize_excessive_newlines(text: str) -&gt; str</code>","text":"<p>Replace 3+ consecutive newlines with exactly 2.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def normalize_excessive_newlines(text: str) -&gt; str:\n    \"\"\"Replace 3+ consecutive newlines with exactly 2.\"\"\"\n    return EXCESSIVE_NEWLINES_PATTERN.sub('\\n\\n', text)\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.remove_nougat_artifacts","title":"<code>remove_nougat_artifacts(text: str) -&gt; str</code>","text":"<p>Remove Nougat-specific warning and error artifacts.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def remove_nougat_artifacts(text: str) -&gt; str:\n    \"\"\"Remove Nougat-specific warning and error artifacts.\"\"\"\n    text = WARNING_PATTERN.sub('', text)\n    text = ERROR_PATTERN.sub('', text)\n    text = text.replace('[MISSING_PAGE_POST]', '')\n    return text\n</code></pre>"},{"location":"api/utilities/#eve.common.regex_patterns.remove_single_symbol_lines","title":"<code>remove_single_symbol_lines(text: str) -&gt; str</code>","text":"<p>Remove lines that contain only a single symbol or punctuation.</p> Source code in <code>eve/common/regex_patterns.py</code> <pre><code>def remove_single_symbol_lines(text: str) -&gt; str:\n    \"\"\"Remove lines that contain only a single symbol or punctuation.\"\"\"\n    lines = text.split('\\n')\n    cleaned_lines = []\n\n    for line in lines:\n        stripped = line.strip()\n        if re.search(r'\\w', stripped) or len(stripped) != 1:\n            cleaned_lines.append(line)\n\n    return '\\n'.join(cleaned_lines)\n</code></pre>"},{"location":"api/utilities/#prompts","title":"Prompts","text":"<p>Prompt templates used in LLM-based processing.</p>"},{"location":"api/utilities/#eve.common.prompts","title":"<code>eve.common.prompts</code>","text":"<p>Common prompts used across the pipeline.</p>"},{"location":"api/utilities/#eve.common.prompts.get_latex_correction_prompt","title":"<code>get_latex_correction_prompt(formula_type: str, error_message: str, formula: str, context: str) -&gt; str</code>","text":"<p>Generate a LaTeX correction prompt.</p> <p>Parameters:</p> Name Type Description Default <code>formula_type</code> <code>str</code> <p>Type of LaTeX formula (inline, display, etc.)</p> required <code>error_message</code> <code>str</code> <p>The error message from LaTeX compilation</p> required <code>formula</code> <code>str</code> <p>The problematic formula</p> required <code>context</code> <code>str</code> <p>Surrounding context for better understanding</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted prompt string</p> Source code in <code>eve/common/prompts.py</code> <pre><code>def get_latex_correction_prompt(\n    formula_type: str,\n    error_message: str,\n    formula: str,\n    context: str\n) -&gt; str:\n    \"\"\"\n    Generate a LaTeX correction prompt.\n\n    Args:\n        formula_type: Type of LaTeX formula (inline, display, etc.)\n        error_message: The error message from LaTeX compilation\n        formula: The problematic formula\n        context: Surrounding context for better understanding\n\n    Returns:\n        Formatted prompt string\n    \"\"\"\n    context_snippet = context[:1000] + \"...\" if len(context) &gt; 1000 else context\n\n    return LATEX_CORRECTION_PROMPT.format(\n        formula_type=formula_type,\n        error_message=error_message,\n        formula=formula,\n        context_snippet=context_snippet\n    )\n</code></pre>"},{"location":"api/utilities/#logging","title":"Logging","text":"<p>Logging configuration and utilities.</p>"},{"location":"api/utilities/#eve.logging","title":"<code>eve.logging</code>","text":""},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>This section provides practical examples of using the EVE Pipeline for common document processing tasks.</p>"},{"location":"examples/basic-usage/#simple-document-processing","title":"Simple Document Processing","text":"<p>Process all documents from an input directory and export to markdown:</p> <pre><code># config.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_documents\"\n  stages:\n    - name: extraction\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"output\"\n</code></pre> <pre><code># Run the pipeline\neve run\n</code></pre>"},{"location":"examples/basic-usage/#pdf-processing-pipeline","title":"PDF Processing Pipeline","text":"<p>Process PDF documents with cleaning and deduplication:</p> <pre><code># pdf_pipeline.yaml\npipeline:\n  batch_size: 5\n  inputs:\n    path: \"research_papers\"\n  stages:\n    - name: extraction\n      config:\n        format: \"pdf\"\n    - name: duplication\n    - name: cleaning\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"processed_papers\"\n</code></pre>"},{"location":"examples/basic-usage/#web-content-processing","title":"Web Content Processing","text":"<p>Process HTML documents with PII removal:</p> <pre><code># web_content.yaml\npipeline:\n  batch_size: 20\n  inputs:\n    path: \"web_pages\"\n  stages:\n    - name: extraction\n      config:\n        format: \"html\"\n    - name: pii\n      config:\n        url: \"http://127.0.0.1:8000\"\n    - name: export\n      config:\n        format: \"txt\"\n        destination: \"clean_content\"\n</code></pre>"},{"location":"examples/basic-usage/#advanced-pipeline-with-all-features","title":"Advanced Pipeline with All Features","text":"<p>Complete pipeline for scientific document processing:</p> <pre><code># scientific_pipeline.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"scientific_documents\"\n  stages:\n    - name: extraction\n      config:\n        url: \"http://127.0.0.1:8001\"\n    - name: duplication\n      config:\n        method: \"lsh\"\n        shingle_size: 3\n        num_perm: 128\n        threshold: 0.85\n    - name: cleaning\n      config:\n        use_llm: true\n        correct_latex: true\n        correct_tables: true\n        llm_url: \"http://127.0.0.1:8002\"\n    - name: pii\n      config:\n        url: \"http://127.0.0.1:8000\"\n        entities: [\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"]\n    - name: metadata\n      config:\n        extract_pdf_metadata: true\n        extract_html_metadata: true\n        fields: [\"title\", \"authors\", \"doi\", \"year\", \"journal\"]\n    - name: export\n      config:\n        format: \"json\"\n        destination: \"final_output\"\n        include_metadata: true\n        filename_pattern: \"{doc_id}_{timestamp}\"\n</code></pre>"},{"location":"examples/basic-usage/#batch-processing-examples","title":"Batch Processing Examples","text":""},{"location":"examples/basic-usage/#small-documents-email-messages","title":"Small Documents (Email, Messages)","text":"<pre><code># small_docs.yaml\npipeline:\n  batch_size: 50\n  inputs:\n    path: \"emails\"\n  stages:\n    - name: extraction\n    - name: pii\n      config:\n        url: \"http://127.0.0.1:8000\"\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"cleaned_emails\"\n</code></pre>"},{"location":"examples/basic-usage/#large-documents-books-reports","title":"Large Documents (Books, Reports)","text":"<pre><code># large_docs.yaml\npipeline:\n  batch_size: 2\n  inputs:\n    path: \"technical_reports\"\n  stages:\n    - name: extraction\n      config:\n        format: \"pdf\"\n    - name: cleaning\n      config:\n        use_llm: true\n    - name: metadata\n      config:\n        extract_pdf_metadata: true\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"processed_reports\"\n</code></pre>"},{"location":"examples/basic-usage/#mixed-format-processing","title":"Mixed Format Processing","text":"<p>Process different document types in the same pipeline:</p> <pre><code># mixed_format.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"mixed_documents\"\n  stages:\n    - name: extraction\n      # Auto-detect format based on file extension\n    - name: duplication\n      config:\n        method: \"lsh\"\n        threshold: 0.8\n    - name: cleaning\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"unified_output\"\n</code></pre>"},{"location":"examples/basic-usage/#selective-stage-processing","title":"Selective Stage Processing","text":"<p>Skip certain stages based on your needs:</p>"},{"location":"examples/basic-usage/#extraction-only","title":"Extraction Only","text":"<pre><code># extract_only.yaml\npipeline:\n  batch_size: 20\n  inputs:\n    path: \"raw_documents\"\n  stages:\n    - name: extraction\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"extracted_content\"\n</code></pre>"},{"location":"examples/basic-usage/#deduplication-only","title":"Deduplication Only","text":"<pre><code># dedup_only.yaml\npipeline:\n  inputs:\n    path: \"markdown_documents\"\n  stages:\n    - name: duplication\n      config:\n        method: \"lsh\"\n        threshold: 0.9\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"unique_documents\"\n</code></pre>"},{"location":"examples/basic-usage/#cleaning-only","title":"Cleaning Only","text":"<pre><code># clean_only.yaml\npipeline:\n  batch_size: 15\n  inputs:\n    path: \"noisy_content\"\n  stages:\n    - name: cleaning\n      config:\n        use_llm: true\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"clean_content\"\n</code></pre>"},{"location":"examples/basic-usage/#environment-specific-configurations","title":"Environment-Specific Configurations","text":""},{"location":"examples/basic-usage/#development-configuration","title":"Development Configuration","text":"<pre><code># dev_config.yaml\npipeline:\n  batch_size: 3  # Small batches for debugging\n  inputs:\n    path: \"test_data\"\n  stages:\n    - name: extraction\n    - name: cleaning\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"dev_output\"\n</code></pre>"},{"location":"examples/basic-usage/#production-configuration","title":"Production Configuration","text":"<pre><code># prod_config.yaml\npipeline:\n  batch_size: 50  # Large batches for efficiency\n  inputs:\n    path: \"/data/production_documents\"\n  stages:\n    - name: extraction\n      config:\n        url: \"http://extraction-service:8001\"\n    - name: duplication\n      config:\n        method: \"lsh\"\n        num_perm: 256\n        threshold: 0.85\n    - name: cleaning\n      config:\n        use_llm: true\n        llm_url: \"http://cleaning-service:8002\"\n    - name: pii\n      config:\n        url: \"http://pii-service:8000\"\n    - name: metadata\n      config:\n        extract_pdf_metadata: true\n    - name: export\n      config:\n        format: \"json\"\n        destination: \"/data/processed_documents\"\n        include_metadata: true\n</code></pre>"},{"location":"examples/basic-usage/#command-line-usage","title":"Command Line Usage","text":""},{"location":"examples/basic-usage/#basic-execution","title":"Basic Execution","text":"<pre><code># Run with default config file (config.yaml)\neve run\n\n# Run with specific config file\neve run --config custom_config.yaml\n\n# Enable debug logging\neve run --log-level DEBUG\n\n# Run only specific stages\neve run --stages extraction,export\n</code></pre>"},{"location":"examples/basic-usage/#environment-variables","title":"Environment Variables","text":"<pre><code># Set batch size\nexport EVE_BATCH_SIZE=20\n\n# Set input path\nexport EVE_INPUT_PATH=\"./my_documents\"\n\n# Set export format\nexport EVE_EXPORT_FORMAT=\"json\"\n\n# Run with environment variables\neve run\n</code></pre>"},{"location":"examples/basic-usage/#monitoring-progress","title":"Monitoring Progress","text":""},{"location":"examples/basic-usage/#basic-monitoring","title":"Basic Monitoring","text":"<pre><code># The pipeline automatically shows progress:\n[2024-01-15 10:30:00] INFO: Processing 1000 files with batch size 10\n[2024-01-15 10:30:05] INFO: Processing batch 1/100 (10 documents)\n[2024-01-15 10:30:15] INFO: Processing batch 2/100 (20 documents)\n[2024-01-15 10:35:00] INFO: Pipeline completed in 300.45 seconds\n[2024-01-15 10:35:00] INFO: Processed 850 documents successfully\n</code></pre>"},{"location":"examples/basic-usage/#custom-logging","title":"Custom Logging","text":"<pre><code># config_with_logging.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"documents\"\n  stages:\n    - name: extraction\n      config:\n        debug: true  # Enable debug for this stage\n    - name: export\n      config:\n        destination: \"output\"\n</code></pre>"},{"location":"examples/basic-usage/#error-handling","title":"Error Handling","text":""},{"location":"examples/basic-usage/#continue-on-errors","title":"Continue on Errors","text":"<pre><code># robust_config.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"documents\"\n  stages:\n    - name: extraction\n      config:\n        continue_on_error: true\n        error_output: \"extraction_errors.log\"\n    - name: export\n      config:\n        destination: \"output\"\n</code></pre>"},{"location":"examples/basic-usage/#validation-and-quality-checks","title":"Validation and Quality Checks","text":"<pre><code># quality_config.yaml\npipeline:\n  batch_size: 10\n  inputs:\n    path: \"documents\"\n  stages:\n    - name: extraction\n      config:\n        validate_content: true\n        min_content_length: 100\n        max_content_length: 1000000\n    - name: export\n      config:\n        destination: \"validated_output\"\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Explore advanced configuration examples</li> <li>Learn about server setup</li> <li>Check the API reference for programmatic usage</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":"<p>This guide covers all configuration options for the EVE Pipeline.</p>"},{"location":"getting-started/configuration/#configuration-file-structure","title":"Configuration File Structure","text":"<p>The pipeline is configured using a YAML file (typically <code>config.yaml</code>) with the following structure:</p> <pre><code>pipeline:\n  batch_size: integer\n  inputs:\n    path: string\n    # ... other input options\n  stages:\n    - name: string\n      config: object\n    # ... more stages\n</code></pre>"},{"location":"getting-started/configuration/#global-configuration","title":"Global Configuration","text":""},{"location":"getting-started/configuration/#batch_size","title":"batch_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>10</code></li> <li>Description: Number of documents to process in each batch</li> <li>Note: Not applicable to deduplication stage</li> </ul> <pre><code>pipeline:\n  batch_size: 20\n</code></pre>"},{"location":"getting-started/configuration/#inputs","title":"inputs","text":""},{"location":"getting-started/configuration/#path","title":"path","text":"<ul> <li>Type: String</li> <li>Required: Yes</li> <li>Description: Path to input directory containing documents</li> </ul> <pre><code>pipeline:\n  inputs:\n    path: \"input_documents\"\n</code></pre>"},{"location":"getting-started/configuration/#pipeline-stages","title":"Pipeline Stages","text":""},{"location":"getting-started/configuration/#extraction-stage","title":"Extraction Stage","text":"<p>Extracts content from various document formats.</p> <pre><code>- name: extraction\n  config:\n    format: \"\"  # or \"pdf\", \"html\", \"xml\", \"markdown\"\n    url: \"http://127.0.0.1:8001\"  # for server-based extraction\n</code></pre>"},{"location":"getting-started/configuration/#options","title":"Options","text":"<ul> <li>format: Document format specification</li> <li><code>\"\"</code> (default): Automatically detect format</li> <li><code>\"pdf\"</code>: PDF documents</li> <li><code>\"html\"</code>: HTML documents</li> <li><code>\"xml\"</code>: XML documents</li> <li> <p><code>\"markdown\"</code>: Markdown documents</p> </li> <li> <p>url: Server URL for nougat extraction</p> </li> </ul>"},{"location":"getting-started/configuration/#deduplication-stage","title":"Deduplication Stage","text":"<p>Removes duplicate and near-duplicate documents.</p> <pre><code>- name: duplication\n  config:\n    method: \"exact\"  # or \"lsh\"\n    # LSH options (when method: \"lsh\")\n    shingle_size: 3\n    num_perm: 128\n    threshold: 0.8\n</code></pre>"},{"location":"getting-started/configuration/#options_1","title":"Options","text":"<ul> <li>method: Deduplication method</li> <li><code>\"exact\"</code> (default): Exact hash-based deduplication</li> <li><code>\"lsh\"</code>: Locality Sensitive Hashing for near-duplicates</li> </ul>"},{"location":"getting-started/configuration/#lsh-options","title":"LSH Options","text":"<ul> <li>shingle_size: Size of text shingles (default: <code>3</code>)</li> <li>num_perm: Number of permutations (default: <code>128</code>)</li> <li>threshold: Similarity threshold (default: <code>0.8</code>)</li> </ul>"},{"location":"getting-started/configuration/#cleaning-stagecheck-this-stage","title":"Cleaning Stage(check this stage?)","text":"<p>Removes noise and improves document quality.</p> <pre><code>- name: cleaning\n  config:\n    ocr_threshold: 0.9\n    min_words: 2\n    enable_latex_correction: True\n</code></pre>"},{"location":"getting-started/configuration/#options_2","title":"Options","text":"<ul> <li>ocr_threshold: OCR duplicate threshold (default: <code>0.99</code>)</li> <li>min_words: Minimum words for processing (default: <code>2</code>)</li> <li>enable_latex_correction: Use LLM to fix latex formulas and tables (default: <code>false</code>)</li> </ul>"},{"location":"getting-started/configuration/#pii-removal-stage","title":"PII Removal Stage","text":"<p>Redacts personally identifiable information.</p> <pre><code>- name: pii\n  config:\n    url: \"http://127.0.0.1:8000\"\n</code></pre>"},{"location":"getting-started/configuration/#options_3","title":"Options","text":"<ul> <li>url: Presidio server URL </li> </ul>"},{"location":"getting-started/configuration/#export-stage","title":"Export Stage","text":"<p>Saves processed documents to output.</p> <pre><code>- name: export\n  config:\n    format: \"md\"  # or \"txt\", \"json\"\n    destination: \"output\"\n</code></pre>"},{"location":"getting-started/configuration/#options_4","title":"Options","text":"<ul> <li>format: Output format</li> <li><code>\"md\"</code> (default): Markdown</li> <li><code>\"txt\"</code>: Plain text</li> <li> <p><code>\"json\"</code>: JSON with metadata</p> </li> <li> <p>destination: Output directory path</p> </li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>This guide will help you install and set up the EVE Pipeline on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following installed:</p> <ul> <li>Python 3.10 or higher</li> <li>uv (recommended) or pip for package management</li> </ul>"},{"location":"getting-started/installation/#install-uv-recommended","title":"Install uv (Recommended)","text":"<pre><code>curl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre>"},{"location":"getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"getting-started/installation/#method-1-using-uv-recommended","title":"Method 1: Using uv (Recommended)","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/eve-esa/eve-pipeline.git\ncd eve-pipeline\n</code></pre> </li> <li> <p>Install dependencies</p> <pre><code>uv sync\n</code></pre> </li> </ol>"},{"location":"getting-started/installation/#method-2-using-pip","title":"Method 2: Using pip","text":"<ol> <li> <p>Clone the repository</p> <pre><code>git clone https://github.com/eve-esa/eve-pipeline.git\ncd eve-pipeline\n</code></pre> </li> <li> <p>Create a virtual environment</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n</code></pre> </li> <li> <p>Install dependencies</p> <pre><code>pip install -r requirements.txt\npip install -e .\n</code></pre> </li> </ol>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#server-setup","title":"Server Setup","text":"<p>Some pipeline stages require external servers:</p>"},{"location":"getting-started/installation/#pii-server","title":"PII Server","text":"<p>For PII (Personally Identifiable Information) removal:</p> <pre><code>cd server\npython3 pii_server.py\n</code></pre>"},{"location":"getting-started/installation/#ocr-server","title":"OCR Server","text":"<pre><code>cd server\npython3 nougat_server.py\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>After installation, proceed to the Quick Start guide to learn how to configure and run your first pipeline.</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>This tutorial will walk you through running your first data processing pipeline with EVE.</p>"},{"location":"getting-started/quick-start/#step-1-prepare-your-data","title":"Step 1: Prepare Your Data","text":"<p>Create an input directory with your documents:</p> <pre><code>mkdir -p input_data\n# Copy your PDF, HTML, XML, or Markdown files here\ncp /path/to/your/documents/* input_data/\n</code></pre>"},{"location":"getting-started/quick-start/#step-2-basic-configuration","title":"Step 2: Basic Configuration","text":"<p>Create a <code>config.yaml</code> file:</p> <pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"input_data\"\n  stages:\n    - name: extraction\n      # Automatically detects file format\n    - name: duplication\n    - name: export\n      config:\n        format: \"md\"\n        destination: \"output\"\n</code></pre>"},{"location":"getting-started/quick-start/#step-3-run-the-pipeline","title":"Step 3: Run the Pipeline","text":"<p>Execute the pipeline:</p> <pre><code>eve run\n</code></pre>"},{"location":"getting-started/quick-start/#step-4-check-results","title":"Step 4: Check Results","text":"<p>Your processed documents will be in the <code>output</code> directory:</p> <pre><code>ls output/\n</code></pre>"},{"location":"getting-started/quick-start/#example-pipeline-configurations","title":"Example Pipeline Configurations","text":""},{"location":"getting-started/quick-start/#pdf-processing-only","title":"PDF Processing Only","text":"<pre><code>pipeline:\n  batch_size: 5\n  inputs:\n    path: \"pdfs\"\n  stages:\n    - name: extraction\n      config: { format: \"pdf\" }\n    - name: cleaning\n    - name: export\n      config: { format: \"md\", destination: \"processed_pdfs\" }\n</code></pre>"},{"location":"getting-started/quick-start/#html-processing-with-pii-removal","title":"HTML Processing with PII Removal","text":"<pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"html_docs\"\n  stages:\n    - name: extraction\n      config: { format: \"html\", url: \"http://127.0.0.1:8001\" }\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: export\n      config: { format: \"md\"}\n</code></pre>"},{"location":"getting-started/quick-start/#advanced-pipeline-with-all-stages","title":"Advanced Pipeline with All Stages","text":"<pre><code>pipeline:\n  batch_size: 10\n  inputs:\n    path: \"mixed_docs\"\n  stages:\n    - name: extraction\n      config: { url: \"http://127.0.0.1:8001\" }\n    - name: duplication\n      config: {\n        method: \"lsh\",\n        shingle_size: 3,\n        num_perm: 128,\n        threshold: 0.8\n      }\n    - name: cleaning\n    - name: pii\n      config: { url: \"http://127.0.0.1:8000\" }\n    - name: metadata\n</code></pre>"},{"location":"getting-started/quick-start/#monitoring-progress","title":"Monitoring Progress","text":"<p>The pipeline provides progress updates:</p> <pre><code>$ eve run\n\n[2024-01-15 10:30:00] INFO: Starting pipeline with 100 documents\n[2024-01-15 10:30:01] INFO: Stage 1/5: Extraction\n[2024-01-15 10:30:15] INFO: Processing batch 1/10 (10 documents)\n[2024-01-15 10:30:30] INFO: Processing batch 2/10 (20 documents)\n...\n[2024-01-15 10:35:00] INFO: Pipeline completed successfully\n[2024-01-15 10:35:00] INFO: Processed 95 documents, 5 duplicates removed\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about configuration options</li> <li>Explore pipeline stages</li> <li>Check out advanced examples</li> </ul>"},{"location":"pipeline-stages/cleaning/","title":"Cleaning Stage","text":"<p>The cleaning stage improves document quality by removing noise artifacts, correcting formatting issues, and enhancing readability.</p>"},{"location":"pipeline-stages/cleaning/#features","title":"Features","text":"<ul> <li>Noise Removal: Eliminates OCR related artifacts and noise</li> <li>LaTeX Correction: Fixes mathematical equations and notation</li> </ul>"},{"location":"pipeline-stages/cleaning/#configuration","title":"Configuration","text":""},{"location":"pipeline-stages/cleaning/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: cleaning\n  config:\n    ocr_threshold: 0.99\n</code></pre>"},{"location":"pipeline-stages/cleaning/#llm-enhanced-cleaning","title":"LLM-Enhanced Cleaning","text":"<p>You need to set the .env key for <code>OPENROUTER_API_KEY</code>.</p> <pre><code>- name: cleaning\n  config:\n    enable_latex_correction: true\n</code></pre>"},{"location":"pipeline-stages/cleaning/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"pipeline-stages/cleaning/#ocr_threshold","title":"ocr_threshold","text":"<ul> <li>Type: Float</li> <li>Default: <code>0.99</code></li> <li>Description: OCR duplicate threshold</li> </ul>"},{"location":"pipeline-stages/cleaning/#min_words","title":"min_words","text":"<ul> <li>Type: Int</li> <li>Default: 2</li> <li>Description: Minimum words for processing</li> </ul>"},{"location":"pipeline-stages/cleaning/#enable_latex_correction","title":"enable_latex_correction","text":"<ul> <li>Type: Boolean</li> <li>Default: <code>false</code></li> <li>Description: Use LLM to fix latex formulas and tables</li> </ul>"},{"location":"pipeline-stages/cleaning/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about PII removal</li> <li>Configure metadata extraction</li> <li>Set up document export</li> </ul>"},{"location":"pipeline-stages/deduplication/","title":"Deduplication Stage","text":"<p>The deduplication stage removes duplicate and near-duplicate documents from your dataset, improving data quality and reducing processing overhead.</p>"},{"location":"pipeline-stages/deduplication/#deduplication-methods","title":"Deduplication Methods","text":""},{"location":"pipeline-stages/deduplication/#exact-deduplication","title":"Exact Deduplication","text":"<p>Uses SHA-256 checksums to identify identical documents:</p> <pre><code>- name: duplication\n  config:\n    method: \"exact\"\n</code></pre>"},{"location":"pipeline-stages/deduplication/#lsh-locality-sensitive-hashing","title":"LSH (Locality Sensitive Hashing)","text":"<p>Finds near-duplicates using MinHash:</p> <pre><code>- name: duplication\n  config:\n    method: \"lsh\"\n    shingle_size: 3\n    num_perm: 128\n    threshold: 0.8\n</code></pre>"},{"location":"pipeline-stages/deduplication/#configuration-parameters","title":"Configuration Parameters","text":""},{"location":"pipeline-stages/deduplication/#lsh-parameters","title":"LSH Parameters","text":""},{"location":"pipeline-stages/deduplication/#shingle_size","title":"shingle_size","text":"<ul> <li>Type: Integer</li> <li>Default: <code>3</code></li> <li>Range: 1-10</li> <li>Description: Size of text chunks (shingles) for comparison</li> </ul>"},{"location":"pipeline-stages/deduplication/#num_perm","title":"num_perm","text":"<ul> <li>Type: Integer</li> <li>Default: <code>128</code></li> <li>Range: 64-1024</li> <li>Description: Number of random permutations for MinHash</li> </ul>"},{"location":"pipeline-stages/deduplication/#threshold","title":"threshold","text":"<ul> <li>Type: Float</li> <li>Default: <code>0.8</code></li> <li>Range: 0.0-1.0</li> <li>Description: Similarity threshold for duplicate detection</li> </ul>"},{"location":"pipeline-stages/deduplication/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about content cleaning</li> <li>Configure PII removal</li> <li>Set up metadata extraction</li> </ul>"},{"location":"pipeline-stages/export/","title":"Export Stage","text":"<p>The final stage of the pipeline where the processed documents are stored in the format required by the user. The default format is .md</p>"},{"location":"pipeline-stages/extraction/","title":"Extraction Stage","text":"<p>The extraction stage is responsible for reading and extracting content from various document formats. It's the first stage in most pipeline configurations.</p>"},{"location":"pipeline-stages/extraction/#supported-formats","title":"Supported Formats","text":"<ul> <li>PDF: Portable Document Format files</li> <li>HTML: Hypertext Markup Language files</li> <li>XML: Extensible Markup Language files</li> <li>Markdown: Markdown text files</li> </ul>"},{"location":"pipeline-stages/extraction/#configuration","title":"Configuration","text":""},{"location":"pipeline-stages/extraction/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: extraction\n  config:\n    format: \"pdf\"  # or , \"html\", \"xml\", \"markdown\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#stage-behavior","title":"Stage Behavior","text":""},{"location":"pipeline-stages/extraction/#input-processing","title":"Input Processing","text":"<p>The extraction stage processes documents from the configured input directory:</p> <pre><code>pipeline:\n  inputs:\n    path: \"input_documents\"\n</code></pre> <ul> <li>Recursively scans the input directory</li> <li>Supports nested folder structures</li> </ul>"},{"location":"pipeline-stages/extraction/#format-specific-features","title":"Format-Specific Features","text":""},{"location":"pipeline-stages/extraction/#pdf-extraction","title":"PDF Extraction","text":"<p>For PDF documents, the extractor:</p> <ul> <li>Extracts text content using Nougat OCR.</li> <li>Preserves document structure (headings, paragraphs).</li> <li>Maintains table and formulas.</li> </ul> <pre><code>- name: extraction\n  config:\n    format: \"pdf\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#html-extraction","title":"HTML Extraction","text":"<p>For HTML documents, the extractor use trafilatura to extract the content.</p> <pre><code>- name: extraction\n  config:\n    format: \"html\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#xml-extraction","title":"XML Extraction","text":"<p>For XML documents, the extractor:</p> <ul> <li>Extracts text content from XML tags</li> <li>Preserves document structure</li> <li>Handles namespaces appropriately</li> <li>Maintains attribute information when relevant</li> </ul> <pre><code>- name: extraction\n  config:\n    format: \"xml\"\n</code></pre>"},{"location":"pipeline-stages/extraction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about deduplication</li> <li>Explore cleaning options</li> <li>Configure PII removal</li> </ul>"},{"location":"pipeline-stages/metadata-extraction/","title":"Metadata Extraction Stage","text":"<p>The metadata extraction stage automatically identifies and extracts structured metadata from documents, including bibliographic information, document properties, and content characteristics.</p>"},{"location":"pipeline-stages/metadata-extraction/#extracted-metadata-fields","title":"Extracted Metadata Fields","text":""},{"location":"pipeline-stages/metadata-extraction/#document-identification","title":"Document Identification","text":"<ul> <li>title: Document title</li> <li>authors: List of author names</li> <li>doi: Digital Object Identifier</li> <li>url: Source URL or link</li> <li>year: Publication year</li> <li>journal: Journal or publication name</li> <li>publisher: Publisher name</li> </ul>"},{"location":"pipeline-stages/metadata-extraction/#extraction-methods","title":"Extraction Methods","text":""},{"location":"pipeline-stages/metadata-extraction/#pdf-metadata-extraction","title":"PDF Metadata Extraction","text":"<p>For PDF documents, the extractor uses MonkeyOCR server with crossref to extract the metadata information.</p>"},{"location":"pipeline-stages/metadata-extraction/#html-metadata-extraction","title":"HTML Metadata Extraction","text":"<p>For HTML documents, the extractor uses regex patterns to extract the metadata information.</p>"},{"location":"pipeline-stages/metadata-extraction/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about document export</li> </ul>"},{"location":"pipeline-stages/pii-removal/","title":"PII Removal Stage","text":"<p>The PII (Personally Identifiable Information) removal stage automatically detects and redacts NAMES and EMAILS to protect privacy and ensure compliance.</p>"},{"location":"pipeline-stages/pii-removal/#configuration","title":"Configuration","text":""},{"location":"pipeline-stages/pii-removal/#basic-configuration","title":"Basic Configuration","text":"<pre><code>- name: pii\n  config:\n    url: \"http://127.0.0.1:8000\"\n</code></pre>"},{"location":"pipeline-stages/pii-removal/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about metadata extraction</li> <li>Configure document export</li> </ul>"}]}